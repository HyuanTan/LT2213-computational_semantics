论文《Word sense disambiguation using a bidirectional LSTM》（Mikael Kågebäck & Hans Salomonsson, Chalmers University of Technology）的**方法和结论**。

---

## 方法

### 1. 模型设计理念

论文的核心思想是：
- **利用双向长短期记忆网络（BLSTM）**来进行词义消歧（WSD）。
- **模型完全端到端训练**：直接从原始文本到词义标签。
- **不依赖任何外部资源**：如知识图谱、词性标注、解析器或手工设计特征。

### 2. 主要模型结构

整体结构如下：
- 输入：一句话（包含目标歧义词）。
- **输入层**：词向量（GloVe 预训练词向量）。
- **BLSTM 层**：捕获目标词前后的上下文顺序信息。
- **隐藏层**：线性单元。
- **Softmax 层**：针对目标词的可能词义做概率分布输出。

> **关键**：BLSTM 和隐藏层参数对所有单词共享，但 softmax 层的参数针对不同词单独设置（因为不同词有不同的可能词义集合）。

### 3. Dropword 正则化

为了避免模型过度依赖单个上下文词，作者提出了**dropword**：
- 随机把输入句子的一些词替换成 `<dropped>` 标签。
- `<dropped>` 也有一个可训练的嵌入。
- 类似 Dropout 的词级版本，但比简单的词丢弃更灵活。

### 4. 损失函数

使用 **交叉熵损失** 对模型参数进行训练。

\[
L(\Omega) = - \sum_{i \in I} \sum_{j \in S(w_i)} t_{i,j} \log y_j(i)
\]

其中：
- \( t_{i,j} \) 是真实的词义标签。
- \( y_j(i) \) 是 softmax 输出的预测概率。

### 5. 实验设置

- 数据集：SensEval 2（SE2）和 SensEval 3（SE3）标准 WSD 数据。
- 超参数：如嵌入维度（100）、BLSTM 隐藏层大小（2×74）、Dropout（50%）、Dropword 比例（10%）等。
- 优化算法：带动量的随机梯度下降（SGD）。
- 词向量初始化：GloVe（在 Wikipedia + Gigaword 上预训练）。

---

## 结论

### 主要实验结果

在 SE2 和 SE3 两个数据集上：
- **SE2 F1：66.9**
- **SE3 F1：73.4**

成绩和当时的最优方法持平或超过，尤其值得注意的是：
- **不使用额外资源**（如词性标注、句法分析、知识库等）。
- **仅依靠顺序信息和词嵌入**，效果媲美甚至超越了复杂的特征工程方法。

| 方法                  | SE2 F1 | SE3 F1 |
|----------------------|--------|--------|
| 本文 BLSTM 模型       | 66.9   | 73.4   |
| 100JHU（特征工程）    | 64.2   | -      |
| htsa3（特征工程）     | -      | 72.9   |
| IMS+adapted CW（复杂特征 + 词向量） | 66.2 | 73.4 |

**额外发现**：
- 移除 Dropword → 成绩下降。
- 不使用 GloVe → 成绩大幅下降。
- 打乱词序 → 成绩大幅下降（说明顺序信息非常重要）。

---

## 论文的主要贡献

1. **提出了一个纯粹基于深度学习的词义消歧模型**，不依赖任何外部知识或语言特定特征。
2. **共享参数**：不同词共享 BLSTM 和隐藏层参数，使得在大词汇表场景下可扩展。
3. **实验证明上下文词序的重要性**，克服了传统 bag-of-words 模型的局限。
4. **提出 Dropword 技术**：一种新颖的正则化方法。

---

## 未来工作

作者提出了两个主要方向：
- 在多语言环境测试，验证模型的语言无关性。
- 扩展到“大词汇量的全词义消歧”（all-words WSD），不仅是小样本词集合。

---

# GloVe，Dropword 和 BLSTM 实现的细节, 在模型中的具体用法


---

## 1️⃣ GloVe（Global Vectors for Word Representation）

**核心思想**：
- GloVe 结合了 **计数统计（co-occurrence matrix）** 和 **预测模型（如 word2vec 的 skip-gram）** 的优点。
- 它不是仅仅根据周围窗口预测单词（像 word2vec），而是根据**整个语料库中单词对的共现概率**来学习向量。

**数学公式**（简化版）：
\[
J = \sum_{i,j=1}^V f(P_{ij}) \left( w_i^T \tilde{w}_j + b_i + \tilde{b}_j - \log (P_{ij}) \right)^2
\]
其中：
- \( P_{ij} \)：单词 i 和单词 j 的共现次数。
- \( w_i \)、\( \tilde{w}_j \)：词向量。
- \( f \)：控制共现频次对损失函数的影响。

**在本文中的实现**：
- 使用 **公开的 GloVe 预训练向量**，在 Wikipedia + Gigaword 上训练的。
- 嵌入维度：**100维**。
- 对于预训练向量中没有出现的词：初始化为高斯分布 \( \mathcal{N}(0, 0.1) \)。
- 嵌入矩阵 \( W^x \) 的标准差（σi）会在训练中更新，用于控制输入噪声。

**作用**：
- 提供了丰富的语义信息，即使训练数据有限，模型也能获得良好的词表示。
- 实验表明：去掉 GloVe，F1 分数严重下降（SE2 从 66.9 降到 54.6）。

---

## 2️⃣ Dropword（正则化技术）

**灵感来源**：
- **Dropout**（神经元级别的随机失活）。
- **Word Dropout**（Iyyer 等人提出），把一些词设置为零。

**Dropword 的实现**：
- 随机选取句子中的一些单词。
- 将这些词 **替换为一个特殊的 `<dropped>` 标签**。
- `<dropped>` 本身也有一个独立的嵌入，训练时学习。

**和 Word Dropout 的区别**：
| 方法         | 替换方式                              |
|--------------|-----------------------------------|
| Word Dropout | 把词向量置为全零                  |
| Dropword     | 用 `<dropped>` 标签，标签有自己可训练的向量 |

**实验中 Dropword 的设置**：
- Dropword 概率：**10%**（也尝试过 0% 到 20%）。
- 替换是**动态进行**的，不是固定替换，即每次训练时替换的词不同。

**作用**：
- 减少对特定上下文词的依赖，提高模型的泛化能力。
- 实验结果：加入 Dropword 后，性能一致提高。

---

## 3️⃣ BLSTM（Bidirectional Long Short-Term Memory）

**为什么选择 BLSTM**：
- 词义消歧强烈依赖**上下文顺序信息**。
- 普通 RNN 存在梯度消失/爆炸问题，长距离依赖效果差。
- BLSTM 结合了前向（左到右）和后向（右到左）的上下文信息。

**BLSTM 结构细节**：
- **输入**：GloVe 词向量。
- **隐藏层大小**：每个方向 74，左右拼接后为 148 维。
- **输出**：[前向隐藏状态 \( h^L \) ; 后向隐藏状态 \( h^R \)] 拼接。
- **正则化**：对 BLSTM 输出应用 50% 的 Dropout。

**在模型中的使用**：
- BLSTM 输出传入一个 **线性隐藏层**（不加非线性激活），进一步映射。
- 然后进入 **Softmax 层**：不同词有不同的 Softmax 权重和偏置，因为不同词的词义集合不同。

**优势**：
- 能同时捕捉目标词的**左右上下文**信息。
- 实验证明：如果打乱词序，性能明显下降（SE2 从 66.9 降到 58.8），说明顺序信息至关重要。

---

## 小结：三者配合的优势

| 组件      | 功能                                               | 对性能的贡献                     |
|-----------|----------------------------------------------------|--------------------------------|
| GloVe     | 提供预训练语义信息，尤其在数据不足时表现关键       | 去掉后 F1 大幅下降              |
| Dropword  | 防止过拟合，提高模型对上下文变动的适应能力         | 增加后 F1 提升                  |
| BLSTM     | 捕捉上下文顺序信息，弥补 Bag-of-Words 的缺陷       | 打乱顺序后 F1 明显下降          |

---

# 不同词有不同的 Softmax 权重和偏置具体如何实现

---

## 🔎 为什么要这么设计？

WSD 任务的特点：
- 每个词（word type）都有**不同的可能词义集合**（senses）。
- 比如：
  - *rock* → {stone, music, ...}
  - *bank* → {financial institution, river side, ...}

所以：
- Softmax 层不能所有词共用同一组权重和偏置。
- 必须**针对每个目标词（word type）单独设计 Softmax 层参数**，否则无法输出正确类别。

---

## 🔧 实现方式（论文第3.1节）

### Softmax 输出公式：
\[
y(n) = \text{softmax}(W^a_{w_n} \cdot a + b^a_{w_n})
\]
其中：
- \( y(n) \)：位置 n 的目标词的 sense 概率分布。
- \( W^a_{w_n} \)：**目标词 \(w_n\)** 专属的 softmax 权重矩阵。
- \( b^a_{w_n} \)：**目标词 \(w_n\)** 专属的 softmax 偏置向量。
- \( a \)：隐藏层的输出，所有词共用的线性变换。

> ⚠ 注意：只有 Softmax 的权重 \( W^a \) 和偏置 \( b^a \) 是**词类型（word type）特定的**。  
> 其他部分（嵌入层 BLSTM 和隐藏层）都是所有词共享参数。

---

### Softmax 参数的维度：
假设：
- 词 \(w_n\) 的可能词义数 = \( S(w_n) \)
- 隐藏层的维度 = \( H \)

那么：
- \( W^a_{w_n} \) 是一个 **\( S(w_n) \times H \)** 的矩阵。
- \( b^a_{w_n} \) 是一个 **\( S(w_n) \)** 的向量。

### 举个例子：
假设：
- 当前目标词：rock。
- rock 有 3 个词义（stone、music、climbing hold）。

如果隐藏层是 148 维：
- \( W^a_{rock} \)：**3 × 148** 的矩阵。
- \( b^a_{rock} \)：长度为 3 的偏置向量。

对于 bank（假如有 4 个词义），则：
- \( W^a_{bank} \)：**4 × 148**。
- \( b^a_{bank} \)：长度为 4。

---

## 🔍 在代码或模型训练时怎么做？

用**参数查表（parameter lookup）**的方法：
1. 为词表中的每个词预留一个独立的 \( W^a \) 和 \( b^a \)。
2. 训练时根据目标词索引选择相应的参数。
3. 反向传播时，只更新对应词的 softmax 权重/偏置。

**优点**：
- 不需要为每个目标词单独训练一个完整模型。
- 共享 BLSTM 和隐藏层的参数，可以有效利用所有训练数据。

---

## 📝 小结

| 模块                     | 参数共享？                | 说明                                   |
|--------------------------|---------------------------|----------------------------------------|
| 词向量（嵌入）            | 所有词共享                 | GloVe 初始化，OOV 词随机初始化         |
| BLSTM 层                  | 所有词共享                 | 捕捉上下文顺序信息                     |
| 隐藏层                    | 所有词共享                 | 把 BLSTM 输出映射到 softmax 前的空间   |
| Softmax 权重和偏置（\( W^a, b^a \)） | 不共享（按 word type 单独） | 不同词不同 sense 集合，参数独立         |

---

# **BLSTM 中 Bidirectional 的具体含义** 和 **实现方式**。

---

## 🔎 Bidirectional 的含义

### 普通 LSTM（单向）
- **只能从前向后处理序列**。
- 也就是说，时间步 \( t \) 的隐藏状态 \( h_t \) 只能看到前面的输入（\( x_1, x_2, ..., x_t \)）。

**问题**：  
词义消歧（WSD）时，单向 LSTM 只能看到目标词前面的上下文，但**很多时候，目标词后面的上下文同样重要**。

**例子**：
> "She sat on the **bank** and watched the water flow."
>
> 如果只看 "She sat on the", 不知道 bank 是 "河岸" 还是 "银行"。

---

### Bidirectional LSTM（BLSTM）
**双向处理序列**：
- 一个 LSTM 从 **左到右**（前向，Forward）。
- 一个 LSTM 从 **右到左**（后向，Backward）。

对于序列中第 n 个单词：
\[
\text{前向隐藏状态} = h^L_n
\]
\[
\text{后向隐藏状态} = h^R_n
\]

最终：
\[
\mathbf{h}_n = [h^L_n; h^R_n]
\]
（把前向和后向的隐藏状态 **拼接** 起来）

这样：
- **\( h^L_n \)** 包含了从句子起点到当前词的信息。
- **\( h^R_n \)** 包含了从句子末尾到当前词的信息。

**目标词的表示包含了整个上下文的综合信息**（前+后）。

---

## 🔧 具体实现

**输入**：整句话的词向量序列 \( x_1, x_2, ..., x_T \)。

**过程**：

|方向|输入|隐藏状态更新|
|---|---|---|
|前向（Forward）|从 \( x_1 \) 到 \( x_T \)| \( h^L_1, h^L_2, ..., h^L_T \)|
|后向（Backward）|从 \( x_T \) 到 \( x_1 \)| \( h^R_T, h^R_{T-1}, ..., h^R_1 \)|

每个词位置 n：
\[
\mathbf{h}_n = [h^L_n; h^R_n]
\]

然后传给后面的隐藏层和 softmax。

---

## 📝 在 TensorFlow（或 PyTorch）中的实现（简要伪代码）

```python
import tensorflow as tf

# 假设输入是 batch_size × 序列长度 × 词向量维度
inputs = tf.placeholder(tf.float32, [batch_size, seq_length, embedding_dim])

# 定义两个 LSTM
lstm_fw = tf.nn.rnn_cell.LSTMCell(hidden_size)   # Forward
lstm_bw = tf.nn.rnn_cell.LSTMCell(hidden_size)   # Backward

# 使用 tf.nn.bidirectional_dynamic_rnn
(outputs_fw, outputs_bw), _ = tf.nn.bidirectional_dynamic_rnn(
    lstm_fw, lstm_bw, inputs, dtype=tf.float32
)

# 拼接前向和后向输出
outputs = tf.concat([outputs_fw, outputs_bw], axis=2)  # dim = 2 * hidden_size
```

**注意**：
- TensorFlow 的 `bidirectional_dynamic_rnn` 已经自动完成了两个方向的计算和拼接。
- PyTorch 也是类似（`nn.LSTM` 中 `bidirectional=True`）。

---

## ✅ 小结

| 特征| 单向 LSTM | 双向 LSTM（BLSTM） |
|---|---|---|
| 处理方向| 只能前向 | 前向 + 后向 |
| 可用上下文| 目标词前面 | 前后全部 |
| 适合任务| 简单序列预测 | 需要完整上下文的信息抽取，如 WSD |

---

**所以**：  
BLSTM 的 **Bidirectional** 本质上就是 **两个方向的 LSTM 并行处理**，然后把两个方向的隐藏状态拼接，形成对目标词的完整上下文建模。

---

# 关于Dropword的具体实现细节
。

---

## 🔎 Dropword 是什么？

论文（第 3.2 节）定义：  
> Dropword 是一种 **正则化技术**，类似于 word dropout。  
> **训练时**，随机将输入句子的某些词替换成一个特殊的标签：`<dropped>`。  
> **这个标签也有一个词嵌入**，而不是简单地丢弃或置零。

---

## 🔧 Dropword 的实现细节

### 什么时候使用？
- **只在训练阶段**使用（和 Dropout 类似）。
- 每次训练迭代时，对输入句子的词随机应用。

### 实现步骤：

假设：
- **输入句子**：  
  "She sat on the bank and watched the water flow."

- **目标词（待消歧的词）**："bank"

---

1️⃣ **确定 Dropword 比例**  
论文的实验设置是 **10%**，也就是：
- 对每个词，以 10% 概率随机选择替换。

2️⃣ **随机选词**  
假设这次抽中了 "sat" 和 "flow"。

3️⃣ **替换**  
把选中的词替换成 `<dropped>`：
```plaintext
She <dropped> on the bank and watched the water <dropped>.
```

4️⃣ **词嵌入映射**  
- "She" → GloVe 向量
- `<dropped>` → **一个可训练的嵌入向量**
- "on", "the", "bank", ... → 各自的词向量

注意：  
- `<dropped>` 是一个真实的词，嵌入初始化后和其他词一样被训练。
- 每次训练迭代时，**被替换的词随机变化**。

---

## 📌 和其他方法的区别

| 方法 | 做法 | 被替换词的向量 |
|-----|-----|-------------|
| Dropout | 随机置零神经元 | 不适用（作用于隐藏层） |
| Word Dropout | 把词嵌入设为全零 | 全零向量 |
| **Dropword（本文）** | 替换为 `<dropped>` | 一个可训练的向量 |

**优势**：
- **保持输入长度不变**。
- 模型不会依赖个别上下文词，提高泛化能力。
- 被替换的 `<dropped>` 向量可以学习到“缺失词的平均表示”，而不是完全信息丢失。

---

## ✅ 举个完整例子

**原句**：
```plaintext
The rock was too heavy to lift.
```

假设目标词是 "rock"（有多种可能词义：stone / music）。

**应用 Dropword（假设 20% 概率）**：
随机选中 "too" 和 "lift"。

**替换后句子**：
```plaintext
The rock was <dropped> heavy to <dropped>.
```

**模型看到的词向量**：
- "The" → GloVe 向量。
- "rock" → GloVe 向量。
- "was" → GloVe 向量。
- `<dropped>` → **训练中逐步学习的特殊向量**。
- "heavy" → GloVe 向量。

**好处**：
- 模型学会：即使丢掉了一些上下文信息，依然能根据剩余信息判断 rock 的正确词义。

---

## 📝 小结

| 步骤 | 内容 |
|---|---|
| 训练时 | 随机替换部分词为 `<dropped>` |
| 替换方式 | 不是置零，而是用一个可训练的向量 |
| 频率 | 论文实验用 10%（调参时测试过 0%-20%） |
| 优点 | 降低对特定词的依赖，提高泛化性能 |

---

# 关于 dropped 的进一步解释

`<dropped>` **不是一个有真实语义的词**，它是一个**特殊的占位符**，**完全为了训练中的正则化而设计**。

---

## 🔎 `<dropped>` 的含义

在 Dropword 方法里，`<dropped>`：

- **只是一个标签**（token），
- 表示「这个位置原来的词被随机替换掉了」。

**它本身没有语义**。  
不会告诉模型 "dropped" 的含义（像 "落下" 或 "丢弃"），
而是：
- 在训练过程中，它会被当作普通词一样**赋予一个随机初始化的向量**。
- 随着训练进行，这个向量会被优化，学习到「缺失词的平均表示」。

**所以**：
- `<dropped>` 并不表示英文里的 "dropped"（过去分词）。
- 它和 `<unk>`（未知词）、`<pad>`（填充词）这些标签一样，是训练的技术性工具。

---

## 🔧 为什么用 `<dropped>` 而不是置零？

如果简单地把被丢弃的词置零（像 Word Dropout），会有这些缺点：

| 方法 | 缺点 |
|---|---|
| Word Dropout（置零）| 彻底丢失词的信息，无法给模型提供「这里有词丢失」的信号。|
| Dropword（`<dropped>` 标签）| 保留「这个位置有缺词」的信息，同时允许模型学习如何处理缺失的上下文。|

**举例**：

句子（原始）：
```plaintext
She sat on the bank and watched the water flow.
```

替换后：
```plaintext
She sat on the <dropped> and watched the water flow.
```

**模型可以学到**：
- 句子里虽然 "bank" 左边丢了一个词，但 "watched the water flow" 强烈提示 "bank" 是「河岸」而不是「银行」。
- `<dropped>` 的向量会学习到：「我这里缺了一个可能有用的信息，但我不能猜具体是什么」。

---

## 📝 总结

| 属性 | `<dropped>` 的特点 |
|---|---|
| 语义 | 没有真实语义，只表示「有词被丢弃」 |
| 作用 | 正则化，防止模型过拟合具体上下文词 |
| 向量 | 初始化随机，训练过程中学习「缺失词的平均表现」 |
| 和 Dropout 区别 | Dropout 是神经元置零，Dropword 是用可学习向量替换 |

---


## 🔧 `<dropped>` 向量在训练过程中是如何更新的？

### 1️⃣ 初始化
- `<dropped>` 作为一个**特殊词**，在词汇表里有一个唯一的索引（token ID）。
- 它的词向量（embedding）初始化为一个**随机向量**，比如从正态分布 \( \mathcal{N}(0, 0.1) \) 采样。

### 2️⃣ 前向传播时的使用
- 当句子里的某个词被 Dropword 替换时，模型会用 `<dropped>` 的词向量代替原词的向量。
- 这个 `<dropped>` 向量被送入 **BLSTM**，继续传递到隐藏层和 softmax。

### 3️⃣ 反向传播时的更新
- 因为 `<dropped>` 的向量参与了正向计算，所以：
  - 损失函数（cross-entropy）计算出的误差信号（gradient）会**反向传递**回来。
  - 误差信号更新 `<dropped>` 向量的参数，使它更好地帮助完成词义消歧任务。

**每当 `<dropped>` 被用在不同的上下文中，它的向量都会微调。**

### 4️⃣ 多次训练后
- `<dropped>` 向量逐渐学习到一个「**泛化的上下文缺失表示**」：
  - 它不会像具体词那样捕捉特定含义。
  - 它反映「这里缺了一个可能重要的信息，但你（模型）需要学会在信息缺失时继续做出合理的判断」。

---

## 🤔 为什么 `<dropped>` 向量能帮助泛化？

### 传统模型的问题：
- 如果训练时每次都用完整的上下文，模型**容易过拟合**——对某些固定上下文词高度依赖。

**例如**：
> 句子："She sat on the **bank** and watched the water flow."

训练时，模型可能习惯于看到 "water"、"flow" 和 "bank" 同时出现，并认为 "bank" ＝ "河岸"。

如果未来遇到：
> "She rested by the **bank** as ducks swam nearby."

"flow" 和 "water" 不在上下文，模型就可能预测错误。

---

### Dropword 的好处：
- 训练时，随机遮蔽部分上下文词。
- 迫使模型**学会在信息缺失的情况下也能推断正确的词义**。

**举个例子**：

| 训练句 | 目标词 | Dropword 作用 |
|---|---|---|
| "She sat on the <dropped> and watched the water flow." | bank | 学习在丢失部分上下文时仍然正确分类。|
| "She rested by the bank as ducks swam nearby." | bank | 在类似但不完全相同的上下文中，做出正确预测。|

这种训练相当于**数据增强**：
- 人为制造「上下文不完整」的情况。
- 提前锻炼模型面对未来未见过或不完整的上下文。

---

## ✅ 和 Dropout 的类比

| 方法 | 遮蔽单位 | 遮蔽后行为 | 作用 |
|---|---|---|---|
| Dropout | 神经元 | 输出置 0 | 防止依赖单个神经元，增强泛化 |
| Word Dropout | 单词 | 嵌入置 0 | 减少对具体词的依赖 |
| Dropword | 单词 | 替换为 `<dropped>` 的可训练向量 | 减少对具体词的依赖，同时允许学习「缺失信息的平均表现」 |

**结论**：  
`<dropped>` 向量相当于在训练过程中，**学会扮演「不确定信息」的角色**，  
帮助模型对信息缺失的鲁棒性更强，避免过拟合，提高泛化能力。

---
