下面是一份对论文 “Climbing towards NLU: On meaning, form, and understanding in the age of data” 的中英文详细解读。该论文由 Emily M. Bender（华盛顿大学）和 Alexander Koller（萨尔大学）发表于 ACL 2020 年年会。它主要讨论了当前大规模语言模型（如 BERT、GPT-2 等）在自然语言任务上的成功与其在“理解（understanding）”或“意义（meaning）”层面的真实能力之间的差距，并强调了在纯粹基于表层形式（form）的训练过程中，模型无法真正学得“意义（meaning）”这一核心观点。

---

## 一、论文背景与动机 (Background and Motivation)

### 英文
The paper addresses the successes of large neural language models (LMs) such as BERT or GPT-2 on many NLP tasks, including tasks that appear to require understanding. These successes can lead to hype or overclaims that these models “comprehend” or “understand” language, or that they capture its “meaning.” The authors argue that such claims mistakenly overlook the crucial distinction between a language’s **form** (i.e., its observable signals, such as characters and words) and its **meaning** (i.e., the link between linguistic expressions and the speaker’s communicative intent or external, real-world referents).

### 中文
作者指出，大规模神经语言模型（如 BERT、GPT-2）在许多 NLP 任务上都取得了显著成功，尤其在一些看似需要语义理解的任务上也有不错表现。这让部分研究者及媒体误以为模型已经实现了对语言的“理解”或“掌握了语言的意义”。作者认为，这种说法忽视了“形式（form）”与“意义（meaning）”的区别：纯粹依赖表层文本的统计学习，不可能完全掌握语言与世界、以及语言使用者意图之间的关联，也就无法真正学到“意义”。

---

## 二、核心概念：形式与意义 (Key Concepts: Form and Meaning)

### 英文
1. **Form**: Any observable realization of language — the sequence of characters, words, or even phonetic signals. In the context of large LMs, the “form” is essentially the text they are trained on.
2. **Meaning**: The connection between linguistic form and something outside of language itself, particularly the speaker’s communicative intent or references in the real (or imagined) world. True language “understanding” requires recognizing this mapping from form to meaning.
3. **Communicative Intent**: Speakers use language to achieve goals in the real world or in social contexts (e.g., requesting, informing, questioning). Meaning must be grounded in these intentions. A purely form-based model, having never interacted with the real world or intentions behind the text, cannot recover these deeper links.

### 中文
1. **形式（Form）**：任何可观察到的语言外在形式，如文本中的字符、词序列或语音信号等。对于大规模语言模型而言，训练数据就是这些庞大的文本语料，它们只能捕捉到表面出现的形式分布。
2. **意义（Meaning）**：语言形式与语言外部世界（包括说话者意图、真实世界事物或抽象概念）之间的联系。要实现真正的语言理解，需要了解语言如何映射到外部的实体或意图。
3. **交际意图（Communicative Intent）**：人类语言的使用是为了实现某种目的，例如传递信息、请求帮助、表达态度等。如果模型无法接触到真实的意图及外部环境，那么就无法真正学会语言在这些情境下的“含义”。

---

## 三、主要论点 (Main Arguments)

作者的核心主张可以概括如下：

1. **无法从纯粹的形式中学习意义**  
   语言模型若只接触到大量文本（形式），而缺乏对外部现实世界或说话者真实意图的接触，就无法学到语言真正的含义。换言之，只有通过文本分布本身，模型无法知道单词或句子到底指涉什么、更不知道人类通过这些句子想要达成什么交流目的。

2. **与“人类语言习得”类比：孩子不会只靠被动看电视就掌握语言**  
   人类儿童的语言习得离不开与现实环境的互动、与照料者共同关注（joint attention）的场景——尤其是当成人和孩子共同指向某个外部对象并给出语言标签时，孩子才会逐步建立起词汇与真实世界之间的映射关系。单纯模仿表层文本，无法习得真正的语义。

3. **作者区分了“形式”与“语用层面”的巧妙现象**  
   例如，在他们提出的“章鱼测试（Octopus Test）”里，一只章鱼只是在海底偷窥到两个人类的电报对话（只看到文本、从未见过对话中提到的真实事物），因此它虽然可以用统计手段预测“这句话之后往往出现那句话”，但它无法真正知道对方说的 “coconut（椰子）” 或 “rope（绳子）” 在真实世界中的指代物。若真让它去完成与椰子或绳子相关的任务，它就暴露了不具备真实理解的问题。

4. **对 NLP 社区在任务设计和结论解读方面的建议**  
   - 要保持对语言复杂性的敬畏。  
   - 要识别当前很多基准任务（benchmark）可能还不足以真正考查模型是否具有“语义”或“理解”能力。  
   - 要借助更真实的、包含外部世界或人机交互信息的任务去评估模型是否真正掌握语义。  
   - 对模型超出预期的高分要保持警惕，深入分析是否利用了数据偏差或表层分布特征，而非真正的语言推理或理解。

---

## 四、“章鱼测试”（Octopus Test）及其它核心例子 (Key Thought Experiments)

### 英文
- **The Octopus Test**  
  Two humans, A and B, stranded on islands, exchange messages via telegraph. An octopus, O, taps into the telegraph line and reads all their text but never sees the actual world (coconuts, rope, catapult, etc.). O can predict and mimic likely responses based on statistical patterns. But if A suddenly asks something that requires real-world knowledge or grounding (e.g., “Help, I’m being chased by a bear; how do I build a weapon?”), O cannot truly help. It has no meaning-grounded knowledge about “bears,” “weapons,” or “sticks.” This illustrates how form alone (text strings) is not sufficient to guarantee true understanding.

- **Java Programming Analogy**  
  If one only trains on a huge repository of Java code without ever compiling or running it (i.e., no input-output or runtime errors as feedback), the system may predict next lines of code statistically but cannot truly “execute” code or know what it does. Form-based data alone does not teach the semantics (meaning) of the code.

### 中文
- **“章鱼测试”**  
  岛上两个人 A 和 B 通过电报交流，他们的文字对话被海底的聪明章鱼 O 窃听到。章鱼仅能观测到文字流，却不知道这些文字在现实世界里指向什么。虽然章鱼可以学会在某些句子出现后用最常见的回答来“应对”，但真正涉及到现实场景操作（如帮 A 赶走熊、或者搭建椰子弹射器）时，它无法执行，也无法真的理解文本背后的意图。这表明仅靠纯文本训练无法获得对语言的真实语义理解。

- **Java 程序类比**  
  若只看他人写的 Java 代码，而从不实际编译或运行，更没有配套输入、输出或错误信息，就只能猜测 “某段代码之后通常会接这一段”，但无法知道这段代码实际做了什么、将如何影响程序的输入输出，也就无法真正理解“程序的含义”。

---

## 五、对分布式语义（distributional semantics）与常见反驳的回应 (Distributional Semantics and Counterarguments)

### 英文
- **Distributional semantics** has long recognized the gap between co-occurrence patterns in text and genuine grounding in the real world. Although word embeddings capture useful statistics about lexical similarity, they do not, by themselves, link “rope” to an actual physical rope in a real scenario.
- **Human language acquisition** involves more than passive text ingestion: children learn language through joint attention and real-world interactions. The “meaning is use” idea (often linked to Wittgenstein) emphasizes that the “use” is anchored in real-world, social, or mental contexts — not merely text co-occurrence.
- **Counterarguments**: 
  - “Aren’t big LMs discovering semantics on their own?” The authors would say these models discover certain *reflections* of meaning (e.g., syntactic or lexical pattern regularities) that *correlate* with meaning, but that does not constitute full grounding or real comprehension.
  - “But we have huge data!” Regardless of size, if the data is purely text (form), it omits the essential connection to real-world referents and speaker intentions.

### 中文
- **分布式语义**：该领域一直知道，仅从文本共现无法获得真正的世界指代或人类意图的 grounding。词向量确实可以学到相似词之间的统计关联，但并不会自动让模型知道词和现实世界的对应关系。
- **人类语言习得**：儿童学习语言不只是“被动”地听大量语音或看大量文字；他们通过与看护人的交互、共同注意到现实中的具体对象，并获得对这些词指向什么、以及说话人意图如何的感知，从而学会语言的含义。
- **常见反驳与作者回应**：  
  1. “大型语言模型不是也在无监督下学到了很多词义吗？”  
     - 作者认为，这只是学到了一些统计共现的规律，可能与语义有对应，但并不是完整的概念理解或者与世界的关联。  
  2. “只要数据足够大就能学到一切？”  
     - 作者则强调，无论数据规模多大，如果数据仅限文本本身，没有提供外部世界的信息或与意图相关的交互信号，就依旧无法学到完整语义。

---

## 六、对 NLP 研究与任务设计的启示 (Implications for NLP Research and Task Design)

### 英文
1. **Avoid Overclaims**  
   Researchers and practitioners should avoid describing purely text-trained models as “understanding language” or “learning meaning” in the human sense. Precise language fosters clarity in research reporting and reduces media hype.
2. **Design More Grounded Tasks**  
   If the goal is genuine understanding, tasks should include real-world grounding (e.g., images, sensor data, or interactive dialogue that gives feedback). Simply scaling up text is insufficient.
3. **Guard Against Data Artifacts**  
   Many current benchmarks (NLI, QA, etc.) can be exploited by superficial cues in training data. We need more robust evaluations that ensure models cannot rely purely on easy artifacts.
4. **Be Skeptical of “Too Good to Be True”**  
   When large LMs achieve near human-level performance, thorough error analysis should verify that the model is not simply exploiting spurious correlations.
5. **The Right Hill Problem**  
   There is a risk that the field is climbing a local hill (improving on certain benchmarks) rather than the taller peak (achieving genuine natural language understanding). A top-down perspective (asking “What does it really mean to understand?”) can help the community steer research more effectively.

### 中文
1. **避免夸大模型的“理解能力”**  
   仅仅在文本分布上训练的模型不具备对语言的真正理解；我们在学术论文或媒体宣传中应谨慎用词，避免误导。
2. **设计更具“现实 grounding”的任务**  
   若研究目标是让机器获得类人理解能力，就应在任务和数据中融入对外部世界或社交互动信息的映射。单纯把文本规模扩大并不足以解决问题。
3. **警惕数据偏差及表面特征**  
   很多基准测试（如部分问答或自然语言推断数据集）可能存在一些可被机器投机利用的统计偏差，并不能真正衡量模型的“语义”或“推理”水平。
4. **对异常高分保持怀疑**  
   当模型性能看似“媲美人类”时，应做深入分析，看其是否利用了一些巧合关联或“伪信号”。
5. **攀登正确的山峰**  
   论文最后用“爬山”的比喻：当前的快速进展可能只是沿着一座“局部山峰”快速攀升，但不代表离“真正的自然语言理解”之巅越来越近。需要自上而下地问：“我们真正要的语言理解是什么？当前的努力是否有效帮助我们达成这个终极目标？”

---

## 七、结论 (Conclusion)

### 英文
Bender and Koller conclude by emphasizing that we must distinguish between “form” (what we see in text) and “meaning” (the communicative intent tied to the external world). Large-scale LMs have given NLP unprecedented progress on many tasks, but we should interpret this progress with a keen awareness of what these models can and cannot do. Merely observing linguistic form—even in massive quantities—cannot grant a model true grounding or real human-like understanding. Therefore, building tasks that embed language in real or simulated contexts, carefully evaluating models for robust, meaningful comprehension, and maintaining conceptual clarity are vital steps forward.

### 中文
作者在结语中再次强调，“语言形式”与“语言意义”是截然不同的层面，后者需要与外部世界或说话者的意图相联系。尽管当代的大型语言模型取得了惊人的进展，但我们必须清楚地认识到这些模型的局限：依赖“形式”本身无法带来“意义”的真正掌握。为此，学界需要设计更能检验真实语义理解的任务和评估标准，并在研究和传播中更严谨地使用“理解”或“意义”等术语，避免助长对 AI 的不切实际的炒作。

---

## 参考与延伸阅读 (References and Further Reading)

- Bender, E. M., & Koller, A. (2020). **Climbing towards NLU: On meaning, form, and understanding in the age of data**. In *Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics*, pages 5185–5198.  
- Harnad, S. (1990). “The symbol grounding problem.” *Physica D* 42:335–346.  
- Searle, J. (1980). “Minds, brains, and programs.” *Behavioral and Brain Sciences*, 3(3):417–457.  
- Wittgenstein, L. (1953). *Philosophical Investigations*. MacMillan, New York.

---

## 总结 (Overall Summary)

- **核心观点**：大规模语言模型只依赖文本的形式信息进行训练，并不能真正获得语言在现实世界或交际场景中的“意义”；换言之，表层形式训练不足以学习到“理解”。
- **主要论据**：通过“章鱼测试”、编程语言类比、人类婴儿语言习得等案例，作者反复阐明：模型缺乏外部世界或使用者意图的信息时，就无法把握词句真正所指的客观与主观意义。
- **对于学界与工业界的建议**：
  - 需更严格地界定“理解”“意义”等术语，避免夸大模型能力。
  - 鼓励设计包含真实环境交互、情境化、多模态或带有反馈信号的研究任务，从而帮助模型真正朝“语义理解”迈进。

这篇论文在当时（2020 年）的研究热点背景下，对如何看待大型语言模型的能力做出了非常清晰的理论阐释，并提出了深入思考和进一步研究方向。它提醒我们在研究与应用中要格外谨慎，在宣称“机器理解”或“获得语言意义”时，一定要先明确模型是否真正具备了外部世界的感知或意图推断能力，否则只会在“形式”层面停留并产生误解或炒作。