# Vector Space Models of Lexical Meaning 2015a


# On vector space models:  å…³äºå‘é‡ç©ºé—´æ¨¡å‹ï¼š

* What **notion of meaning** is represented by distributional representations? åˆ†å¸ƒè¡¨ç¤ºä»£è¡¨äº†ä»€ä¹ˆæ„ä¹‰æ¦‚å¿µï¼Ÿ

- **Semantic similarity or relatedness**: Words with similar distributions tend to have similar meanings (e.g., *cat* and *dog* may appear in similar contexts like "pet", "animal", "feed").
- **Contextual meaning**: A word's vector reflects the kinds of contexts in which it appears, providing a form of **empirical grounding** for lexical meaning.
- **Usage-based meaning**: Meaning is **emergent** from patterns of **language use**, rather than being pre-defined in a lexicon or ontology.

 **Statistical, empirical, and usage-based**, capturing **how words are used in context** to approximate **their semantic properties**.


* What semantic relations do they capture? å®ƒä»¬æ•æ‰åˆ°äº†ä»€ä¹ˆè¯­ä¹‰å…³ç³»ï¼Ÿ
* How do these relate to the semantic relations we intuitively recognise in natural language? è¿™äº›ä¸æˆ‘ä»¬åœ¨è‡ªç„¶è¯­è¨€ä¸­ç›´è§‚è¯†åˆ«çš„è¯­ä¹‰å…³ç³»æœ‰ä½•å…³ç³»ï¼Ÿ
* Are there relations that they do not capture? æ˜¯å¦å­˜åœ¨ä»–ä»¬æ²¡æœ‰æ•æ‰åˆ°çš„å…³ç³»ï¼Ÿ
  - multi-sets in which the frequency of words is accounted for, but the order of words is notï¼Œ not syntactic structures
  - extre case, some meaningles word such as `the`
  - retriew or seprate documents from non-relevant ones, useless

- **Compositional logic-based meaning**: VSMs don't directly encode truth-conditional or logical meaning (e.g., "All men are mortal").
- **Word sense disambiguation** (unless explicitly modeled): A single vector might mix multiple senses unless contextualized (e.g., "bank" as riverbank vs financial institution).
- **Hierarchical or ontological relationshipsæ˜ç¡®çš„æœ¬ä½“å…³ç³»** (unless inferred): Like "is-a" relationships in WordNet.

* Think of examples in natural language that can modelled well with distributional relations and examples that cannot be. æƒ³æƒ³è‡ªç„¶è¯­è¨€ä¸­å¯ä»¥å¾ˆå¥½åœ°ç”¨åˆ†å¸ƒå…³ç³»å»ºæ¨¡çš„ä¾‹å­ä»¥åŠä¸èƒ½ç”¨åˆ†å¸ƒå…³ç³»å»ºæ¨¡çš„ä¾‹å­ã€‚
* How does this notion of meaning different from that taken in model-theoretic semantics that we looked at earlier? è¿™ç§æ„ä¹‰æ¦‚å¿µä¸æˆ‘ä»¬ä¹‹å‰ç ”ç©¶è¿‡çš„æ¨¡å‹è®ºè¯­ä¹‰å­¦ä¸­çš„æ„ä¹‰æ¦‚å¿µæœ‰ä½•ä¸åŒï¼Ÿ
* Sense and reference?  æ„ä¹‰å’Œå‚è€ƒï¼Ÿ
* What are the main ... for representing meaning of natural language this way? ä»¥è¿™ç§æ–¹å¼è¡¨ç¤ºè‡ªç„¶è¯­è¨€çš„å«ä¹‰çš„ä¸»è¦...æ˜¯ä»€ä¹ˆï¼Ÿ
* benefits  å¥½å¤„
* challenges  æŒ‘æˆ˜
* limitations (and dangers!) å±€é™æ€§ï¼ˆå’Œå±é™©ï¼ï¼‰
* What computational resources, tools and methods do we use to create these representations? æˆ‘ä»¬ä½¿ç”¨ä»€ä¹ˆè®¡ç®—èµ„æºã€å·¥å…·å’Œæ–¹æ³•æ¥åˆ›å»ºè¿™äº›è¡¨ç¤ºï¼Ÿ
* For what tasks can we use these representations? For what tasks we cannot use them? æˆ‘ä»¬å¯ä»¥åœ¨å“ªäº›ä»»åŠ¡ä¸­ä½¿ç”¨è¿™äº›è¡¨å¾ï¼Ÿåœ¨å“ªäº›ä»»åŠ¡ä¸­æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨å®ƒä»¬ï¼Ÿ
* What would be alternative representations? æ›¿ä»£è¡¨ç°å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ


---

## ğŸ§  **1. Think of examples in natural language that can be modelled well with distributional relations and examples that cannot be.**  
**æƒ³æƒ³è‡ªç„¶è¯­è¨€ä¸­å¯ä»¥å¾ˆå¥½åœ°ç”¨åˆ†å¸ƒå…³ç³»å»ºæ¨¡çš„ä¾‹å­ï¼Œä»¥åŠä¸èƒ½ç”¨åˆ†å¸ƒå…³ç³»å»ºæ¨¡çš„ä¾‹å­ã€‚**

- âœ… **Can be modeled wellï¼ˆå¯ä»¥å¾ˆå¥½å»ºæ¨¡ï¼‰ï¼š**  
  - Synonymy (åŒä¹‰è¯): *happy* â‰ˆ *joyful*  
  - Semantic similarity (è¯­ä¹‰ç›¸ä¼¼): *cat* and *dog* appear in similar contexts  
  - Sentiment polarity (æƒ…æ„Ÿææ€§): *great* vs *terrible*

- âŒ **Cannot be modeled wellï¼ˆéš¾ä»¥å»ºæ¨¡ï¼‰ï¼š**  
  - Logical entailment (é€»è¾‘è•´å«): e.g. "All dogs are animals"  
  - Truth-conditional meaning (çœŸå€¼æ¡ä»¶æ„ä¹‰)  
  - Deictic or referential expressions (æŒ‡ç§°è¯ï¼Œå¦‚ "here", "she")

---

## ğŸ”„ **2. How does this notion of meaning differ from that taken in model-theoretic semantics?**  
**è¿™ç§æ„ä¹‰æ¦‚å¿µä¸æˆ‘ä»¬ä¹‹å‰ç ”ç©¶è¿‡çš„æ¨¡å‹è®ºè¯­ä¹‰ä¸­çš„æ„ä¹‰æ¦‚å¿µæœ‰ä½•ä¸åŒï¼Ÿ**

- ğŸ“¦ **Distributional semanticsï¼ˆåˆ†å¸ƒå¼è¯­ä¹‰ï¼‰ï¼š**  
  - Based on usage patterns (åŸºäºä½¿ç”¨æ¨¡å¼)  
  - Represents meaning as vectors (ç”¨å‘é‡è¡¨ç¤ºæ„ä¹‰)  
  - Captures similarity but not logical structure (è¡¨è¾¾ç›¸ä¼¼æ€§ä½†ä¸è¡¨è¾¾é€»è¾‘ç»“æ„)

- ğŸ“ **Model-theoretic semanticsï¼ˆæ¨¡å‹è®ºè¯­ä¹‰ï¼‰ï¼š**  
  - Based on logic and truth values (åŸºäºé€»è¾‘å’ŒçœŸå€¼)  
  - Meaning is compositional and interpretable in a model (æ„ä¹‰å¯ç»„åˆå¹¶å…·æœ‰å¯è§£é‡Šæ€§)  
  - Better for formal reasoning (é€‚ç”¨äºå½¢å¼æ¨ç†)

---

## ğŸ§­ **3. Sense and reference?**  
**æ„ä¹‰å’ŒæŒ‡ç§°ï¼Ÿ**

- â€œSenseâ€ï¼ˆæ„ä¹‰ï¼‰ï¼šè¯çš„æ¦‚å¿µæˆ–è®¤çŸ¥å†…å®¹  
- â€œReferenceâ€ï¼ˆæŒ‡ç§°ï¼‰ï¼šè¯å®é™…æŒ‡ä»£çš„å¯¹è±¡  
- åˆ†å¸ƒå¼è¯­ä¹‰ä¸»è¦å»ºæ¨¡â€œæ„ä¹‰â€ï¼Œå¾ˆéš¾å¤„ç†å…·ä½“çš„â€œæŒ‡ç§°â€

---

## ğŸ¯ **4. What are the main benefits, challenges, limitations (and dangers!) of representing meaning this way?**  
**ä»¥è¿™ç§æ–¹å¼è¡¨ç¤ºè‡ªç„¶è¯­è¨€çš„å«ä¹‰çš„ä¸»è¦å¥½å¤„ã€æŒ‘æˆ˜å’Œå±€é™ï¼ˆå’Œå±é™©ï¼‰æ˜¯ä»€ä¹ˆï¼Ÿ**

### âœ… Benefits å¥½å¤„
- Data-driven, automatic learningï¼ˆæ•°æ®é©±åŠ¨ï¼Œå¯è‡ªåŠ¨å­¦ä¹ ï¼‰  
- Scalable to large corporaï¼ˆé€‚åˆå¤§è§„æ¨¡è¯­æ–™ï¼‰  
- Effective in many NLP tasksï¼ˆåœ¨è®¸å¤šè‡ªç„¶è¯­è¨€å¤„ç†ä»»åŠ¡ä¸­æ•ˆæœå¥½ï¼‰

### âš ï¸ Challenges æŒ‘æˆ˜
- Lacks logical compositionalityï¼ˆç¼ºä¹é€»è¾‘ç»„åˆèƒ½åŠ›ï¼‰  
- Difficult to handle polysemyï¼ˆéš¾ä»¥å¤„ç†ä¸€è¯å¤šä¹‰ï¼‰  
- Sensitive to corpus biasï¼ˆå®¹æ˜“å—è¯­æ–™åè§å½±å“ï¼‰

### â— Limitations & dangers å±€é™æ€§ä¸é£é™©
- Canâ€™t reason logicallyï¼ˆä¸èƒ½è¿›è¡Œé€»è¾‘æ¨ç†ï¼‰  
- Meaning not interpretableï¼ˆç¼ºä¹å¯è§£é‡Šæ€§ï¼‰  
- May reflect social biasï¼ˆå¯èƒ½åæ˜ ç¤¾ä¼šåè§ï¼‰

---

## ğŸ›  **5. What computational resources, tools and methods do we use to create these representations?**  
**æˆ‘ä»¬ä½¿ç”¨ä»€ä¹ˆè®¡ç®—èµ„æºã€å·¥å…·å’Œæ–¹æ³•æ¥åˆ›å»ºè¿™äº›è¡¨ç¤ºï¼Ÿ**

- **Resourcesï¼ˆèµ„æºï¼‰**: Wikipedia, news corpora, Common Crawl  
- **Toolsï¼ˆå·¥å…·ï¼‰**: Word2Vec, GloVe, BERT, FastText  
- **Methodsï¼ˆæ–¹æ³•ï¼‰**: Co-occurrence matrix, neural embeddings, dimensionality reduction

---

## ğŸ’¼ **6. For what tasks can we use these representations? For what tasks we cannot use them?**  
**æˆ‘ä»¬å¯ä»¥åœ¨å“ªäº›ä»»åŠ¡ä¸­ä½¿ç”¨è¿™äº›è¡¨å¾ï¼Ÿåœ¨å“ªäº›ä»»åŠ¡ä¸­æˆ‘ä»¬ä¸èƒ½ä½¿ç”¨å®ƒä»¬ï¼Ÿ**

- âœ… **Can use inï¼ˆå¯ä»¥ä½¿ç”¨äºï¼‰ï¼š**  
  - Information retrievalï¼ˆä¿¡æ¯æ£€ç´¢ï¼‰  
  - Text classificationï¼ˆæ–‡æœ¬åˆ†ç±»ï¼‰  
  - Machine translationï¼ˆæœºå™¨ç¿»è¯‘ï¼‰  
  - Named entity recognitionï¼ˆå‘½åå®ä½“è¯†åˆ«ï¼‰

- âŒ **Cannot use inï¼ˆä¸é€‚åˆç”¨äºï¼‰ï¼š**  
  - Logical inferenceï¼ˆé€»è¾‘æ¨ç†ï¼‰  
  - Formal semanticsï¼ˆå½¢å¼è¯­ä¹‰åˆ†æï¼‰  
  - Context-sensitive truth conditionsï¼ˆä¸Šä¸‹æ–‡æ•æ„Ÿçš„çœŸå€¼è¯­ä¹‰ï¼‰

---

## ğŸ” **7. What would be alternative representations?**  
**æ›¿ä»£è¡¨ç°å½¢å¼æ˜¯ä»€ä¹ˆï¼Ÿ**

- Model-theoretic semanticsï¼ˆæ¨¡å‹è®ºè¯­ä¹‰ï¼‰  
- Frame semantics / FrameNetï¼ˆæ¡†æ¶è¯­ä¹‰ï¼‰  
- Logical form representationsï¼ˆé€»è¾‘å½¢å¼è¡¨ç¤ºï¼‰  
- DisCoCat / Compositional Distributional Semanticsï¼ˆç»„åˆå¼åˆ†å¸ƒè¯­ä¹‰ï¼‰  
- Knowledge graph representationsï¼ˆçŸ¥è¯†å›¾è°±ï¼‰

 


# On compositionality:  å…³äºç»„åˆæ€§ï¼š

* What are the reasons and benefits of combining formal representations with distributional ones? å°†å½¢å¼åŒ–è¡¨ç¤ºä¸åˆ†å¸ƒå¼è¡¨ç¤ºç›¸ç»“åˆçš„åŸå› å’Œå¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ
* What do you think are the biggest challenges of such hybrid models and representations? æ‚¨è®¤ä¸ºè¿™ç§æ··åˆæ¨¡å‹å’Œè¡¨ç¤ºå½¢å¼é¢ä¸´çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ
* To what degree can we interpret distributional representations? æˆ‘ä»¬èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šè§£é‡Šåˆ†å¸ƒè¡¨ç¤ºï¼Ÿ
* How does this relate to how well a mapping between two types of representations can be achieved?è¿™ä¸ä¸¤ç§ç±»å‹çš„è¡¨ç¤ºä¹‹é—´çš„æ˜ å°„å®ç°ç¨‹åº¦æœ‰ä½•å…³ç³»ï¼Ÿ
* There are several different ways to write a formal grammar. How would this affect the mapping? ç¼–å†™å½¢å¼è¯­æ³•æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ã€‚è¿™ä¼šå¦‚ä½•å½±å“æ˜ å°„ï¼Ÿ

---

### ğŸ§© 1. What are the reasons and benefits of combining formal representations with distributional ones?  
**å°†å½¢å¼åŒ–è¡¨ç¤ºä¸åˆ†å¸ƒå¼è¡¨ç¤ºç›¸ç»“åˆçš„åŸå› å’Œå¥½å¤„æ˜¯ä»€ä¹ˆï¼Ÿ**


- Formal semantics offers compositionality and logical reasoning.
- Distributional semantics offers empirical, data-driven meaning grounded in language use.
- Combining them can:
  - Enable both *logical inference* and *semantic similarity*
  - Improve natural language understanding in NLP systems
  - Represent meaning more flexibly yet with structure


- å½¢å¼è¯­ä¹‰å…·æœ‰ç»„åˆæ€§å’Œé€»è¾‘æ¨ç†èƒ½åŠ›  
- åˆ†å¸ƒå¼è¯­ä¹‰èƒ½å¤Ÿä»å¤§è§„æ¨¡è¯­æ–™ä¸­è·å–è¯­ä¹‰ç›¸ä¼¼æ€§  
- äºŒè€…ç»“åˆå¯ä»¥ï¼š
  - å…¼é¡¾â€œæ¨ç†èƒ½åŠ›â€å’Œâ€œè¯­ä¹‰ç›¸å…³æ€§â€  
  - æé«˜è‡ªç„¶è¯­è¨€å¤„ç†ç³»ç»Ÿçš„è¡¨ç°  
  - æ„å»ºæ›´çµæ´»ä¸”ç»“æ„åŒ–çš„è¯­ä¹‰æ¨¡å‹

---

### ğŸš§ 2. What do you think are the biggest challenges of such hybrid models and representations?  
**æ‚¨è®¤ä¸ºè¿™ç§æ··åˆæ¨¡å‹å’Œè¡¨ç¤ºå½¢å¼é¢ä¸´çš„æœ€å¤§æŒ‘æˆ˜æ˜¯ä»€ä¹ˆï¼Ÿ**

- Aligning symbolic and vector spaces is non-trivial  
- Mapping logical forms to continuous space is hard  
- Lack of interpretability in vector models  
- Sparse annotated data for training such hybrids

- å°†ç¬¦å·ç»“æ„ä¸å‘é‡ç©ºé—´å¯¹é½éå¸¸å›°éš¾  
- ä»é€»è¾‘å½¢å¼æ˜ å°„åˆ°è¿ç»­ç©ºé—´è¾ƒå¤æ‚  
- å‘é‡æ¨¡å‹ç¼ºä¹å¯è§£é‡Šæ€§  
- ç¼ºä¹é«˜è´¨é‡è¯­ä¹‰ç»„åˆæ•°æ®è¿›è¡Œè®­ç»ƒ

---

### ğŸ” 3. To what degree can we interpret distributional representations?  
**æˆ‘ä»¬èƒ½åœ¨å¤šå¤§ç¨‹åº¦ä¸Šè§£é‡Šåˆ†å¸ƒè¡¨ç¤ºï¼Ÿ**


- Interpretation is possible via:
  - Nearest neighbors (semantic relatedness)
  - Dimension labeling (if dimensions are interpretable)
- But:
  - Most vectors are opaque (â€œblack-boxâ€)
  - No clear mapping to symbolic logic


- å¯è§£é‡Šæ€§æ–¹å¼åŒ…æ‹¬ï¼š
  - è§‚å¯Ÿç›¸è¿‘è¯ï¼ˆè¯­ä¹‰é‚»è¿‘ï¼‰  
  - åˆ†æç‰¹å¾ç»´åº¦ï¼ˆå¦‚å·²çŸ¥ç»´åº¦å«ä¹‰ï¼‰  
- ä½†é—®é¢˜åœ¨äºï¼š
  - å¤šæ•°å‘é‡æ˜¯é»‘ç®±  
  - æ— æ³•ç›´æ¥å¯¹åº”é€»è¾‘ç»“æ„

---

### ğŸ”— 4. How does this relate to how well a mapping between two types of representations can be achieved?  
**è¿™ä¸ä¸¤ç§ç±»å‹çš„è¡¨ç¤ºä¹‹é—´çš„æ˜ å°„å®ç°ç¨‹åº¦æœ‰ä½•å…³ç³»ï¼Ÿ**

- The quality of the mapping depends on:
  - Grammar formalism used
  - Availability of parallel symbolic-distributional data
  - Structural similarity between models

- æ˜ å°„çš„å¯å®ç°ç¨‹åº¦å–å†³äºï¼š
  - æ‰€é€‰è¯­æ³•å½¢å¼ï¼ˆå¦‚CCGã€LFGï¼‰  
  - æ˜¯å¦æœ‰å¯¹é½çš„æ•°æ®ï¼ˆç¬¦å·-å‘é‡ï¼‰  
  - ä¸¤ç§è¡¨ç¤ºç»“æ„çš„ä¸€è‡´æ€§

---

### âœï¸ 5. There are several different ways to write a formal grammar. How would this affect the mapping?  
**ç¼–å†™å½¢å¼è¯­æ³•æœ‰å‡ ç§ä¸åŒçš„æ–¹æ³•ã€‚è¿™ä¼šå¦‚ä½•å½±å“æ˜ å°„ï¼Ÿ**

- Different grammars provide different levels of syntactic detail
- More fine-grained grammars (like CCG) enable more precise mappings
- Simpler grammars may lose semantic alignment

- ä¸åŒè¯­æ³•æè¿°æä¾›ä¸åŒç¨‹åº¦çš„å¥æ³•ç»“æ„  
- ç²¾ç»†çš„è¯­æ³•ï¼ˆå¦‚CCGï¼‰æœ‰åŠ©äºæ›´ç²¾ç¡®åœ°æ˜ å°„åˆ°è¯­ä¹‰ç©ºé—´  
- ç®€åŒ–è¯­æ³•å¯èƒ½æŸå¤±è¯­ä¹‰ç»†èŠ‚ï¼Œé™ä½æ˜ å°„è´¨é‡



# Compositional Vector Space Modelsï¼ˆç»„åˆå¼å‘é‡ç©ºé—´æ¨¡å‹ï¼‰
## ğŸ§  ä¸€ã€ä»€ä¹ˆæ˜¯ Compositional Vector Space Models?

ç»„åˆå¼å‘é‡ç©ºé—´æ¨¡å‹è¯•å›¾è§£å†³è¿™æ ·ä¸€ä¸ªé—®é¢˜ï¼š

> **å¦‚æœæˆ‘ä»¬å·²ç»æœ‰äº†è¯çš„å‘é‡è¡¨ç¤ºï¼Œå¦‚ä½•ç»„åˆå®ƒä»¬æ¥è¡¨ç¤ºçŸ­è¯­æˆ–å¥å­çš„æ„ä¹‰ï¼Ÿ**

è¿™å¯¹åº”çš„æ˜¯è‡ªç„¶è¯­è¨€è¯­ä¹‰ä¸­çš„â€œ**æ„ä¹‰çš„ç»„åˆæ€§ï¼ˆcompositionalityï¼‰**â€åŸåˆ™ï¼š  
> å¥å­çš„æ„ä¹‰æ˜¯å…¶ç»„æˆéƒ¨åˆ†ï¼ˆè¯ï¼‰çš„æ„ä¹‰ä»¥åŠå®ƒä»¬ç»„åˆæ–¹å¼çš„å‡½æ•°ã€‚

---

## ğŸ“ äºŒã€æ¨¡å‹åŠ¨æœºï¼šå…‹æœè¯è¢‹æ¨¡å‹çš„å±€é™

ä¼ ç»Ÿçš„å‘é‡ç©ºé—´æ¨¡å‹ï¼ˆå¦‚è¯è¢‹æ¨¡å‹ï¼‰ä¸èƒ½å¤„ç†**è¯åº**ã€**å¥æ³•ç»“æ„**å’Œ**å¥å­å±‚çº§çš„æ„ä¹‰å·®å¼‚**ã€‚  
ä¾‹å¦‚ï¼š

- "dog bites man" ä¸ "man bites dog" åœ¨è¯è¢‹æ¨¡å‹ä¸­æ˜¯ä¸€æ ·çš„ï¼Œä½†å®ƒä»¬çš„è¯­ä¹‰å®Œå…¨ä¸åŒã€‚

å› æ­¤ï¼Œéœ€è¦èƒ½å¤Ÿç»„åˆè¯å‘é‡çš„æœºåˆ¶ï¼Œä½¿æ¨¡å‹èƒ½æ•æ‰**ç»“æ„ä¿¡æ¯ä¸è¯­åºä¿¡æ¯**ã€‚

---

## ğŸ§° ä¸‰ã€ä¸»è¦æ–¹æ³•ä»‹ç»

### 1. **å‘é‡åŠ æ³• / å¹³å‡æ³•ï¼ˆVector Addition / Averagingï¼‰**

- æœ€ç®€å•çš„ç»„åˆæ–¹æ³•ï¼šå°†è¯å‘é‡ç›¸åŠ æˆ–å–å¹³å‡
- ä¾‹å­ï¼š  
  > å¥å­å‘é‡ = å‘é‡("the") + å‘é‡("dog") + å‘é‡("barks")

- **ä¼˜ç‚¹**ï¼šè®¡ç®—ç®€å•ã€æœ‰æ•ˆ  
- **ç¼ºç‚¹**ï¼šå¿½ç•¥è¯­æ³•ç»“æ„å’Œè¯åºï¼Œæ•ˆæœæœ‰é™

---

### 2. **ä¹˜æ³•æ¨¡å‹ï¼ˆPointwise Multiplicationï¼‰**

- å‘é‡ä¹‹é—´æŒ‰å…ƒç´ ç›¸ä¹˜ï¼ˆHadamard productï¼‰  
- ä¼šçªå‡ºå…±åŒç‰¹å¾ã€å‹åˆ¶æ— å…³ç‰¹å¾  
- **ç¼ºç‚¹**ï¼šå’ŒåŠ æ³•ä¸€æ ·ï¼Œç¼ºä¹ç»“æ„æ•æ„Ÿæ€§

---

### 3. **çº¿æ€§æ˜ å°„ / å‡½æ•°æ¨¡å‹ï¼ˆMatrix-Vector Modelï¼‰**

- è®©åŠ¨è¯ã€ä»‹è¯ç­‰åŠŸèƒ½è¯ä½œä¸º**å˜æ¢å‡½æ•°ï¼ˆfunctionï¼‰**ï¼Œåè¯ç­‰ä½œä¸º**å‚æ•°ï¼ˆargumentï¼‰**
- ä¾‹å­ï¼šåŠ¨è¯æ˜¯ä¸€ä¸ªçŸ©é˜µ \( M_{\text{eat}} \)ï¼Œåè¯â€œappleâ€æ˜¯ä¸€ä¸ªå‘é‡ \( \vec{a} \)ï¼Œåˆ™ï¼š
  > åƒè‹¹æœ = \( M_{\text{eat}} \cdot \vec{a} \)

- ç±»ä¼¼å‡½æ•°è¯­ä¹‰å­¦ä¸­çš„ç»„åˆï¼š**å‡½æ•° + å‚æ•° â†’ è¯­ä¹‰ç»“æœ**

---

### 4. **å¼ é‡æ¨¡å‹ï¼ˆTensor-based Compositionï¼‰**

- æ›´å¤æ‚çš„æ¨¡å‹ï¼šè®©ä¸åŒè¯ç±»ä½¿ç”¨é«˜é˜¶å¼ é‡ï¼ˆtensorï¼‰è¡¨ç¤º
- åè¯ç”¨å‘é‡ï¼ŒåŠ¨è¯ç”¨çŸ©é˜µæˆ–ä¸‰é˜¶å¼ é‡ï¼ˆtensorï¼‰
- ä¾‹å¦‚ï¼ŒåŠç‰©åŠ¨è¯å¯è¡¨ç¤ºä¸ºä¸‰é˜¶å¼ é‡ \( T_{\text{eat}} \)ï¼Œä¸ä¸»è¯­/å®¾è¯­çš„å‘é‡ç»„åˆå¾—åˆ°å¥å­è¡¨ç¤º

- **ç†è®ºåŸºç¡€ï¼š**  
  Clark ä¸ Coeckeã€Sadrzadeh æå‡ºçš„ä¸€ç§ç§°ä¸º **DisCoCat** çš„æ¨¡å‹ï¼ˆDistributional Compositional Categorical Semanticsï¼‰ï¼Œèåˆäº†ï¼š
  - è¯­æ³•ç»“æ„ï¼ˆåŸºäº Lambek çš„é¢„ç¾¤è¯­æ³• pregroup grammarï¼‰
  - è¯­ä¹‰è¡¨ç¤ºï¼ˆåŸºäºå‘é‡ç©ºé—´ï¼‰
  - èŒƒç•´è®ºå·¥å…·ï¼ˆCategory Theoryï¼‰

---

## ğŸ“Š å››ã€ä¼˜ç‚¹ä¸æŒ‘æˆ˜

### âœ… ä¼˜ç‚¹ï¼š
- ä¿ç•™åˆ†å¸ƒå¼è¯­ä¹‰çš„ç»Ÿè®¡ä¼˜åŠ¿
- å¼•å…¥å¥æ³•ç»“æ„ä¿¡æ¯ï¼ˆç»„åˆæ€§ï¼‰
- å¯æ‹“å±•åˆ°æ›´é«˜å±‚çº§ï¼ˆä»è¯åˆ°å¥å­ï¼‰

### âš ï¸ æŒ‘æˆ˜ï¼š
- æ¨¡å‹å¤æ‚åº¦é«˜ï¼Œè®¡ç®—èµ„æºéœ€æ±‚å¤§
- éœ€è¦å¥æ³•åˆ†ææ”¯æŒï¼ˆå¦‚CCGæˆ–ä¾å­˜åˆ†æï¼‰
- å¼ é‡æ¨¡å‹è®­ç»ƒæ•°æ®è¦æ±‚é«˜ï¼Œç¨€ç–é—®é¢˜ä¸¥é‡
- ç†è®ºä¸Šä¼˜ç¾ï¼Œå®è·µä¸Šæ•ˆæœä¸ç¨³å®šï¼ˆç‰¹åˆ«æ˜¯é•¿å¥å­ï¼‰

---

## âœ¨ æ€»ç»“ä¸€å¥è¯ï¼š

> **Compositional Vector Space Models æ˜¯å°†è¯å‘é‡é€šè¿‡æ•°å­¦ç»“æ„ç»„åˆèµ·æ¥ï¼Œä»¥è¡¨è¾¾å¥å­è¯­ä¹‰çš„æ¨¡å‹ï¼Œæ˜¯è¿æ¥è¯ä¹‰ä¸å¥ä¹‰ä¹‹é—´çš„é‡è¦æ¡¥æ¢ã€‚**




# Vector Space Models of Lexical Meaning

### Content
**1. Thesis**
- ğŸ“ The paper explores the use of vector space models to represent lexical meaning.
- ğŸ“ It contrasts this geometric approach with traditional set-theoretic models.
  
 **2. Introduction**
- ğŸ“˜ Sets the stage for the shift from set-theoretic models to vector space models for representing natural language semantics.
  
**3. Distributional Methods in Linguistics**
- ğŸ“Š Describes how word meanings are derived from their contextual usage in large text corpora.
- ğŸ“Š Introduces the distributional hypothesis which states that "words that occur in similar contexts tend to have similar meanings."

**4. Vector Space Models for Document Retrieval**
- ğŸ” Discusses the application of vector space models in information retrieval, particularly in search engines.
- ğŸ” Highlights term frequency-inverse document frequency (tf-idf) and other weighting schemes.

**5. Representing Word Meanings as Vectors**
- ğŸ”¤ Details the construction of term-term matrices from corpora to derive word vectors.
- ğŸ”¤ Explores various methods to refine context definitions and weighting schemes to improve semantic accuracy.

**6. Approach**
- ğŸ›  Utilizes mathematical frameworks of vector spaces and linear algebra.
- ğŸ›  Leverages contexts of word occurrences in corpora for deriving high-dimensional semantic spaces.

**7. Experiments**
- ğŸ§ª Illustrates experiments conducted using large corpora, discussing the size and quality of data and impact on results.
- ğŸ§ª Provides examples of automatically extracted synonyms to validate the distributional models.

**8. Compositional Vector Space Models**
- ğŸ— Examines recent advancements in combining vectors for larger linguistic units such as phrases and sentences.
- ğŸ— Discusses frameworks to generate vectors for sentences from individual word vectors, enhancing language processing applications.

**9. Conclusion**
- âš– Summarizes the effectiveness of vector space models in capturing semantic relations in NLP.
- âš– Acknowledges challenges and future directions, including integrating traditional logic-based semantics.

### Summary
The document presents a comprehensive study on the use of vector space models for representing lexical meanings in computational linguistics. Beginning with the limitations of set-theoretic models, it introduces vector spaces as a way to capture semantic similarity through contextual usage in large corpora. The effectiveness of these models in various applications, such as information retrieval and automated synonym extraction, is highlighted through experiments. The paper also discusses advanced methods for combining word vectors into larger units of meaning like phrases and sentences, and the potential for integrating these models into broader NLP applications. The conclusion recognizes the robustness of vector space models while pointing out areas for future research.

### å†…å®¹
**1. è®ºæ–‡**
- ğŸ“ æœ¬æ–‡æ¢è®¨äº†å¦‚ä½•ä½¿ç”¨å‘é‡ç©ºé—´æ¨¡å‹æ¥è¡¨ç¤ºè¯æ±‡æ„ä¹‰ã€‚
- ğŸ“ æœ¬æ–‡å°†è¿™ç§å‡ ä½•æ–¹æ³•ä¸ä¼ ç»Ÿçš„é›†åˆè®ºæ¨¡å‹è¿›è¡Œäº†å¯¹æ¯”ã€‚

**2. å¼•è¨€**
- ğŸ“˜ ä¸ºä»é›†åˆè®ºæ¨¡å‹å‘å‘é‡ç©ºé—´æ¨¡å‹è½¬å˜ä»¥è¡¨ç¤ºè‡ªç„¶è¯­è¨€è¯­ä¹‰å¥ å®šäº†åŸºç¡€ã€‚

**3. è¯­è¨€å­¦ä¸­çš„åˆ†å¸ƒæ–¹æ³•**
- ğŸ“Š æè¿°äº†å¦‚ä½•åœ¨å¤§å‹æ–‡æœ¬è¯­æ–™åº“ä¸­æ ¹æ®è¯­å¢ƒæ¨å¯¼å‡ºè¯ä¹‰ã€‚
- ğŸ“Š ä»‹ç»åˆ†å¸ƒå‡è®¾ï¼Œå³â€œå‡ºç°åœ¨ç›¸ä¼¼è¯­å¢ƒä¸­çš„è¯è¯­å¾€å¾€å…·æœ‰ç›¸ä¼¼çš„å«ä¹‰â€ã€‚

**4. ç”¨äºæ–‡æ¡£æ£€ç´¢çš„å‘é‡ç©ºé—´æ¨¡å‹**
- ğŸ” è®¨è®ºå‘é‡ç©ºé—´æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢ï¼ˆå°¤å…¶æ˜¯åœ¨æœç´¢å¼•æ“ä¸­ï¼‰ä¸­çš„åº”ç”¨ã€‚
- ğŸ” é‡ç‚¹ä»‹ç»è¯é¢‘-é€†æ–‡æ¡£é¢‘ç‡ (TF-IDF) å’Œå…¶ä»–åŠ æƒæ–¹æ¡ˆã€‚

**5. å°†è¯ä¹‰è¡¨ç¤ºä¸ºå‘é‡**
- ğŸ”¤ è¯¦ç»†ä»‹ç»å¦‚ä½•ä»è¯­æ–™åº“æ„å»ºè¯-è¯çŸ©é˜µä»¥ç”Ÿæˆè¯å‘é‡ã€‚
- ğŸ”¤ æ¢ç´¢å„ç§æ”¹è¿›è¯­å¢ƒå®šä¹‰å’ŒåŠ æƒæ–¹æ¡ˆçš„æ–¹æ³•ï¼Œä»¥æé«˜è¯­ä¹‰å‡†ç¡®æ€§ã€‚

**6. æ–¹æ³•**
- ğŸ›  åˆ©ç”¨å‘é‡ç©ºé—´å’Œçº¿æ€§ä»£æ•°çš„æ•°å­¦æ¡†æ¶ã€‚
- ğŸ›  åˆ©ç”¨è¯­æ–™åº“ä¸­è¯è¯­å‡ºç°çš„è¯­å¢ƒæ¥å¯¼å‡ºé«˜ç»´è¯­ä¹‰ç©ºé—´ã€‚

**7. å®éªŒ**
- ğŸ§ª é˜è¿°ä½¿ç”¨å¤§å‹è¯­æ–™åº“è¿›è¡Œçš„å®éªŒï¼Œè®¨è®ºæ•°æ®çš„å¤§å°å’Œè´¨é‡åŠå…¶å¯¹ç»“æœçš„å½±å“ã€‚
- ğŸ§ª æä¾›è‡ªåŠ¨æå–åŒä¹‰è¯çš„ç¤ºä¾‹ï¼Œä»¥éªŒè¯åˆ†å¸ƒæ¨¡å‹ã€‚

**8. ç»„åˆå‘é‡ç©ºé—´æ¨¡å‹**
- ğŸ— æ¢è®¨äº†ç»„åˆå‘é‡ç”¨äºæ›´å¤§è¯­è¨€å•ä½ï¼ˆä¾‹å¦‚çŸ­è¯­å’Œå¥å­ï¼‰çš„æœ€æ–°è¿›å±•ã€‚
- ğŸ— è®¨è®ºäº†ä»å•ä¸ªè¯å‘é‡ç”Ÿæˆå¥å­å‘é‡çš„æ¡†æ¶ï¼Œä»¥å¢å¼ºè¯­è¨€å¤„ç†åº”ç”¨ã€‚

**9. ç»“è®º**
- âš– æ€»ç»“äº†å‘é‡ç©ºé—´æ¨¡å‹åœ¨è‡ªç„¶è¯­è¨€å¤„ç† (NLP) ä¸­æ•æ‰è¯­ä¹‰å…³ç³»çš„æœ‰æ•ˆæ€§ã€‚
- âš– æŒ‡å‡ºäº†æŒ‘æˆ˜å’Œæœªæ¥å‘å±•æ–¹å‘ï¼ŒåŒ…æ‹¬æ•´åˆä¼ ç»Ÿçš„åŸºäºé€»è¾‘çš„è¯­ä¹‰åˆ†æã€‚

### æ€»ç»“
æœ¬æ–‡å¯¹å‘é‡ç©ºé—´æ¨¡å‹åœ¨è®¡ç®—è¯­è¨€å­¦ä¸­è¡¨å¾è¯æ±‡æ„ä¹‰çš„åº”ç”¨è¿›è¡Œäº†å…¨é¢çš„ç ”ç©¶ã€‚æœ¬æ–‡ä»é›†åˆè®ºæ¨¡å‹çš„å±€é™æ€§å…¥æ‰‹ï¼Œå¼•å…¥å‘é‡ç©ºé—´ä½œä¸ºä¸€ç§åœ¨å¤§å‹è¯­æ–™åº“ä¸­é€šè¿‡è¯­å¢ƒä½¿ç”¨æ¥æ•æ‰è¯­ä¹‰ç›¸ä¼¼æ€§çš„æ–¹æ³•ã€‚è®ºæ–‡é€šè¿‡å®éªŒå¼ºè°ƒäº†è¿™äº›æ¨¡å‹åœ¨ä¿¡æ¯æ£€ç´¢å’Œè‡ªåŠ¨åŒä¹‰è¯æå–ç­‰å„ç§åº”ç”¨ä¸­çš„æœ‰æ•ˆæ€§ã€‚æ­¤å¤–ï¼Œæœ¬æ–‡è¿˜æ¢è®¨äº†å°†è¯å‘é‡ç»„åˆæˆçŸ­è¯­å’Œå¥å­ç­‰æ›´å¤§æ„ä¹‰å•å…ƒçš„é«˜çº§æ–¹æ³•ï¼Œä»¥åŠå°†è¿™äº›æ¨¡å‹é›†æˆåˆ°æ›´å¹¿æ³›çš„è‡ªç„¶è¯­è¨€å¤„ç† (NLP) åº”ç”¨ä¸­çš„æ½œåŠ›ã€‚ç»“è®ºéƒ¨åˆ†è®¤å¯äº†å‘é‡ç©ºé—´æ¨¡å‹çš„ç¨³å¥æ€§ï¼Œå¹¶æŒ‡å‡ºäº†æœªæ¥çš„ç ”ç©¶æ–¹å‘ã€‚