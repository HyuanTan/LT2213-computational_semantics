{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A2: Vector Semantics\n",
    "\n",
    "By Nikolai Ilinykh, Mehdi Ghanimifard, Wafia Adouane and Simon Dobnik. Updated in 2025 by Ricardo Muñoz Sánchez\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will look at how to build distributional semantic models from corpora and use semantic similarity captured by these models to do semantic tasks. We are also going to examine how different vector composition functions for vectors work in approximating semantic similarity of phrases when compared to human judgements.\n",
    "\n",
    "This lab uses code from a file called `dist_erk.py` which contains functions similar to those shown in the lecture. You can use either set of functions to solve these tasks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The code for dist_erk.py uses both Spacy and NLTK, so make sure to have them installed!\n",
    "# Our code also uses SciPY and scikit-learn, so you'll need to install it as well.\n",
    "# If you're unsure how to do this, check out these websites:\n",
    "### https://scipy.org/beginner-install/\n",
    "### https://scikit-learn.org/stable/install.html\n",
    "### https://spacy.io/usage\n",
    "### https://www.nltk.org/install.html\n",
    "\n",
    "\n",
    "# We also need to make sure we have the necessary models and datasets for Spacy\n",
    "import spacy\n",
    "spacy.cli.download('en_core_web_sm')\n",
    "spacy.cli.download('en_core_web_lg')\n",
    "spacy.cli.download('en_core_web_trf')\n",
    "\n",
    "# You only need to run this cell once\n",
    "# You *need* to restart the kernel after downloading the model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# the following command simply imports all the methods from the dist_erk file\n",
    "from dist_erk import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Loading a corpus\n",
    "\n",
    "To train a distributional model, we first need a sufficiently large collection of texts which contain different words used frequently enough in different contexts. Here we will use a section of the Wikipedia corpus `wikipedia.txt` stored in `wikipedia.zip`. This file has been borrowed from another lab by [Richard Johansson](http://www.cse.chalmers.se/~richajo/).\n",
    "\n",
    "When unpacked, the file is 151mb, hence if you are using the MLT servers you should store it in a temporary folder outside your home and adjust the `corpus_dir` path below. It may already exist in `/srv/data/computational-semantics/`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# corpus_dir = './wikipedia/'\n",
    "corpus_dir = '/srv/data/computational-semantics/assignments/wikipedia/'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Building a model\n",
    "\n",
    "Now you are ready to build the model.  \n",
    "Using the methods from the code imported above build three word matrices with 1000 dimensions as follows:  \n",
    "\n",
    "(i) with raw counts (saved to a variable `space_1k`);  \n",
    "(ii) with PPMI (`ppmispace_1k`);  \n",
    "(iii) with reduced dimensions SVD (`svdspace_1k`).  \n",
    "For the latter use `svddim=5`. **[5 marks]**\n",
    "\n",
    "Your task is to replace `...` with function calls to functions from `dist_erk.py` which are similar to functions shown during the lecture.\n",
    "\n",
    "Do not despair if the code takes a bit long to run!\n",
    "It took me about 9 minutes for the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reading file wikipedia.txt\n",
      "create count matrices\n",
      "reading file wikipedia.txt\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1145485it [01:30, 12678.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ppmi transform\n",
      "svd transform\n",
      "done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 1000\n",
    "svddim = 5\n",
    "\n",
    "# Which words to use as targets and context words?\n",
    "# We need to count the words and keep only the N most frequent ones\n",
    "# Which function would you use here with which variable?\n",
    "ktw = do_word_count(corpus_dir, numdims)\n",
    "\n",
    "wi = make_word_index(ktw) # word index\n",
    "words_in_order = sorted(wi.keys(), key=lambda w:wi[w]) # sorted words\n",
    "\n",
    "# Create different spaces (the original matrix space, the ppmi space, the svd space)\n",
    "# Which functions with which arguments would you use here?\n",
    "space_1k = make_space(corpus_dir, wi, numdims)\n",
    "\n",
    "\n",
    "ppmispace_1k = ppmi_transform(space_1k, wi)\n",
    "\n",
    "\n",
    "svdspace_1k = svd_transform(ppmispace_1k, numdims, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2551 3714 3104  567  962  627  443  185  311  189  131   28   93  169\n",
      "   81  125  151  408  194   89   79   29  217  184   62   15   31   70\n",
      "   10    1   41   21    1   31   37    1   30    5   25    7    3   20\n",
      "   11    1   32   36    2    5   65    4    0   46    8   18   28    0\n",
      "   20    7    8   16   10   40    0  175   10    2    7   19    1  174\n",
      "   11    3    1    6    0    0    0   10    9   11    7   24    4    4\n",
      "   14   23   58    7    0   10    2    3   10    6   18    6   13    3\n",
      "   22    0    3    5    3    7   14    3   40   20   19   15    6    8\n",
      "   23    4    5    1   19    0    3    1    0   14    0   14   53    7\n",
      "    7   11    6    5    5    4   12    6   53    1    1  433    4    0\n",
      "    5    7    7   12    1    1    3    4   17    8   16    1    2   31\n",
      "    1   12   14    1   44    6   14    9   38    7    2    6    8    1\n",
      "   10    6   10    1    9    7    9    4    3    9    0   11    3    2\n",
      "    0    2   11   37    2    0    2    1    5    9   10   16    4    6\n",
      "    0   21    1    1    0    2   47    3   27    7    0    2   13    1\n",
      "    2    0    5   31    0    1    0    3    9    0    1    0    3    3\n",
      "   17    1    1   16    3    7    4    7   15    4    0    0    2    5\n",
      "    0    2    0    5    0    9    0    0    8    0   10    0    0    0\n",
      "    2    0    1    3    1    3   15    1    9    0   19   14    0    0\n",
      "    3    2   18    3    1    3    2   19    5    2    4    1   10    6\n",
      "    0    3    3    6    4    2   25    4    6    3    1   25   10   15\n",
      "    3   10   15    1   10    1    8    1   13    1    2    9    9    1\n",
      "    4    1   25    0    4    6    5    5   36    0    2    2    2    0\n",
      "    0    2    3    3    0    1    4    6    5    0   50    2    5    2\n",
      "   14    6    2    2    4    1    9    4    5    3    1    0   12    3\n",
      "    3    2    2    0    0    1    4    7   12    5    0    2    1    2\n",
      "    3    4    7    3    5    0   27    7    1    1    0    3    3    3\n",
      "   10    0   14    2    0    2    4    6    0    5    0    0    1    1\n",
      "    4    1    1    0    0    0    0    3   20    0    0    2    1    5\n",
      "    3    8    3    5    1    2   66    1    2   19    2    1    3    3\n",
      "   21    5    4    2    2    0    4    3    5    0    7    1    6    1\n",
      "    3    3    1    0    3    0    2    0   89    2    3    1    1   14\n",
      "    0    2    1    9    2    3    2    4    2    0   25    0    0   23\n",
      "    0    6    2    1    3    0    2    5    0    4    4    3    0    4\n",
      "   58    3    1    6    2    4    3    3   11    1    1    1   10    0\n",
      "    7    3    1    6    1   18    1    0    4    2    0    8    5    2\n",
      "    0    0    0    0    5    1    2    1    1    3    1    2    1    1\n",
      "    0    6    1    4    1    3   20    1    0    5    2    5    2    1\n",
      "    0    0    0    2    6    1    1    0    1    1    1    0    0    3\n",
      "    3    0    0    6    6   74    3    0   13    5    2    2    1    5\n",
      "    3    3    1    7    4    0    0    2    3    0    4    0    4    1\n",
      "    0    2    5    2    1   14    2    0    0   19    0    1    2    1\n",
      "    0    3    2    0    0    3    1    3    3    2    7   18    7    6\n",
      "    6    0    1    9    1   10    2    0    2    0    2    4    0    0\n",
      "    1    2    0    1    0    2    0    0    0    2    0    2    2    0\n",
      "    3    2    2    0    0    1    2    3    1    1    1    2    0    0\n",
      "    3    0    7    2   39    0   14    0    1    1    0    1    5    3\n",
      "   11    0    3    0    1    1    0    0    1    9    2    1    0   11\n",
      "    1    3    7    0    0    0   32    1    0    0    0    1    1    3\n",
      "    0    9    0    2    0    1    3    2    6    0    3    0    0    2\n",
      "    3    0    1    0    1    4    0    0    1    1    0    0    5   21\n",
      "    2    1    1    3    0    1    7    1    3    4    0    5    3    0\n",
      "    7    2    0    4    2    0    2    1    4    4    0    0    0    5\n",
      "    3    2    2    0    4    0   23    2    2    2    4    0    1    0\n",
      "    4    0    3    5    3    0    8    0    1   16    1    2    2    7\n",
      "    0    0    1   11    1    0    4    0    1    0    1    2    1    5\n",
      "    0   97    0    2    0    3    0    8    1   14    4    9    2    3\n",
      "    1    1    0    3    4    0    5    1    5    2    0    0    0    2\n",
      "    1    2    1    1    1    1   12    0    2    5    1    0    0   13\n",
      "    2    0    0    0    2    2    0    0    3    1    1    1    1    0\n",
      "    1    2    1    0    0    0   10    0    1    0    1    1    1    1\n",
      "    0    1    0    0    3    2    5    0    0    2    1    0   23    0\n",
      "    0    4    0    1    0    0    0    1    1    2    1    0    1    0\n",
      "    0    4    1    0    1    1    5    1    1    0    1    0    0    0\n",
      "    1    0    0    2    2    3    0    1    0    4    3    3    1    4\n",
      "    0    0    0    6    1    2    1    0    5    3    0    0    1    2\n",
      "    0    5    0    0    2    1    1    4   15    0    0    1    1    3\n",
      "    1    0    1    4    1    1    2    8    1    3    0    0    0    0\n",
      "    1    3    2    1    0    1    0    2    0    0    0    0    1    1\n",
      "    0    1    3    7    0    0   42    4    0    1    2    3    1    0\n",
      "    1    3    2    0    0    1    0    0    0    4    2    0    0    8\n",
      "    2    0    1   15    0    0]\n"
     ]
    }
   ],
   "source": [
    "# now, to test the space, you can print vector representation for some words\n",
    "print('house:', space_1k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Oxford Advanced Dictionary has 185,000 words, hence 1,000 words is not representative. We trained a model with 10,000 words, and 50 dimensions on truncated SVD. All matrices are available in the folder `pretrained` of the `wikipedia.zip`file. These are `ktw_wikipediaktw.npy`, `raw_wikipediaktw.npy`, `ppmi_wikipediaktw.npy`, `svd50_wikipedia10k.npy`. Make sure they are in your path as we load them below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Please wait...\n",
      "Done.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "numdims = 10000\n",
    "svddim = 50\n",
    "\n",
    "ktw_10k       = np.load(f'{corpus_dir}/pretrained/ktw_wikipediaktw.npy', allow_pickle=True)\n",
    "space_10k     = np.load(f'{corpus_dir}/pretrained/raw_wikipediaktw.npy', allow_pickle=True).tolist()\n",
    "ppmispace_10k = np.load(f'{corpus_dir}/pretrained/ppmi_wikipediaktw.npy', allow_pickle=True).tolist()\n",
    "svdspace_10k  = np.load(f'{corpus_dir}/pretrained/svd50_wikipedia10k.npy', allow_pickle=True).tolist()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "house: [2554 3774 3105 ...    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "# Testing semantic space\n",
    "print('house:', space_10k['house'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Testing semantic similarity\n",
    "\n",
    "The file `similarity_judgements.txt` contains 7,576 pairs of words and their lexical and visual similarities (based on the pictures) collected through crowd-sourcing using Mechanical Turk as described in [1]. The scores range from 1 (highly dissimilar) to 5 (highly similar). Note: this is a different dataset from the phrase similarity dataset we discussed during the lecture [2]. You can find more details about how they were collected in the papers.\n",
    "\n",
    "The following code will transform similarity scores into a Python-friendly format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of available words to test: 155\n",
      "number of available word pairs to test: 774\n"
     ]
    }
   ],
   "source": [
    "word_pairs = [] # Test suit word pairs\n",
    "semantic_similarity = [] \n",
    "visual_similarity = []\n",
    "test_vocab = set()\n",
    "\n",
    "for index, line in enumerate(open('similarity_judgements.txt')):\n",
    "    data = line.strip().split('\\t')\n",
    "    if index > 0 and len(data) == 3:\n",
    "        w1, w2 = tuple(data[0].split('#'))\n",
    "        # Checks if both words from each pair exist in the word matrix.\n",
    "        if w1 in ktw_10k and w2 in ktw_10k:\n",
    "            word_pairs.append((w1, w2))\n",
    "            test_vocab.update([w1, w2])\n",
    "            semantic_similarity.append(float(data[1]))\n",
    "            visual_similarity.append(float(data[2]))\n",
    "        \n",
    "print('number of available words to test:', len(test_vocab-(test_vocab-set(ktw_10k))))\n",
    "print('number of available word pairs to test:', len(word_pairs))\n",
    "#list(zip(word_pairs, visual_similarity, semantic_similarity))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to test how the cosine similarity between vectors of each of the three spaces (normal space, ppmi, svd) compares with the human similarity judgements for the words in the similarity dataset. Which of the three spaces best approximates human judgements?\n",
    "\n",
    "For comparison of several scores, we can use [the Spearman correlation coefficient](https://en.wikipedia.org/wiki/Spearman's_rank_correlation_coefficient) which is implemented in `scipy.stats.spearmanr` [here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.spearmanr.html). The values of the Sperman correlation coefficient range from -1, 0 to 1, where 0 indicates no correlation, 1 perfect correaltion and -1 negative correlation. Hence, the greater the number the better the similarity scores align. The p values tells us if the coefficient is statistically significant. For this to be the case, it must be less than or equal to $< 0.05$.\n",
    "\n",
    "Here is how you can calculate the Spearman correlation coefficient betweeen the scores of visual similarity and semantic similarity of the available words in the test suite:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Semantic Similarity:\n",
      "rho     = 0.7122\n",
      "p-value = 0.0000\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, visual_similarity)\n",
    "print(\"\"\"Visual Similarity vs. Semantic Similarity:\n",
    "rho     = {:.4f}\n",
    "p-value = {:.4f}\"\"\".format(rho, pval))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now calculate the cosine similarity scores of all word pairs in an ordered list using all three matrices. **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_similarities  = [cosine(w1, w2, space_10k) for w1, w2 in word_pairs]\n",
    "ppmi_similarities = [cosine(w1, w2, ppmispace_10k) for w1, w2 in word_pairs]\n",
    "svd_similarities  = [cosine(w1, w2, svdspace_10k) for w1, w2 in word_pairs]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate correlation coefficients between lists of similarity scores and the real semantic similarity scores from the experiment. The scores of what model best correlates them? Is this expected? **[6 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Semantic Similarity vs. Raw Similarity:\n",
      "rho = 0.1522\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "Semantic Similarity vs. PPMI Similarity:\n",
      "rho = 0.4547\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "Semantic Similarity vs. SVD Similarity:\n",
      "rho = 0.4232\n",
      "p-value = 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, raw_similarities)\n",
    "print(f\"Semantic Similarity vs. Raw Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, ppmi_similarities)\n",
    "print(f\"Semantic Similarity vs. PPMI Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(semantic_similarity, svd_similarities)\n",
    "print(f\"Semantic Similarity vs. SVD Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "The **rho** above shows the Spearman rank correlation coefficient between `raw_similarity`, `ppmi_similarity`, and `svd_similarity` (normal space, ppmi, svd) with the real `semantic_similarity`. We can see that **PPMI similarity** has the highest **rho** value of **0.4547**, indicating the strongest correlation. **SVD similarity** has a **rho** of **0.4232**, also showing a high correlation.\n",
    "\n",
    "PPMI similarity is clearly higher than the raw count method, as PPMI accounts for the context of the target word, which helps refine the similarity measure by emphasizing co-occurrence patterns that are more likely to reflect the true semantic relationships. On the other hand, **SVD similarity** is slightly lower than PPMI similarity, which might be due to the information loss during the dimensionality reduction process when using SVD on the PPMI model. The SVD approach reduces the vector space dimensions to capture the most important latent features, but it can result in a loss of some fine-grained context, reducing the overall similarity.\n",
    "\n",
    "The **p-value** for all correlations is less than **0.05**, indicating that the observed correlations are statistically significant.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also calculate correlation coefficients between lists of cosine similarity scores and the real visual similarity scores from the experiment. Which similarity model best correlates with them? How do the correlation coefficients compare with those from the previous comparison - and can you speculate why do we get such results? **[7 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Visual Similarity vs. Raw Similarity:\n",
      "rho = 0.1212\n",
      "p-value = 0.0007\n",
      "\n",
      "\n",
      "Visual Similarity vs. PPMI Similarity:\n",
      "rho = 0.3838\n",
      "p-value = 0.0000\n",
      "\n",
      "\n",
      "Visual Similarity vs. SVD Similarity:\n",
      "rho = 0.3097\n",
      "p-value = 0.0000\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from scipy import stats\n",
    "\n",
    "rho, pval = stats.spearmanr(visual_similarity, raw_similarities)\n",
    "print(f\"Visual Similarity vs. Raw Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(visual_similarity, ppmi_similarities)\n",
    "print(f\"Visual Similarity vs. PPMI Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")\n",
    "\n",
    "rho, pval = stats.spearmanr(visual_similarity, svd_similarities)\n",
    "print(f\"Visual Similarity vs. SVD Similarity:\\nrho = {rho:.4f}\\np-value = {pval:.4f}\\n\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "The **PPMI model** shows the strongest correlation, clearly outperforming the raw count method, followed by **SVD**, which is slightly weaker than the **PPMI model**. This trend is similar to what we observed with **Semantic Similarity**. However, the correlation between **cosine similarity scores** and **real visual similarity scores** is overall lower than the correlation between **cosine similarity scores** and **real semantic similarity scores**. This is because the correlation between **Visual Similarity** and **Semantic Similarity** is **0.7122**, indicating that lexical and visual similarities inherently differ. Visual similarity may also involve a degree of subjectivity, which could contribute to the lower correlation with real-world visual judgments.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Operations on similarities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can perform mathematical operations on vectors to derive meaning predictions.\n",
    "\n",
    "For example, we can perform `king - man` and add the resulting vector to `woman` and we hope to get the vector for `queen`. What would be the result of `stockholm - sweden + denmark`? Why? **[3 marks]**\n",
    "\n",
    "If you want to learn more about vector differences between words (and words in analogy relations), check this paper [4]."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "In the example **king - man + woman = queen** try to perform vector arithmetic to capture the relationship between words. Subtracting \"man\" from \"king\" isolates the part of the \"king\" vector that represents \"male,\" and adding \"woman\" gives us the vector representation of \"queen\" because \"queen\" is the female counterpart to \"king.\"\n",
    "\n",
    "For the analogy **Stockholm - Sweden + Denmark**, we expect a vector close to **Copenhagen**, which is the capital city of Denmark. This is because **Stockholm** is the capital of **Sweden**, by subtracting \"Sweden\" from \"Stockholm,\" we isolate the \"capital city\" part of Stockholm's vector. Adding \"Denmark\" introduces the country of Denmark, so the result is expected to be the capital of Denmark, **Copenhagen**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is some code that allows us to calculate such comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial import distance\n",
    "\n",
    "def normalize(vec):\n",
    "    return vec / veclen(vec)\n",
    "\n",
    "def find_similar_to(vec1, space):\n",
    "    # vector similarity funciton\n",
    "    #sim_fn = lambda a, b: 1-distance.euclidean(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.correlation(a, b)\n",
    "    #sim_fn = lambda a, b: 1-distance.cityblock(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: 1-distance.chebyshev(normalize(a), normalize(b))\n",
    "    #sim_fn = lambda a, b: np.dot(normalize(a), normalize(b))\n",
    "    sim_fn = lambda a, b: 1-distance.cosine(a, b)\n",
    "\n",
    "    sims = [\n",
    "        (word2, sim_fn(vec1, space[word2]))\n",
    "        for word2 in space.keys()\n",
    "    ]\n",
    "    return sorted(sims, key = lambda p:p[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how you apply this code. Comment on the results you get. **[3 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('long', 0.8733111261346902),\n",
       " ('above', 0.8259671977311956),\n",
       " ('around', 0.8030776291120686),\n",
       " ('sun', 0.7692439111243974),\n",
       " ('just', 0.767848197477811),\n",
       " ('wide', 0.7672574319922534),\n",
       " ('each', 0.7665960260861158),\n",
       " ('circle', 0.7647746702909335),\n",
       " ('length', 0.7601066921319761),\n",
       " ('almost', 0.7542351860536627)]"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "short = normalize(svdspace_10k['short'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "long = normalize(svdspace_10k['long'])\n",
    "heavy = normalize(svdspace_10k['heavy'])\n",
    "\n",
    "find_similar_to(light - (heavy - long), svdspace_10k)[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "The vector arithmetic `light - (heavy - long)`, which can be simplified to `light - heavy + long`. This operation try to blend concepts of **lightness**, **length** and the negation of **heaviness**.\n",
    "\n",
    "The top result **long** and **length** directly reflect the arithmetic emphasis on `+ long`, since `light - heavy`, the contrary meaning subtraction may leaf not adjective to the enerty. They may also associate with **light** from sun or light with spatial or dimensional properties. The presence of \"sun\" and \"circle\" is the source of light and the shape of sun. The spatial terms **(\"above,\" \"around,\" \"wide\")** are about dimensional attributes, aligning with the geometric interpretation of \"length\". The absence of direct antonyms for \"heavy\" also show challenges in isolating oppositional relationships through simple arithmetic.\n",
    "\n",
    "The word **just，each，almost** have not semantic meaning with the operator object but usually act as qualifiers, determiners, or adverbs, often appearing alongside dimensional terms in sentences. It shows that vector arithmetic don’t perfectly isolate semantic logic. They retrieve words that share fragments of contextual overlap with the query vector, leading to noise.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Find 5 similar pairs of pairs of words and test them. Hint: google for `word analogies examples`. You can also construct analogies that are not only lexical but also express other relations such as grammatical relations, e.g. `see, saw, leave, ?` or analogies that are based on world knowledge as in `question-words.txt` from the [Google analogy dataset](http://download.tensorflow.org/data/questions-words.txt) described in [3]. Does the resulting vector similarity confirm your expectations? Remember you can only do this test if the words are contained in our vector space with 10,000 dimensions. **[10 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Expression:saw - see + go --> went\n",
      "\n",
      "Target: \u001b[32mwent\u001b[0m\n",
      "\n",
      "('gone', np.float64(0.7727247930029633))\n",
      "('go', np.float64(0.7686350482087698))\n",
      "('ahead', np.float64(0.7601887728920765))\n",
      "('move', np.float64(0.7563034683136987))\n",
      "('stand', np.float64(0.7553404522830045))\n",
      "('stay', np.float64(0.7527578152359772))\n",
      "('throw', np.float64(0.7517022141423654))\n",
      "('blow', np.float64(0.7498511450771898))\n",
      "('going', np.float64(0.7452173186091259))\n",
      "('put', np.float64(0.7431018790922572))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "*** Using PPMI space:\n",
      "Expression:saw - see + go --> went\n",
      "\n",
      "Target: \u001b[32mwent\u001b[0m\n",
      "\n",
      "('go', np.float64(0.6037425322598825))\n",
      "('saw', np.float64(0.589673146628872))\n",
      "('get', np.float64(0.1779585939527637))\n",
      "('to', np.float64(0.17562589875377843))\n",
      "\u001b[32m('went', np.float64(0.1753969623869055))\u001b[0m\n",
      "('going', np.float64(0.16931358410259578))\n",
      "('take', np.float64(0.16441004643269452))\n",
      "('you', np.float64(0.16370258486326283))\n",
      "('leave', np.float64(0.1601650333492034))\n",
      "('would', np.float64(0.15595100151632024))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:south - north + inside --> outside\n",
      "\n",
      "Target: \u001b[32moutside\u001b[0m\n",
      "\n",
      "('inside', np.float64(0.7824618623985569))\n",
      "('behind', np.float64(0.7193180437994586))\n",
      "\u001b[32m('outside', np.float64(0.7121483306205937))\u001b[0m\n",
      "('front', np.float64(0.7117090755958663))\n",
      "('into', np.float64(0.7058512223498128))\n",
      "('onto', np.float64(0.7034351319958758))\n",
      "('upper', np.float64(0.6926906145798579))\n",
      "('across', np.float64(0.6838515101392014))\n",
      "('moving', np.float64(0.6789381429989645))\n",
      "('surrounded', np.float64(0.6752830393296485))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:north - south + outside --> inside\n",
      "\n",
      "Target: \u001b[32minside\u001b[0m\n",
      "\n",
      "('51', np.float64(0.7492431881347611))\n",
      "('101', np.float64(0.7481418044300802))\n",
      "('32', np.float64(0.7472866031452325))\n",
      "('54', np.float64(0.7445299445788153))\n",
      "('49', np.float64(0.7430538139423183))\n",
      "('42', np.float64(0.7412309759483819))\n",
      "('47', np.float64(0.740358951732992))\n",
      "('38', np.float64(0.7394181869894031))\n",
      "('36', np.float64(0.739076067874327))\n",
      "('48', np.float64(0.7375195687076839))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:bigger - big + light --> lighter\n",
      "\n",
      "Target: \u001b[32mlighter\u001b[0m\n",
      "\n",
      "('heavier', np.float64(0.8426595491872143))\n",
      "\u001b[32m('lighter', np.float64(0.8133000924018884))\u001b[0m\n",
      "('weaker', np.float64(0.7813196361019621))\n",
      "('overhead', np.float64(0.7780356138963349))\n",
      "('armour', np.float64(0.7714475516195193))\n",
      "('exhaust', np.float64(0.7675413342127478))\n",
      "('slower', np.float64(0.766674326375574))\n",
      "('lens', np.float64(0.7659199657779441))\n",
      "('passive', np.float64(0.7631240900328303))\n",
      "('sufficiently', np.float64(0.7629550011039358))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:athens - greece + china --> beijing\n",
      "\n",
      "Target: \u001b[32mbeijing\u001b[0m\n",
      "\n",
      "('athens', np.float64(0.9116568536395251))\n",
      "\u001b[32m('beijing', np.float64(0.8442931838276914))\u001b[0m\n",
      "('kabul', np.float64(0.8382492681686748))\n",
      "('hong', np.float64(0.8223553025093057))\n",
      "('kyoto', np.float64(0.8169074259632862))\n",
      "('brussels', np.float64(0.815545061512933))\n",
      "('baghdad', np.float64(0.8147426951462164))\n",
      "('columbus', np.float64(0.8131943810698044))\n",
      "('boston', np.float64(0.8107075981004372))\n",
      "('venice', np.float64(0.8103062548538892))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:stockholm - sweden + denmark --> copenhagen\n",
      "\n",
      "Target: \u001b[32mcopenhagen\u001b[0m\n",
      "\n",
      "('stockholm', np.float64(0.9571449438177747))\n",
      "('prague', np.float64(0.9255634442739453))\n",
      "('amsterdam', np.float64(0.8997980639287136))\n",
      "('brussels', np.float64(0.8969830643584372))\n",
      "('oslo', np.float64(0.8922923112439679))\n",
      "('hamburg', np.float64(0.8911355385331753))\n",
      "('cologne', np.float64(0.8904376966250225))\n",
      "('milan', np.float64(0.8895360380024666))\n",
      "('munich', np.float64(0.8874478811027278))\n",
      "('frankfurt', np.float64(0.8872747257104568))\n",
      "('moscow', np.float64(0.8867636154954216))\n",
      "('lisbon', np.float64(0.8773559400400501))\n",
      "('cairo', np.float64(0.8743241913768492))\n",
      "('warsaw', np.float64(0.8731572962331172))\n",
      "('delhi', np.float64(0.8650430956335423))\n",
      "('barcelona', np.float64(0.8600640290209335))\n",
      "('geneva', np.float64(0.859333125107105))\n",
      "\u001b[32m('copenhagen', np.float64(0.8588986077040479))\u001b[0m\n",
      "('madrid', np.float64(0.8528033057184644))\n",
      "('vienna', np.float64(0.8521746123583889))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:king - man + woman --> queen\n",
      "\n",
      "Target: \u001b[32mqueen\u001b[0m\n",
      "\n",
      "('king', np.float64(0.9448889961684729))\n",
      "('emperor', np.float64(0.8984712669397816))\n",
      "('prince', np.float64(0.8847657640840528))\n",
      "('consort', np.float64(0.8794222801811236))\n",
      "\u001b[32m('queen', np.float64(0.8694575540149948))\u001b[0m\n",
      "('regent', np.float64(0.8684243572788882))\n",
      "('empress', np.float64(0.8657733616243543))\n",
      "('grandson', np.float64(0.850167738111101))\n",
      "('heir', np.float64(0.8442840354722895))\n",
      "('pope', np.float64(0.8409412955462182))\n",
      "***************************************************\n",
      "\n",
      "\n",
      "Expression:father - man + woman --> mother\n",
      "\n",
      "Target: \u001b[32mmother\u001b[0m\n",
      "\n",
      "('father', np.float64(0.9473933188783542))\n",
      "('wife', np.float64(0.9179232286108289))\n",
      "\u001b[32m('mother', np.float64(0.8834089585781714))\u001b[0m\n",
      "('grandmother', np.float64(0.8583075101280719))\n",
      "('mistress', np.float64(0.8490538868581158))\n",
      "('daughter', np.float64(0.8451970825001897))\n",
      "('son', np.float64(0.8449726724078512))\n",
      "('brother', np.float64(0.8424654662813444))\n",
      "('parents', np.float64(0.8396024061706576))\n",
      "('grandfather', np.float64(0.8367920028440166))\n",
      "***************************************************\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def print_analogies(target, similar_words, expression=None):\n",
    "    print(f\"Expression:{expression}\\n\")\n",
    "    print(f\"Target: \\033[32m{target}\\033[0m\\n\")\n",
    "    for word in similar_words:\n",
    "        if word[0] == target:\n",
    "            print(f\"\\033[32m{word}\\033[0m\")\n",
    "        else:\n",
    "            print(word)\n",
    "    print(f\"***************************************************\\n\\n\")\n",
    "\n",
    "############## 1. Grammatical Category Relationship ##############\n",
    "# see saw seen\n",
    "# go went gone\n",
    "see = normalize(svdspace_10k['see'])   \n",
    "saw = normalize(svdspace_10k['saw'])\n",
    "go = normalize(svdspace_10k['go'])\n",
    "went = normalize(svdspace_10k['went'])\n",
    "\n",
    "# Analogy computation: saw - see + go --> went\n",
    "result_verb_tense = saw - see + go\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_verb_tense, svdspace_10k)[:10]\n",
    "print_analogies('went', similar_words, expression=\"saw - see + go --> went\")\n",
    "\n",
    "# ***********\n",
    "see = normalize(ppmispace_10k['see'])   \n",
    "saw = normalize(ppmispace_10k['saw'])\n",
    "go = normalize(ppmispace_10k['go'])\n",
    "went = normalize(ppmispace_10k['went'])\n",
    "\n",
    "# Analogy computation: saw - see + go --> went\n",
    "result_verb_tense = saw - see + go\n",
    "\n",
    "# Find similar words to the result\n",
    "print(\"*** Using PPMI space:\")\n",
    "similar_words = find_similar_to(result_verb_tense, ppmispace_10k)[:10]\n",
    "print_analogies('went', similar_words, expression=\"saw - see + go --> went\")\n",
    "\n",
    "\n",
    "############## 2. Spatial Relationship ##############\n",
    "south = normalize(svdspace_10k['south']) \n",
    "north = normalize(svdspace_10k['north'])\n",
    "inside = normalize(svdspace_10k['inside'])\n",
    "outside = normalize(svdspace_10k['outside'])\n",
    "\n",
    "# Analogy computation: south - north + inside --> outside\n",
    "result_spatial = south - north + inside\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_spatial, svdspace_10k)[:10]\n",
    "print_analogies('outside', similar_words, expression=\"south - north + inside --> outside\")\n",
    "\n",
    "# ***********\n",
    "# Analogy computation: north - south + outside --> inside\n",
    "result_spatial = north - south + outside\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_spatial, svdspace_10k)[:10]\n",
    "print_analogies('inside', similar_words, expression=\"north - south + outside --> inside\")\n",
    "\n",
    "############## 3. Comparative Relationship ##############\n",
    "big = normalize(svdspace_10k['big'])\n",
    "bigger = normalize(svdspace_10k['bigger'])\n",
    "light = normalize(svdspace_10k['light'])\n",
    "lighter = normalize(svdspace_10k['lighter'])\n",
    "\n",
    "# Analogy computation: bigger - big + light --> lighter\n",
    "result_comparative = bigger - big + light\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_comparative, svdspace_10k)[:10]\n",
    "print_analogies('lighter', similar_words, expression=\"bigger - big + light --> lighter\")\n",
    "\n",
    "############## 4. City-Country ##############\n",
    "city1 = normalize(svdspace_10k['athens'])\n",
    "country1 = normalize(svdspace_10k['greece'])\n",
    "city2 = normalize(svdspace_10k['beijing'])\n",
    "country2 = normalize(svdspace_10k['china'])\n",
    "\n",
    "# Analogy computation:  city1 - country1 + country2 --> city2\n",
    "result_antonym = city1 - country1 + country2\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_antonym, svdspace_10k)[:10]\n",
    "print_analogies(f'beijing', similar_words, expression=\"athens - greece + china --> beijing\")\n",
    "\n",
    "#######\n",
    "city1 = normalize(svdspace_10k['stockholm'])\n",
    "country1 = normalize(svdspace_10k['sweden'])\n",
    "city2 = normalize(svdspace_10k['copenhagen'])\n",
    "country2 = normalize(svdspace_10k['denmark'])\n",
    "\n",
    "# Analogy computation:  city1 - country1 + country2 --> city2\n",
    "result_antonym = city1 - country1 + country2\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_antonym, svdspace_10k)[:20]\n",
    "print_analogies(f'copenhagen', similar_words, expression=\"stockholm - sweden + denmark --> copenhagen\")\n",
    "\n",
    "\n",
    "############## 5. Relationship ##############\n",
    "king = normalize(svdspace_10k['king'])\n",
    "man = normalize(svdspace_10k['man'])\n",
    "queen = normalize(svdspace_10k['queen'])\n",
    "woman = normalize(svdspace_10k['woman'])\n",
    "\n",
    "# Analogy computation: king - man + woman --> queen\n",
    "result_comparative = king - man + woman\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_comparative, svdspace_10k)[:10]\n",
    "print_analogies('queen', similar_words, expression=\"king - man + woman --> queen\")\n",
    "\n",
    "# *********** \n",
    "father = normalize(svdspace_10k['father'])\n",
    "man = normalize(svdspace_10k['man'])\n",
    "mother = normalize(svdspace_10k['mother'])\n",
    "woman = normalize(svdspace_10k['woman'])\n",
    "\n",
    "# Analogy computation: father - man + woman --> mother\n",
    "result_comparative = father - man + woman\n",
    "\n",
    "# Find similar words to the result\n",
    "similar_words = find_similar_to(result_comparative, svdspace_10k)[:10]\n",
    "print_analogies('mother', similar_words, expression=\"father - man + woman --> mother\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Your answer should go here:**\n",
    "\n",
    "In case **\"saw - see + go --> went\"**, using **svdspace_10k**, the resulting words include **gone**, **go**, **move**, and **going**, but **went** does not shown in top 10, which is the expected word. This indicates that **SVD** loses some fine-grained details during the dimensionality reduction process. However, when using the **ppmispace_10k** model, the target word **went** appears in top 10, which aligns with the expected result. This shows that **PPMI** better captures the detailed relationships than **SVD** but sacrify the calculation. In case **\"bigger - big + light --> lighter\"**, The **svdspace_10k** model is able to correctly capture the grammatical rule and predict **lighter**. This result shows that **SVD** can handle certain grammatical transformations, like adjective comparison, effectively in some cases.\n",
    "\n",
    "Incase **Spatial Relationship - \"south - north + inside --> outside\"**, the **svdspace_10k** model correctly predicts **outside** as the result of the operation. However, in the reverse analogy, **\"north - south + outside --> inside\"** the result deviates from expectations, and the output is all numeric values. The model may strong at capturing certain relationships, but struggle with reversing certain analogies or with relationships that involve more subtle semantic distinctions.\n",
    "\n",
    "The remaining examples generally align well with the expected results, demonstrating that Vector-based Models can capture the intended relationships for most of the analogies tested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Semantic composition and phrase similarity **[20 marks]**\n",
    "\n",
    "In this task, we are going to examine how the composed vectors of phrases by different semantic composition functions/models introduced in [2] correlate with human judgements of similarity between phrases. We will use the dataset from this paper which is stored in `mitchell_lapata_acl08.txt`. If you are interested about further details about this task also refer to this paper.\n",
    "\n",
    "(i) Process the dataset. The dataset contains human judgemements of similarity between phrases recorded one per line. The first column indicates the id of a participant making a judgement (`participant`), the next column is `verb`, followed by `noun` and `landmark`. From these three columns we can construct phrases that were compared by human informants, namely `verb noun` vs `verb landmark`. The next column `input` indicates a similarity score a participant assigned to a pair of such phrases on a scale from 1 to 7 where 1 is lowest and 7 is highest. The last column `hilo` groups the phrases into two sets: phrases where we expect low and phrases where we expect high similarity scores. This is because we want to test our compositional functions on two tasks and examine whether a function is discriminative between them. Correlation between scores could also be due to other reasons than semantic similarity and hence good prediction on both tasks simultaneously shows that a function is truly discriminating the phrases using some semantic criteria.\n",
    "\n",
    "For extracting information you can use the code from the lecture to start with. How to structure this data is up to you - a dictionary-like format would be a good choice. Remember that each example was judged by several participants and phrases will repeat in the dataset. Therefore, you have to collect all judgments for a particular set of phrases and average them. This will become useful in step (iii).\n",
    "\n",
    "(ii) Compose the vectors of the extracted word pairs by testing different compositional functions. In the lecture we introduced simple additive, simple multiplicative and combined models (details are described in [2]). Your task is to take a pair of phrases, e.g. the first example in the dataset `stray thought` and `stray roam` and for each phrase compute a composition of the vectors of their words using these functions, using one function per experiment run. For each phrase you will get a single vector. You can encode the words with any vector space introduced earlier (standard space, ppmi or svd) but your code should be structured in a way that it will be easy to switch between them. Finally, take the resulting (composed) vectors of phrase pairs in the dataset and calculate a cosine similarity between them.\n",
    "\n",
    "(iii) Now you have cosine similairity scores between vectors of phrases but how do they compare with the average human scores that you calculated from the individual judgements from the `input` column of the dataset for the same phrases? Calculate Spearman rank correlation coefficient between two lists of the scores both for the `high` and the `low` task . \n",
    "\n",
    "We use the Spearmank rank correlation coefficient (or Spearman's rho) rather than Peason's correlation coefficent because we cannot compare cosine scores with human judgements directly. Cosine is a constinuous measure and human judgements are expressed as ranks. Also, we cannot say if 0.28 to 1 is the same (or different) to 6 to 7 in the human scores.  The Spearman rank correlation coeffcient turns the scores for all examples within each group first to ranks and then these ranks are correlated (or approximated to a linear function). \n",
    "\n",
    "In the end you should get a table similar to the one below from the paper. What is the best compositional function from those that you evaluated with your vector spaces and why?\n",
    "\n",
    "<img src=\"res.png\" alt=\"drawing\" width=\"500\"/>\n",
    "\n",
    "Note that you might not get results in the same range as those in the paper.\n",
    "That is ok, a good interpretation of results and discussion why sometimes they are not as good as you would expect is better than giving the best performing results with little to no analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Number of rows in the dataset: 3600\n",
      "Number of unique combinations of 'verb', 'noun', and 'landmark': 120\n",
      "Number of participants: 60\n",
      "Number of unique words: 95\n",
      "Number of words in pretrained wikipedia 10k model: 58\n",
      "\u001b[32mNumber of unique combinations found in the 10k wikipidia model: 8\u001b[0m\n",
      "\u001b[32mNumber of unique combinations found: 120\u001b[0m\n",
      "4 pairs of high in 10k wikipidia model(with 120 samples): {('boom', 'noise', 'thunder'): {'input': [6, 6, 6, 7, 7, 7, 6, 6, 6, 5, 5, 6, 7, 7, 6, 7, 6, 7, 5, 7, 6, 7, 5, 6, 7, 3], 'hilo': 'high', 'input_mean': 6.1154}, ('bow', 'government', 'submit'): {'input': [6, 6, 3, 7, 7, 5, 3, 2, 5, 5, 6, 7, 6, 6, 7, 6, 3, 7, 4, 6, 3, 2, 7, 7, 7, 7], 'hilo': 'high', 'input_mean': 5.3846}, ('bow', 'company', 'submit'): {'input': [5, 6, 6, 4, 6, 5, 5, 4, 6, 2, 2, 5, 6, 6, 2, 4, 4, 6, 7, 6, 6, 2, 3, 3, 2, 4, 2, 5, 6, 2, 5, 2, 5, 3], 'hilo': 'high', 'input_mean': 4.3235}, ('boom', 'gun', 'thunder'): {'input': [6, 6, 7, 5, 6, 6, 5, 6, 6, 6, 6, 6, 6, 7, 4, 6, 7, 6, 7, 5, 7, 6, 3, 6, 4, 3, 3, 6, 6, 7, 5, 6, 7, 3], 'hilo': 'high', 'input_mean': 5.6176}}\n",
      "4 pairs of low in 10k wikipidia model(with 120 samples): {('bow', 'butler', 'submit'): {'input': [3, 2, 2, 3, 4, 5, 4, 2, 2, 2, 3, 5, 4, 2, 2, 4, 6, 3, 7, 1, 2, 2, 5, 2, 2, 1, 2, 3, 3, 1, 1, 1, 4, 5], 'hilo': 'low', 'input_mean': 2.9412}, ('boom', 'sale', 'thunder'): {'input': [3, 5, 2, 3, 1, 2, 2, 2, 4, 5, 3, 2, 4, 1, 3, 2, 3, 4, 6, 5, 2, 2, 1, 3, 1, 1, 2, 2, 2, 1, 6, 3, 4, 3], 'hilo': 'low', 'input_mean': 2.7941}, ('boom', 'export', 'thunder'): {'input': [2, 4, 3, 5, 2, 3, 5, 3, 2, 1, 4, 2, 1, 2, 1, 2, 1, 1, 4, 6, 1, 3, 2, 3, 5, 4], 'hilo': 'low', 'input_mean': 2.7692}, ('bow', 'head', 'submit'): {'input': [3, 4, 3, 3, 5, 2, 5, 5, 1, 2, 3, 6, 2, 4, 4, 4, 1, 1, 1, 3, 3, 4, 5, 2, 7, 2], 'hilo': 'low', 'input_mean': 3.2692}}\n"
     ]
    }
   ],
   "source": [
    "# (i) - Process the data\n",
    "import pandas as pd\n",
    "# Load the dataset\n",
    "data_file = './mitchell_lapata_acl08.txt'\n",
    "df = pd.read_csv(data_file, sep=' ', header=None, names=[\"participant\", \"verb\", \"noun\", \"landmark\", \"input\", \"hilo\"], skiprows=1)\n",
    "\n",
    "print(f\"\\nNumber of rows in the dataset: {len(df)}\")\n",
    "\n",
    "\n",
    "unique_combinations = set(zip(df['verb'], df['noun'], df['landmark']))\n",
    "print(f\"Number of unique combinations of 'verb', 'noun', and 'landmark': {len(unique_combinations)}\")\n",
    "\n",
    "print(f\"Number of participants: {len(set(df['participant']))}\")\n",
    "\n",
    "unique_words = set(df['verb'].tolist() + df['noun'].tolist() + df['landmark'].tolist())\n",
    "print(f\"Number of unique words: {len(unique_words)}\")\n",
    "counter = 0\n",
    "words_in_model = []\n",
    "\n",
    "for word in unique_words:\n",
    "    if word in ktw_10k:\n",
    "        words_in_model.append(word)\n",
    "        counter += 1\n",
    "print(f\"Number of words in pretrained wikipedia 10k model: {counter}\")\n",
    "\n",
    "unique_combinations_inmodel = {}\n",
    "unique_combinations_all = {}\n",
    "for comb in unique_combinations:\n",
    "    unique_combinations_all[comb] = {'input': [], 'hilo': ''}\n",
    "    # Check if all three words are in the 10k wikipidia model\n",
    "    if comb[0] in words_in_model and comb[1] in words_in_model and comb[2] in words_in_model:\n",
    "        unique_combinations_inmodel[comb] = {'input': [], 'hilo': ''}\n",
    "\n",
    "    \n",
    "        \n",
    "print(f\"\\033[32mNumber of unique combinations found in the 10k wikipidia model: {len(unique_combinations_inmodel)}\\033[0m\")\n",
    "print(f\"\\033[32mNumber of unique combinations found: {len(unique_combinations_all)}\\033[0m\")\n",
    "\n",
    "# Iterate over each row in the dataframe\n",
    "for _, row in df.iterrows():\n",
    "    key = (row['verb'], row['noun'], row['landmark'])\n",
    "    value = {'input': row['input'], 'hilo': row['hilo']}\n",
    "\n",
    "    unique_combinations_all[key]['input'].append(value['input'])\n",
    "    unique_combinations_all[key]['hilo'] = value['hilo']\n",
    "    \n",
    "    if key in unique_combinations_inmodel:\n",
    "        unique_combinations_inmodel[key]['input'].append(value['input'])\n",
    "        if unique_combinations_inmodel[key]['hilo'] == '':\n",
    "            unique_combinations_inmodel[key]['hilo'] = value['hilo']\n",
    "        else:\n",
    "            if unique_combinations_inmodel[key]['hilo'] != value['hilo']:\n",
    "                print(f\"Conflict in hilo for combination {key}: {unique_combinations_inmodel[key]['hilo']} vs {value['hilo']}\")\n",
    "\n",
    "unique_combinations_inmodel_high = {}\n",
    "unique_combinations_inmodel_low = {}\n",
    "high_samples_num = 0\n",
    "low_samples_num = 0\n",
    "for key in unique_combinations_inmodel:\n",
    "    input_values = unique_combinations_inmodel[key]['input']\n",
    "    if input_values:  # Ensure that the list is not empty\n",
    "        input_mean = sum(input_values) / len(input_values)\n",
    "        unique_combinations_inmodel[key]['input_mean'] = round(input_mean, 4)\n",
    "    if unique_combinations_inmodel[key]['hilo'] == 'high':\n",
    "        unique_combinations_inmodel_high[key] = unique_combinations_inmodel[key]\n",
    "        high_samples_num += len(input_values)\n",
    "    elif unique_combinations_inmodel[key]['hilo'] == 'low':\n",
    "        unique_combinations_inmodel_low[key] = unique_combinations_inmodel[key]\n",
    "        low_samples_num += len(input_values)\n",
    "    else:\n",
    "        print(f\"Unexpected hilo value for combination {key}: {unique_combinations_inmodel[key]['hilo']}\")\n",
    "\n",
    "for key in unique_combinations_all:\n",
    "    input_values = unique_combinations_all[key]['input']\n",
    "    if input_values:  # Ensure that the list is not empty\n",
    "        input_mean = sum(input_values) / len(input_values)\n",
    "        unique_combinations_all[key]['input_mean'] = round(input_mean, 4)\n",
    "     \n",
    "        \n",
    "print(f\"{len(unique_combinations_inmodel_high)} pairs of high in 10k wikipidia model(with {high_samples_num} samples): {unique_combinations_inmodel_high}\")\n",
    "print(f\"{len(unique_combinations_inmodel_low)} pairs of low in 10k wikipidia model(with {low_samples_num} samples): {unique_combinations_inmodel_low}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "UpperBound Spearman correlation (inter-subject agreement): 0.6604 with p-value 0.0001\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# leave-one-out Spearman correlation（UpperBound）\n",
    "\n",
    "df = pd.read_csv(data_file, sep=' ', header=None, names=[\"participant\", \"verb\", \"noun\", \"landmark\", \"input\", \"hilo\"], skiprows=1)\n",
    "# key: (verb, noun, landmark)\n",
    "pivot = df.pivot_table(index=[\"verb\", \"noun\", \"landmark\"], columns=\"participant\", values=\"input\")\n",
    "rhos, ps = [], []\n",
    "for participant in pivot.columns:\n",
    "    self_scores = pivot[participant]\n",
    "    others_mean = pivot.drop(columns=participant).mean(axis=1)\n",
    "    rho, p = spearmanr(self_scores, others_mean, nan_policy='omit')\n",
    "    rhos.append(rho)\n",
    "    ps.append(p)\n",
    "\n",
    "upper_bound_rho = sum(rhos) / len(rhos)\n",
    "upper_bound_p = sum(ps) / len(ps)\n",
    "print(f\"UpperBound Spearman correlation (inter-subject agreement): {upper_bound_rho:.4f} with p-value {upper_bound_p:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing verb: boom, noun: noise, landmark: thunder\n",
      "Additive Composition Similarity:  0.702345\n",
      "Multiplicative Composition Similarity:  0.46010327\n",
      "Combined Composition Similarity:  0.69281757\n"
     ]
    }
   ],
   "source": [
    "# (ii) - Compose the vectors of the extracted word pairs by testing different compositional functions\n",
    "# Did the paper point out any specific words vector models to use??\n",
    "# TODO: Change to additive_composition(vector1, vector2) and multiplicative_composition(vector1, vector2) functions to avoid calculate word vectors multiple times in (iii)\n",
    "import numpy as np\n",
    "import spacy\n",
    "import pandas as pd\n",
    "from scipy.spatial.distance import cosine\n",
    "\n",
    "enabled_fasttext_model = True\n",
    "if enabled_fasttext_model:\n",
    "    import fasttext.util\n",
    "    import fasttext\n",
    "    # fasttext.util.download_model('en', if_exists='ignore')\n",
    "    ft = fasttext.load_model('/home/gushuota@GU.GU.SE/model/fasttext/cc.en.300.bin')\n",
    "    # ft = fasttext.load_model('cc.en.300.bin')\n",
    "\n",
    "# get word vector from model\n",
    "# Here, we can easily change to other models\n",
    "def get_word_vector(word):\n",
    "    if enabled_fasttext_model:\n",
    "        word_vector = ft.get_word_vector(word)\n",
    "    else:\n",
    "        # loaded the pretrained wikipidia model, miss a lot of words\n",
    "        # word_vector = space_10k[word] \n",
    "        # word_vector = ppmispace_10k[word]\n",
    "        word_vector = svdspace_10k[word]\n",
    "    \n",
    "    # word_vector = normalize(word_vector)\n",
    "    return word_vector\n",
    "\n",
    "\n",
    "# cosine similarity\n",
    "def cosine_similarity(vec1, vec2):\n",
    "    similarity = 1 - cosine(vec1, vec2)\n",
    "    # print(f\"Cosine Similarity: {similarity}\")\n",
    "    if similarity == np.nan:\n",
    "        similarity = 0.0\n",
    "    return  similarity\n",
    "\n",
    "# compute additive composition of word vectors\n",
    "def additive_composition(vector1, vector2, weight1=1, weight2=1):\n",
    "    # return vector1*weight1 + vector2*weight2  # Weighted sum of the two vectors\n",
    "    return normalize(vector1*weight1 + vector2*weight2)\n",
    "\n",
    "\n",
    "# compute multiplicative composition of word vectors\n",
    "def multiplicative_composition(vector1, vector2):\n",
    "    # return vector1 * vector2  # Element-wise multiplication\n",
    "    return normalize(vector1 * vector2)\n",
    "\n",
    "# Compute the combined model by combining both additive and multiplicative compositions\n",
    "def combined_composition(vector1, vector2, alpha=0.5, beta=0.5):\n",
    "    # Compute additive and multiplicative compositions\n",
    "    additive_vector = vector1 + vector2\n",
    "    multiplicative_vector = vector1 * vector2\n",
    "    # Combine using the weighted sum\n",
    "    combined_vector = alpha * additive_vector + beta * multiplicative_vector\n",
    "\n",
    "    # return combined_vector\n",
    "    return normalize(combined_vector)\n",
    "\n",
    "# Example usage for the first pair in the dataset\n",
    "verb, noun, landmark = list(unique_combinations_inmodel_high.keys())[0]\n",
    "print(f\"Testing verb: {verb}, noun: {noun}, landmark: {landmark}\")\n",
    "\n",
    "verb_vector = get_word_vector(verb)\n",
    "noun_vector = get_word_vector(noun)\n",
    "landmark_vector = get_word_vector(landmark)\n",
    "\n",
    "additive_vector = additive_composition(verb_vector, noun_vector)\n",
    "additive_landmark_vector = additive_composition(landmark_vector, noun_vector)\n",
    "print(f\"Additive Composition Similarity: \", cosine_similarity(additive_vector, additive_landmark_vector))\n",
    "\n",
    "multiplicative_vector = multiplicative_composition(verb_vector, noun_vector)\n",
    "multiplicative_landmark_vector = multiplicative_composition(landmark_vector, noun_vector)\n",
    "print(f\"Multiplicative Composition Similarity: \", cosine_similarity(multiplicative_vector, multiplicative_landmark_vector))\n",
    "\n",
    "combine_vector = combined_composition(verb_vector, noun_vector)\n",
    "combine_landmark_vector = combined_composition(landmark_vector, noun_vector)\n",
    "print(f\"Combined Composition Similarity: \", cosine_similarity(combine_vector, combine_landmark_vector))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             Method  HighAvgScore  LowAvgScore  SpearmanCor  SpearmanPvalue\n",
      "0           NonComp      0.333687     0.313450     0.253610        0.005191\n",
      "1          Additive      0.686278     0.677492     0.046742        0.612188\n",
      "2  WeightedAdditive      0.427350     0.405725     0.222157        0.014741\n",
      "3    Multiplicative      0.440778     0.393511     0.337915        0.000160\n",
      "4          Combined      0.685325     0.676609     0.043999        0.633242\n",
      "5             Human      5.084542     3.286200     0.660356        0.000096\n"
     ]
    }
   ],
   "source": [
    "# (iii) - Compare the cosine similarity scores between vectors of phrases with the average human scores\n",
    "\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "# compare the cosine similarity for each phrase pair with human input\n",
    "human_scores_high = df[df['hilo'] == 'high']['input'].values\n",
    "human_scores_low = df[df['hilo'] == 'low']['input'].values\n",
    "\n",
    "# Collect the cosine similarities for the \"high\" and \"low\" tasks\n",
    "hscore_high, hscore_low = [], []\n",
    "base_cosin_sim_high, base_cosin_sim_low = [], []\n",
    "add_cosin_sim_high, add_cosin_sim_low = [], []\n",
    "weight_add_cosin_sim_high, weight_add_cosin_sim_low = [], []\n",
    "multipli_cosin_sim_high, multipli_cosin_sim_low = [], []\n",
    "combine_cosin_sim_high, combine_cosin_sim_low = [], []\n",
    "\n",
    "\n",
    "if enabled_fasttext_model:\n",
    "    test_unique_combinations = unique_combinations_all\n",
    "else:\n",
    "    test_unique_combinations = unique_combinations_inmodel\n",
    "\n",
    "for comb, values in test_unique_combinations.items():\n",
    "    verb, noun, landmark = comb\n",
    "    verb_vector = get_word_vector(verb)\n",
    "    noun_vector = get_word_vector(noun)\n",
    "    landmark_vector = get_word_vector(landmark)\n",
    "    \n",
    "    ## additive composition\n",
    "    add_com_vector = additive_composition(verb_vector, noun_vector)\n",
    "    add_landmark_vector = additive_composition(landmark_vector, noun_vector)\n",
    "    add_cosine_sim = cosine_similarity(add_com_vector, add_landmark_vector)\n",
    "\n",
    "    ## weighted additive composition\n",
    "    # For the best performing model the weight for the verb was 80% and for the noun 20%.\n",
    "    weight_add_com_vector = additive_composition(verb_vector, noun_vector, weight1=0.8, weight2=0.2)\n",
    "    weight_add_landmark_vector = additive_composition(landmark_vector, noun_vector, weight1=0.8, weight2=0.2)\n",
    "    weight_add_cosine_sim = cosine_similarity(weight_add_com_vector, weight_add_landmark_vector)\n",
    "    \n",
    "    ## multiplicative composition\n",
    "    multipli_com_vector = multiplicative_composition(verb_vector, noun_vector)\n",
    "    multipli_landmark_vector = multiplicative_composition(landmark_vector, noun_vector)\n",
    "    multipli_cosine_sim = cosine_similarity(multipli_com_vector, multipli_landmark_vector)\n",
    "    \n",
    "    ## combined composition\n",
    "    combine_com_vector = combined_composition(verb_vector, noun_vector, alpha=0.8, beta=0.2) # alpha=0.5, beta=0.5, alpha=0.4, beta=0.6\n",
    "    combine_landmark_vector = combined_composition(landmark_vector, noun_vector, alpha=0.4, beta=0.6)\n",
    "    combine_cosine_sim = cosine_similarity(combine_com_vector, combine_landmark_vector)\n",
    "\n",
    "    base_cosin_sim = cosine_similarity(verb_vector, landmark_vector)\n",
    "    \n",
    "    if values['hilo'] == 'high':\n",
    "        hscore_high.extend([values['input_mean']])\n",
    "        add_cosin_sim_high.extend([add_cosine_sim])\n",
    "        weight_add_cosin_sim_high.extend([weight_add_cosine_sim])\n",
    "        multipli_cosin_sim_high.extend([multipli_cosine_sim])\n",
    "        combine_cosin_sim_high.extend([combine_cosine_sim])\n",
    "        base_cosin_sim_high.extend([base_cosin_sim])\n",
    "    elif values['hilo'] == 'low':\n",
    "        hscore_low.extend([values['input_mean']])\n",
    "        add_cosin_sim_low.extend([add_cosine_sim])\n",
    "        weight_add_cosin_sim_low.extend([weight_add_cosine_sim])\n",
    "        multipli_cosin_sim_low.extend([multipli_cosine_sim])\n",
    "        combine_cosin_sim_low.extend([combine_cosine_sim])\n",
    "        base_cosin_sim_low.extend([base_cosin_sim])\n",
    "    else:\n",
    "        print(f\"Unexpected hilo value for combination {comb}: {values['hilo']}\")\n",
    "\n",
    "# Spearman rank correlation between cosine similarity and human scores\n",
    "add_spearman = spearmanr(hscore_high + hscore_low, add_cosin_sim_high + add_cosin_sim_low)\n",
    "weight_add_spearman = spearmanr(hscore_high + hscore_low, weight_add_cosin_sim_high + weight_add_cosin_sim_low)\n",
    "multipli_spearman = spearmanr(hscore_high + hscore_low, multipli_cosin_sim_high + multipli_cosin_sim_low)\n",
    "combine_spearman = spearmanr(hscore_high + hscore_low, combine_cosin_sim_high + combine_cosin_sim_low)\n",
    "base_spearman = spearmanr(hscore_high + hscore_low, base_cosin_sim_high + base_cosin_sim_low)\n",
    "\n",
    "\n",
    "# Calculate the average cosine similarity for high and low tasks\n",
    "avg_hscore_high = np.mean(hscore_high)\n",
    "avg_hscore_low = np.mean(hscore_low)\n",
    "\n",
    "avg_add_cosin_sim_high = np.mean(add_cosin_sim_high)\n",
    "avg_add_cosin_sim_low = np.mean(add_cosin_sim_low)\n",
    "avg_weight_add_cosin_sim_high = np.mean(weight_add_cosin_sim_high)\n",
    "avg_weight_add_cosin_sim_low = np.mean(weight_add_cosin_sim_low)\n",
    "\n",
    "avg_multipli_cosin_sim_high = np.mean(multipli_cosin_sim_high)\n",
    "avg_multipli_cosin_sim_low = np.mean(multipli_cosin_sim_low)\n",
    "\n",
    "avg_combine_cosin_sim_high = np.mean(combine_cosin_sim_high)\n",
    "avg_combine_cosin_sim_low = np.mean(combine_cosin_sim_low)\n",
    "\n",
    "avg_base_cosin_sim_high = np.mean(base_cosin_sim_high)\n",
    "avg_base_cosin_sim_low = np.mean(base_cosin_sim_low)\n",
    "\n",
    "# Print Spearman correlations for both high and low tasks\n",
    "data_print = {\n",
    "    \"Method\": [\"NonComp\", \"Additive\", \"WeightedAdditive\", \"Multiplicative\", \"Combined\", \"Human\"],\n",
    "    \"HighAvgScore\": [avg_base_cosin_sim_high, avg_add_cosin_sim_high,  avg_weight_add_cosin_sim_high, avg_multipli_cosin_sim_high, avg_combine_cosin_sim_high, avg_hscore_high],\n",
    "    \"LowAvgScore\": [avg_base_cosin_sim_low, avg_add_cosin_sim_low,  avg_weight_add_cosin_sim_low, avg_multipli_cosin_sim_low, avg_combine_cosin_sim_low, avg_hscore_low],\n",
    "    \"SpearmanCor\": [base_spearman.correlation, add_spearman.correlation, weight_add_spearman.correlation, multipli_spearman.correlation, combine_spearman.correlation, upper_bound_rho],\n",
    "    \"SpearmanPvalue\": [base_spearman.pvalue, add_spearman.pvalue, weight_add_spearman.pvalue, multipli_spearman.pvalue, combine_spearman.pvalue, upper_bound_p],\n",
    "    \n",
    "}\n",
    "\n",
    "# Create a DataFrame\n",
    "df_print = pd.DataFrame(data_print)\n",
    "print(df_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Any comments/thoughts should go here:**\n",
    "\n",
    "We evaluated five compositional functions—**NonCompositional (NonComp)**, **Additive**, **Weighted Additive**, **Multiplicative**, and **Combined**—using the sentence similarity dataset `mitchell_lapata_acl08.txt`, which contains 120 subject-verb-landmark triplets. The evaluation was based on each model's ability to:\n",
    "1. Distinguish between high and low similarity sentence pairs, and  \n",
    "2. Correlate with human similarity judgments, as measured by **Spearman’s rank correlation coefficient**.\n",
    "\n",
    "\n",
    "During the data preprocessing stage, we initially used the `wikipedia_10k` semantic space model as we used before. However, it only contained **8 complete triplets** from the evaluation dataset, which was insufficient for a robust analysis. Moreover, the resulting scores were not statistically significant.\n",
    "To overcome this limitation, we adopted the **`fasttext` model**, which provides broader vocabulary coverage. This change enabled us to evaluate all **120 sentence pairs** in the dataset, ensuring statistical validity and completeness of the results.\n",
    "\n",
    "\n",
    "Among the evaluated methods, the **Multiplicative model clearly worked better** than the others. It achieved the **highest Spearman correlation (ρ = 0.3379, p = 0.00016)** with human judgments and effectively distinguished between High similarity pairs (average score: **0.4408**) and Low similarity pairs (average score: **0.3935**).  A moderate effect size is indicated by the **Spearman correlation** between the human assessments and the model-calculated phrases, which is *ρ = 0.3379 < 0.5*. We can therefore infer from its effect size that, while not consistent across all samples, it captures a certain amount of semantic similarity that is consistent with human assessments.\n",
    "\n",
    " In contrast, The **Additive** and **Combined** models failed to show significant differences between high and low similarity items and their correlation with human ratings was near zero. The **Weighted Additive** model performed moderately better but still underperformed compared to the Multiplicative model and required parameter tuning.\n",
    "\n",
    "\n",
    "The **Multiplicative model** performs better lies in its ability to highlight **shared, meaningful features** between two word vectors. Unlike additive models, which simply average or sum the information and may weaken semantic signals, the multiplicative model emphasizes the dimensions where **both vectors have strong values**. This leads to more **focused representation**, Better **filtering of irrelevant components** and stronger emphasis on **semantically aligned features**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "[1] C. Silberer and M. Lapata. Learning grounded meaning representations with autoencoders. In Proceedings of the 52nd Annual Meeting of the Association for Computational Linguistics, pages 721–732, Baltimore, Maryland, USA, June 23–25 2014 2014. Association for Computational Linguistics.  \n",
    "\n",
    "[2] Mitchell, J., & Lapata, M. (2008). Vector-based Models of Semantic Composition. In Proceedings of ACL-08: HLT (pp. 236–244). Association for Computational Linguistics.\n",
    "  \n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "\n",
    "[4] E. Vylomova, L. Rimell, T. Cohn, and T. Baldwin. Take and took, gaggle and goose, book and read: Evaluating the utility of vector differences for lexical relation learning. arXiv, arXiv:1509.01692 [cs.CL], 2015."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "The assignment is marked on a 7-level scale where 4 is sufficient to complete the assignment; 5 is good solid work; 6 is excellent work, covers most of the assignment; and 7: creative work. \n",
    "\n",
    "This assignment has a total of 60 marks. These translate to grades as follows: 1 = 17% 2 = 34%, 3 = 50%, 4 = 67%, 5 = 75%, 6 = 84%, 7 = 92% where %s are interpreted as lower bounds to achieve that grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
