{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Making a count-based distributional model\n",
    "\n",
    "\n",
    "## A tiny space\n",
    "\n",
    "We start with a tiny corpus, so that we can easily inspect our data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "sam_corpus = \"\"\"I am Sam.\n",
    "Sam I am.\n",
    "I do not like green eggs and ham.\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to split the corpus into sentences using the Natural Language Toolkit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I am Sam.', 'Sam I am.', 'I do not like green eggs and ham.']"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "\n",
    "sam_sentences = nltk.sent_tokenize(sam_corpus)\n",
    "sam_sentences"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to get the context around a target word. To do this, we make a function that, for a target index, yields the words preceding and succeeding (if any). For simplicity, we use a one-word window on either side of the target. \n",
    "\n",
    "(It has \"yield\" instead of \"return\", so it is intended to be used in a for-loop.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def each_contextword_1wordwindow(wordlist, targetindex):\n",
    "    if targetindex > 0:\n",
    "        # preceding word\n",
    "        yield wordlist[targetindex - 1]\n",
    "        \n",
    "    if targetindex < len(wordlist)- 1:\n",
    "        # succeeding word\n",
    "        yield wordlist[targetindex + 1]\n",
    "\n",
    "\n",
    "def each_contextword_2wordwindow(wordlist, targetindex):\n",
    "    if targetindex > 1:\n",
    "        # preceding word 1\n",
    "        yield wordlist[targetindex - 1]\n",
    "        # preceding word 2\n",
    "        yield wordlist[targetindex - 2]\n",
    "    \n",
    "    if targetindex < len(wordlist) - 2:\n",
    "        # succeeding word 1\n",
    "        yield wordlist[targetindex + 1]\n",
    "        # succeeding word 2\n",
    "        yield wordlist[targetindex + 2]\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to use it: We count, for each target word, how often each context word appears with it. \n",
    "\n",
    "If you wanted to use a larger context, you could extend the function. For a context window of 2 words on either side (but not crossing sentence boundaries), for example, you would add two more `yield` statements to the function so that it also yields the word 2 indices before the target, and the word 2 indices after the target.\n",
    "\n",
    "## An aside: data structures for counting words\n",
    "\n",
    "As an aside, first a few quick words about data structures in NLTK that support us in counting words (or word groups, or pieces of syntactic structure). The first is basically a dictionary mapping words to counts, called a FreqDist. Conveniently, you can just initialize it by giving it a list of items, and it will count how often each item appears in the list:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'a': 2, 'b': 2, 'c': 1, 'd': 1})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NLTK data structures for counting stuff:\n",
    "# count individual words or other items:\n",
    "\n",
    "fd = nltk.FreqDist([\"a\", \"b\", \"c\", \"a\", \"b\", \"d\"])\n",
    "fd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second data structure relevant for us today is the ConditionalFreqDist. It also has counts, but it can be used to count, for each target, how often each context word appears, or more generally, how often each word appears given some other word. Say \"a\" is a target, and \"b\" and \"c\" are context items, then a ConditionalFreqDist can be used like a two-deep dictionary, whose first-level keys are called \"conditions\":"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 1 conditions>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# for targets, count context words,\n",
    "# or in general, for one sort of items, \n",
    "# count another sort of items\n",
    "cfd = nltk.ConditionalFreqDist()\n",
    "cfd[\"a\"][\"b\"] += 1\n",
    "cfd[\"a\"][\"c\"] += 1\n",
    "cfd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the \"condition\" 'a', the entry is again a FreqDist object that counts appearances of 'b' and 'c':"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also initialize a ConditionalFreqDist by a list of pairs. It then counts, for each first item of the pair, how often each second item appears. In the next example, the ConditionalFreqDist will record that given \"a\", both \"b\" and \"c\" appeared once, and that given \"d\", \"e\" appeared once:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<ConditionalFreqDist with 2 conditions>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd = nltk.ConditionalFreqDist([(\"a\", \"b\"), (\"a\", \"c\"), (\"d\", \"e\")])\n",
    "cfd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'b': 1, 'c': 1})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfd[\"a\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Counting words in the \"Sam corpus\"\n",
    "\n",
    "Now back to our Sam corpus. We can count context words for each target using a ConditionalFreqDist where the conditions are targets, and the keys of the FreqDist's are context words:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we will store the target words and their context word counts\n",
    "# this is a data type with method  conditions() to get the list of target words,\n",
    "# and sam_context_counts[t][cx] gets you the count for target t and context word cx\n",
    "sam_context_counts = nltk.ConditionalFreqDist()\n",
    "\n",
    "# iterate\n",
    "for sentence in sam_sentences:\n",
    "    wordlist = nltk.word_tokenize(sentence)\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            sam_context_counts[target][contextword] += 1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['I', 'am', 'Sam', '.', 'do', 'not', 'like', 'green', 'eggs', 'and', 'ham']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the target words from our corpus\n",
    "sam_context_counts.conditions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FreqDist({'green': 1, 'and': 1})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here are the counts for one target\n",
    "sam_context_counts[\"eggs\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is how to read this: For the target word \"eggs\", we've seen the word \"green\" once in its context, and the word \"and\" once in its context.\n",
    "\n",
    "Here are the context counts for all the targets in the corpus. The output shows first the target word, then the context words and their counts as a FreqDist object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('I', FreqDist({'am': 2, 'Sam': 1, 'do': 1})),\n",
       " ('am', FreqDist({'I': 2, 'Sam': 1, '.': 1})),\n",
       " ('Sam', FreqDist({'am': 1, '.': 1, 'I': 1})),\n",
       " ('.', FreqDist({'Sam': 1, 'am': 1, 'ham': 1})),\n",
       " ('do', FreqDist({'I': 1, 'not': 1})),\n",
       " ('not', FreqDist({'do': 1, 'like': 1})),\n",
       " ('like', FreqDist({'not': 1, 'green': 1})),\n",
       " ('green', FreqDist({'like': 1, 'eggs': 1})),\n",
       " ('eggs', FreqDist({'green': 1, 'and': 1})),\n",
       " ('and', FreqDist({'eggs': 1, 'ham': 1})),\n",
       " ('ham', FreqDist({'and': 1, '.': 1}))]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(sam_context_counts.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here's an example of how to access one count \n",
    "# for target \"I\" and context \"do\"\n",
    "sam_context_counts[\"I\"][\"do\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targets ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n",
      "Contexts ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n"
     ]
    }
   ],
   "source": [
    "# We put our rows and columns in order.\n",
    "# For now, our rows are the same as our columns:\n",
    "# All target words are also context words, and vice versa.\n",
    "# But that doesn't have to be the case.\n",
    "targetlist = sorted(sam_context_counts.conditions())\n",
    "contextlist = sorted(list(set(c for t in sam_context_counts.conditions() \n",
    "                              for c in sam_context_counts[t].keys())))\n",
    "\n",
    "print(\"Targets\", targetlist)\n",
    "print(\"Contexts\", contextlist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with numpy\n",
    "\n",
    "numpy is a Python package for working with vectors and matrices. Here is a vector:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "a = np.array([1,2,3])\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we can address entries in the vector by their index\n",
    "a[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3],\n",
       "       [4, 5, 6]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and here is a two-dimensional matrix\n",
    "m = np.array([[1,2,3], [4,5,6]])\n",
    "m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "6"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's address the 3rd entry in the 2nd row.\n",
    "# here we use *two* indices separated by comma\n",
    "m[1,2]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we put all our counts into a numpy matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 1, 1, 0, 0, 0, 0, 1, 0, 0],\n",
       "       [0, 0, 1, 2, 0, 1, 0, 0, 0, 0, 0],\n",
       "       [1, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [1, 2, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 1, 0, 0],\n",
       "       [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 1, 0, 0, 1, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 1, 0, 0, 1, 0],\n",
       "       [1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0],\n",
       "       [0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 1],\n",
       "       [0, 0, 0, 0, 0, 1, 0, 0, 0, 1, 0]])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sam_rows = [ ]\n",
    "for t in targetlist:\n",
    "    # for context words c for which we don't have an entry,\n",
    "    # the ConditionalFreqDist returns zero\n",
    "    sam_rows.append( [sam_context_counts[t][c] for c in contextlist])\n",
    "    \n",
    "sam_count_matrix = np.array(sam_rows)\n",
    "sam_count_matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But which row goes with which target, and which column goes with which context word? Let's make a lookup dictionary for this. We make separate dictionaries for targets and contexts; even though they are the same in our cases, that needn't always be so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "sam_target_dict = { }\n",
    "for index, target in enumerate(targetlist):\n",
    "    sam_target_dict[target] = index\n",
    "    \n",
    "sam_context_dict = { }\n",
    "for index, context in enumerate(contextlist):\n",
    "    sam_context_dict[context] = index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All targets: ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n",
      "row for Sam: 2\n",
      "and the row is: [1 1 0 1 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# now we can use it like this:\n",
    "print(\"All targets:\", targetlist)\n",
    "print(\"row for Sam:\", sam_target_dict[\"Sam\"])\n",
    "#notation: we only give *one* index here, to select the entire row\n",
    "print('and the row is:', sam_count_matrix[ sam_target_dict[\"Sam\"]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All contexts: ['.', 'I', 'Sam', 'am', 'and', 'do', 'eggs', 'green', 'ham', 'like', 'not']\n",
      "column for am: 3\n",
      "and the column is: [1 2 1 0 0 0 0 0 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "print(\"All contexts:\", contextlist)\n",
    "print(\"column for am:\", sam_context_dict[\"am\"])\n",
    "# look at the notation: the first index is :, for \"all rows\".\n",
    "# the second index is the column we want\n",
    "print(\"and the column is:\", sam_count_matrix[:, sam_context_dict[\"am\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cosine similarity\n",
    "\n",
    "Now we can compute cosine similarity in our space. Python has a built-in function for this -- with one catch: It computes 1 - cosine, as a distance. Here's how to get back to cosine similarity:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4714045207910318"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import scipy\n",
    "\n",
    "def cosine_sim(vec1, vec2):\n",
    "    return 1 - scipy.spatial.distance.cosine(vec1, vec2)\n",
    "\n",
    "cosine_sim(sam_count_matrix[ sam_target_dict[\"I\"]], \n",
    "           sam_count_matrix[ sam_target_dict[\"Sam\"]])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we want to build our own cosine similarity function, here's how to do this. I've made the implementation quite verbose so you can see what is what."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def ourcosine_sim(vec1, vec2):\n",
    "    # for the denominator: vector lengths\n",
    "    veclen1 = math.sqrt(sum(vec1**2))\n",
    "    veclen2 = math.sqrt(sum(vec2**2))\n",
    "    \n",
    "    # for the numerator: dot product\n",
    "    dotprod =  sum(vec1 * vec2)\n",
    "    \n",
    "    return dotprod / (veclen1 * veclen2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.47140452079103173"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ourcosine_sim(sam_count_matrix[ sam_target_dict[\"I\"]], \n",
    "              sam_count_matrix[ sam_target_dict[\"Sam\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From word co-occurrence counts to association weights\n",
    "\n",
    "As we discussed in class, raw frequency counts may not be what we want -- we don't need to know that all words co-occur a lot with \"the\" and \"a\". Even if we ditch stopwords, the frequency bias in the data may not be what we want: Do we need to know that all words co-occur a lot with \"said\"? \n",
    "\n",
    "Several methods have been developed for going from counts to association weights, including tf/idf and pointwise mutual information. Here, we demonstrate how to compute pointwise mutual information, defined as\n",
    "\n",
    "$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)}$ \n",
    "\n",
    "In the numerator, we have the joint probability of a *and* b. The formula compares this to the denominator, which has the product of the probability of a and the probability of b: If a and b were completely independent, had zero association, we would expect them to co-occur only by chance, that is, we would expect $P(a, b) = P(a)P(b)$. If $P(a, b)$ is larger than $P(a)P(b)$, then a and b are positively associated -- they co-occur more often than you would expect just from chance encounters. If $P(a, b)$ is smaller than $P(a)P(b)$, then a and b are negatively associated -- they really don't want to go together. \n",
    "\n",
    "In practice, we are often not interested in negative associations, and only use positive ones. Then we get PPMI:\n",
    "\n",
    "\n",
    "$PPMI(a, b) = \\left\\{\\begin{array}{ll}PMI(a, b) & \\text{if } PMI(a, b) > 0\\\\\n",
    "0 & \\text{else}\n",
    "\\end{array}\\right.$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target count for Sam 3\n",
      "overall count 28\n",
      "context item count for eggs 2\n"
     ]
    }
   ],
   "source": [
    "# Here are the pieces we need:\n",
    "# pointwise mutual information (PMI):\n",
    "#                    P(t, c)\n",
    "# PMI(t, c) = log --------------\n",
    "#                   P(t) P(c)\n",
    "#\n",
    "#    #(t, c): the co-occurrence count of t with c\n",
    "#    #(_, _): the sum of counts in the whole table, across all targets\n",
    "#    #(t, _): the sum of counts in the row of target t\n",
    "#    #(_, c): the sum of counts in the column of context item c\n",
    "#\n",
    "# then\n",
    "# P(t, c) = #(t, c) / #(_, _)\n",
    "# P(t) = #(t, _) / #(_, _)\n",
    "# P(c) = #(_, c) / #(_, _)\n",
    "#\n",
    "# PPMI(t, c) = { PMI(t, c) if PMI(t, c) >= 0\n",
    "#                0, else\n",
    "\n",
    "# target count #(t, _):\n",
    "print(\"target count for Sam\", sam_count_matrix[ sam_target_dict[\"Sam\"]].sum())\n",
    "\n",
    "# overall count #(_, _):\n",
    "print(\"overall count\", sam_count_matrix.sum())\n",
    "\n",
    "# context item count #(_, c):\n",
    "print(\"context item count for eggs\", sam_count_matrix[ :, sam_context_dict[\"eggs\"]].sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "from collections import defaultdict\n",
    "\n",
    "# we'll store the association weights in a dictionary for now\n",
    "sam_pmi = { }\n",
    "\n",
    "count_all = sam_count_matrix.sum()\n",
    "\n",
    "for target in targetlist:\n",
    "    for context in contextlist:\n",
    "        p_t_c = sam_count_matrix[ sam_target_dict[target], sam_context_dict[context]] / count_all\n",
    "        # print(\"p_t_c\", target, context, p_t_c)\n",
    "        p_t = sam_count_matrix[sam_target_dict[target]].sum() / count_all\n",
    "        # print(\"p_t\", target, p_t)\n",
    "        p_c = sam_count_matrix[:, sam_context_dict[context]].sum() / count_all\n",
    "        # print(\"p_c\", context, p_c)\n",
    "\n",
    "        # we need to watch out: if p_t_c is zero, the logarithm is undefined\n",
    "        if p_t_c == 0.0 or p_t == 0.0 or p_c == 0.0:\n",
    "            pmi = 0.0\n",
    "        else: \n",
    "            pmi = math.log( p_t_c / (p_t * p_c))\n",
    "        \n",
    "        # print(\"pmi\", target, context, pmi)\n",
    "        \n",
    "        if target not in sam_pmi: \n",
    "            # this is a dictionary that, where we haven't set a value,\n",
    "            # returns 0.0\n",
    "            sam_pmi[ target] = defaultdict(float)\n",
    "        sam_pmi[target][context]= pmi\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 1.13, 0.85, 0.  , 0.  , 0.  , 0.  , 1.54, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.85, 1.25, 0.  , 1.25, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [1.13, 0.85, 0.  , 0.85, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.85, 1.25, 0.85, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.95, 0.  , 1.95, 0.  , 0.  ],\n",
       "       [0.  , 1.25, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.95],\n",
       "       [0.  , 0.  , 0.  , 0.  , 1.95, 0.  , 0.  , 1.95, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.95, 0.  , 0.  , 1.95, 0.  ],\n",
       "       [1.54, 0.  , 0.  , 0.  , 1.95, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.95, 0.  , 0.  , 1.95],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 1.95, 0.  , 0.  , 0.  , 1.95, 0.  ]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# And we again store the result in a matrix\n",
    "\n",
    "sam_rows = [ ]\n",
    "for t in targetlist:\n",
    "    # for context words c for which we don't have an entry,\n",
    "    # the ConditionalFreqDist returns zero\n",
    "    sam_rows.append( [sam_pmi[t][c] for c in contextlist])\n",
    "    \n",
    "sam_pmi_matrix = np.array(sam_rows)\n",
    "# let's print the matrix with only 2 decimal points\n",
    "# to make it look prettier\n",
    "sam_pmi_matrix.round(2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counts for dot:\n",
      "\t [0 0 1 1 0 0 0 0 1 0 0]\n",
      "Associations for dot:\n",
      "\t [0.   0.   1.13 0.85 0.   0.   0.   0.   1.54 0.   0.  ]\n"
     ]
    }
   ],
   "source": [
    "# We can directly compare counts and associations\n",
    "print(\"Counts for dot:\")\n",
    "print(\"\\t\", sam_count_matrix[sam_target_dict[\".\"]])\n",
    "print(\"Associations for dot:\")\n",
    "print(\"\\t\", sam_pmi_matrix[sam_target_dict[\".\"]].round(2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cosine of 'I' and 'Sam', count-based 0.4714045207910318\n",
      "Cosine of 'I' and 'Sam', pmi-based 0.3274843356832646\n"
     ]
    }
   ],
   "source": [
    "# How has that changed cosines?\n",
    "print(\"Cosine of 'I' and 'Sam', count-based\", \n",
    "      cosine_sim(sam_count_matrix[sam_target_dict[\"I\"]], \n",
    "                 sam_count_matrix[sam_target_dict[\"Sam\"]]))\n",
    "print(\"Cosine of 'I' and 'Sam', pmi-based\", \n",
    "      cosine_sim(sam_pmi_matrix[sam_target_dict[\"I\"]], \n",
    "                 sam_pmi_matrix[sam_target_dict[\"Sam\"]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1.  , 0.49, 0.21, 0.27, 0.52, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.49, 1.  , 0.33, 0.21, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.45],\n",
       "       [0.21, 0.33, 1.  , 0.71, 0.  , 0.28, 0.  , 0.  , 0.43, 0.  , 0.  ],\n",
       "       [0.27, 0.21, 0.71, 1.  , 0.  , 0.39, 0.  , 0.  , 0.3 , 0.  , 0.  ],\n",
       "       [0.52, 0.  , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.28, 0.39, 0.  , 1.  , 0.  , 0.  , 0.  , 0.59, 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 1.  , 0.  , 0.55, 0.5 , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  , 0.  , 0.  , 0.5 ],\n",
       "       [0.  , 0.  , 0.43, 0.3 , 0.  , 0.  , 0.55, 0.  , 1.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.59, 0.5 , 0.  , 0.  , 1.  , 0.  ],\n",
       "       [0.  , 0.45, 0.  , 0.  , 0.  , 0.  , 0.  , 0.5 , 0.  , 0.  , 1.  ]])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Here is how to get all pairwise cosines in our matrix:\n",
    "\n",
    "# Step 1: this computes pairwise cosine *distances*, 1 - cosine\n",
    "sam_cosine_dist = scipy.spatial.distance.cdist(sam_pmi_matrix, sam_pmi_matrix, metric = \"cosine\")\n",
    "\n",
    "# Step 2: convert to cosine similarity\n",
    "sam_cosine_sim = 1 - sam_cosine_dist\n",
    "\n",
    "# Let's look at this. sam_cosine_sim is a numpy ndarray, which\n",
    "# has a method round() for rounding.\n",
    "# As you can see, the cosine similarity of each word with itself is 1,\n",
    "# and the value for \"i\" and \"sam\", row 2, column 3, is 0.33, as computed above.\n",
    "sam_cosine_sim.round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nearest neighbors\n",
    "\n",
    "We want to know about a word's nearest neighbors. Computing this by hand is a major pain: You would have to compute all pairwise cosines, and then rummage through them to find the maximum. In our tiny Sam corpus, this is feasible, but not in a large corpus. Fortunatly scikit-learn has a function NearestNeighbors that can do the work for us. One downside: It does not know cosine similarity outright. \n",
    "\n",
    "First option: We give it the cosine distance function that we used above. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {color: black;background-color: white;}#sk-container-id-1 pre{padding: 0;}#sk-container-id-1 div.sk-toggleable {background-color: white;}#sk-container-id-1 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-1 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-1 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-1 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-1 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-1 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-1 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-1 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-1 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-1 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-1 div.sk-item {position: relative;z-index: 1;}#sk-container-id-1 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-1 div.sk-item::before, #sk-container-id-1 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-1 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-1 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-1 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-1 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-1 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-1 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-1 div.sk-label-container {text-align: center;}#sk-container-id-1 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-1 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(metric=&lt;function cosine at 0x10d5d1940&gt;, n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(metric=&lt;function cosine at 0x10d5d1940&gt;, n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(metric=<function cosine at 0x10d5d1940>, n_neighbors=3)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "# we make a nearest-neighbors object and tell it we'll always want the 3 nearest neighbors at a time\n",
    "nearest_neighbors_obj = NearestNeighbors(n_neighbors=3, metric = scipy.spatial.distance.cosine)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj.fit(sam_pmi_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "cosine_distances, target_indices = nearest_neighbors_obj.kneighbors([sam_pmi_matrix[sam_target_dict[\"Sam\"]]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor is Sam with similarity 1.0\n",
      "Neighbor is am with similarity 0.707098356669986\n",
      "Neighbor is ham with similarity 0.42683129808958764\n"
     ]
    }
   ],
   "source": [
    "# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n",
    "# lists of values\n",
    "cosine_distances = cosine_distances[0].tolist()\n",
    "target_indices = target_indices[0].tolist()\n",
    "\n",
    "for cosinedist, targetindex in zip(cosine_distances, target_indices):\n",
    "    print(\"Neighbor is\", targetlist[targetindex], \"with similarity\", 1 - cosinedist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Second option: We use Euclidean distance (walking distance), but first normalize all vectors to be of length one. This won't give us the same distance values, but the same orderings of what is nearest. See https://stackoverflow.com/questions/34144632/using-cosine-distance-with-scikit-learn-kneighborsclassifier\n",
    "\n",
    "The length of a vector is defined as \n",
    "\n",
    "$||a|| = \\sqrt{\\sum_i a_i^2}$\n",
    "\n",
    "The Python package numpy has an implementation of that. For a vector `a`, we need to call `numpy.linalg.norm(a, ord = 2)`.\n",
    "\n",
    "Note: The sum of values of a vector is called its \"L1 norm\", and the vector length is called its \"L2 norm\". That's why we need the parameter `ord = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.  , 0.  , 0.54, 0.4 , 0.  , 0.  , 0.  , 0.  , 0.74, 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.43, 0.64, 0.  , 0.64, 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.69, 0.51, 0.  , 0.51, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.49, 0.72, 0.49, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.71, 0.  , 0.  ],\n",
       "       [0.  , 0.54, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.84],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.  , 0.71, 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.  , 0.71, 0.  ],\n",
       "       [0.62, 0.  , 0.  , 0.  , 0.78, 0.  , 0.  , 0.  , 0.  , 0.  , 0.  ],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.  , 0.71],\n",
       "       [0.  , 0.  , 0.  , 0.  , 0.  , 0.71, 0.  , 0.  , 0.  , 0.71, 0.  ]])"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "rows = [ sam_pmi_matrix[sam_target_dict[t]] / \n",
    "        np.linalg.norm(sam_pmi_matrix[sam_target_dict[t]], ord = 2)\n",
    "        for t in targetlist ]\n",
    "   \n",
    "sam_pminorm_matrix = np.array(rows)\n",
    "\n",
    "sam_pminorm_matrix.round(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-2 {color: black;background-color: white;}#sk-container-id-2 pre{padding: 0;}#sk-container-id-2 div.sk-toggleable {background-color: white;}#sk-container-id-2 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-2 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-2 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-2 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-2 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-2 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-2 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-2 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-2 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-2 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-2 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-2 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-2 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-2 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-2 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-2 div.sk-item {position: relative;z-index: 1;}#sk-container-id-2 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-2 div.sk-item::before, #sk-container-id-2 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-2 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-2 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-2 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-2 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-2 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-2 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-2 div.sk-label-container {text-align: center;}#sk-container-id-2 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-2 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-2\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>NearestNeighbors(n_neighbors=3)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-2\" type=\"checkbox\" checked><label for=\"sk-estimator-id-2\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">NearestNeighbors</label><div class=\"sk-toggleable__content\"><pre>NearestNeighbors(n_neighbors=3)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "NearestNeighbors(n_neighbors=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# we make a nearest-neighbors object \n",
    "# and tell it we'll always want the 2 nearest neighbors at a time.\n",
    "# Distance metric is default: \n",
    "# minkowski with p=2, which is equivalent to Euclidean distance\n",
    "nearest_neighbors_obj_2 = NearestNeighbors(n_neighbors=3)\n",
    "\n",
    "# we then allow it to compute an internal datastructure from our data\n",
    "nearest_neighbors_obj_2.fit(sam_pminorm_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neighbor is Sam with distance 0.6504565357441716\n",
      "Neighbor is am with distance 1.1789557107969613\n",
      "Neighbor is ham with distance 1.521536646024799\n"
     ]
    }
   ],
   "source": [
    "# and let's look at nearest neighbors again\n",
    "distances, target_indices = nearest_neighbors_obj_2.kneighbors([sam_pmi_matrix[sam_target_dict[\"Sam\"]]])\n",
    "\n",
    "# cosine_distances and target_indices are both two-dimensional arrays. Let's extract \n",
    "# lists of values\n",
    "distances = distances[0].tolist()\n",
    "target_indices = target_indices[0].tolist()\n",
    "\n",
    "for dist, targetindex in zip(distances, target_indices):\n",
    "    print(\"Neighbor is\", targetlist[targetindex], \"with distance\", dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Back to pre-computed spaces\n",
    "\n",
    "The pre-computed spaces that we tested in our previous notebook were offered within `gensim`, and they were in gensim format. But not all pre-computed spaces come as gensim data objects. Some pre-computed spaces are distributed as a matrix of numbers -- just like the ones we computed above for the Sam corpus. This is the case for example for the GloVE vectors from the original GloVe website: https://nlp.stanford.edu/projects/glove/\n",
    "\n",
    "To work with a space like that, you can use the same code as for the Sam corpus above.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# In case you are fast-forwarding through this notebook, let's quit here\n",
    "# in case you don't want to run the following more time-consuming\n",
    "# steps\n",
    "raise Exception(\"stopping fast-forward here\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using a somewhat larger corpus\n",
    "\n",
    "We next demonstrate a somewhat larger corpus, with yet another method of accessing the corpus data: If the data is available within the NLTK corpora, you can use the NLTK's corpus reader to access it.\n",
    "\n",
    "The Brown corpus is a 1 million word corpus of carefully selected text pieces from different genres, originally made to support dictionary-makers, so it's intended to cover a broad variety of genres in English."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The first few sentences of the Brown corpus:\n",
      "\n",
      "['The', 'Fulton', 'County', 'Grand', 'Jury', 'said', 'Friday', 'an', 'investigation', 'of', \"Atlanta's\", 'recent', 'primary', 'election', 'produced', '``', 'no', 'evidence', \"''\", 'that', 'any', 'irregularities', 'took', 'place', '.'] \n",
      "\n",
      "['The', 'jury', 'further', 'said', 'in', 'term-end', 'presentments', 'that', 'the', 'City', 'Executive', 'Committee', ',', 'which', 'had', 'over-all', 'charge', 'of', 'the', 'election', ',', '``', 'deserves', 'the', 'praise', 'and', 'thanks', 'of', 'the', 'City', 'of', 'Atlanta', \"''\", 'for', 'the', 'manner', 'in', 'which', 'the', 'election', 'was', 'conducted', '.'] \n",
      "\n",
      "['The', 'September-October', 'term', 'jury', 'had', 'been', 'charged', 'by', 'Fulton', 'Superior', 'Court', 'Judge', 'Durwood', 'Pye', 'to', 'investigate', 'reports', 'of', 'possible', '``', 'irregularities', \"''\", 'in', 'the', 'hard-fought', 'primary', 'which', 'was', 'won', 'by', 'Mayor-nominate', 'Ivan', 'Allen', 'Jr.', '.'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(\"The first few sentences of the Brown corpus:\\n\")\n",
    "for s in nltk.corpus.brown.sents()[:3]: \n",
    "    print(s, \"\\n\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We compute the target/context counts, noting context items as we go. We only count words that appear at least 10 times in the corpus. This cuts down a lot on the size of our matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('the', 62713),\n",
       " (',', 58334),\n",
       " ('.', 49346),\n",
       " ('of', 36080),\n",
       " ('and', 27915),\n",
       " ('to', 25732),\n",
       " ('a', 21881),\n",
       " ('in', 19536),\n",
       " ('that', 10237),\n",
       " ('is', 10011),\n",
       " ('was', 9777),\n",
       " ('for', 8841),\n",
       " ('``', 8837),\n",
       " (\"''\", 8789),\n",
       " ('The', 7258),\n",
       " ('with', 7012),\n",
       " ('it', 6723),\n",
       " ('as', 6706),\n",
       " ('he', 6566),\n",
       " ('his', 6466)]"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "brown_wordcounts = nltk.FreqDist(nltk.corpus.brown.words())\n",
    "brown_wordcounts.most_common(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "brown_context_counts = nltk.ConditionalFreqDist()\n",
    "\n",
    "frequency_threshold = 20\n",
    "\n",
    "for sentence in nltk.corpus.brown.sents():\n",
    "    # remove punctuation.\n",
    "    # at this point you could also remove stopwords\n",
    "    # or iterate over sents_tagged() instead of sents()\n",
    "    # to get parts of speech, and only retain\n",
    "    # content words\n",
    "    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                brown_context_counts[target][contextword] += 1   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this larger corpus, it now makes sense to look at some context word counts to get a sense of what the tables of counts tell us. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3)]\n",
      "love:\n",
      " [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10)]\n",
      "car: [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6)]\n"
     ]
    }
   ],
   "source": [
    "# 10 most frequent context words: similar across many items\n",
    "# (what can we do about that?)\n",
    "print(\"10 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(10))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(10))\n",
    "print(\"car:\", brown_context_counts[\"car\"].most_common(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100 most frequent contexts for some targets:\n",
      "\n",
      "election:\n",
      " [('the', 21), ('of', 11), ('to', 6), ('for', 4), ('was', 3), ('and', 3), ('his', 3), ('I', 3), ('on', 3), ('in', 3), ('an', 3), ('primary', 2), ('campaign', 2), ('last', 2), ('Presidential', 2), ('results', 2), ('November', 2), ('is', 2), ('April', 2), ('year', 2), ('produced', 1), ('laws', 1), ('general', 1), ('orderly', 1), ('8', 1), ('were', 1), ('investigation', 1), ('day', 1), ('told', 1), ('possible', 1), ('special', 1), ('might', 1), ('did', 1), (\"you'll\", 1), ('received', 1), ('they', 1), ('into', 1), ('bond', 1), ('The', 1), ('will', 1), ('Board', 1), ('Thursday', 1), ('His', 1), ('national', 1), ('close', 1), ('over', 1), ('procedures', 1), ('that', 1), ('inspired', 1), ('missed', 1), ('as', 1), ('falls', 1), ('law', 1), ('dates', 1), ('what', 1), ('plans', 1), ('may', 1)]\n",
      "love:\n",
      " [('of', 36), ('and', 34), ('in', 26), ('for', 22), ('to', 21), ('the', 17), ('with', 14), ('I', 13), ('you', 10), ('is', 10), ('that', 7), ('my', 6), ('it', 5), ('we', 5), ('as', 4), ('her', 4), ('this', 4), ('a', 4), ('him', 4), ('us', 3), ('not', 3), ('his', 3), ('but', 3), ('His', 3), ('only', 3), ('true', 3), (\"God's\", 2), ('through', 2), ('without', 2), ('Christian', 2), ('just', 2), ('them', 2), ('was', 2), ('songs', 2), (\"I'd\", 2), ('by', 2), ('romantic', 2), ('My', 2), ('made', 2), ('could', 2), ('can', 1), ('Eisenhower', 1), ('boys', 1), ('children', 1), ('somebody', 1), ('little', 1), (\"mother's\", 1), ('first', 1), ('light', 1), ('deep', 1), ('wisdom', 1), ('common', 1), ('God', 1), ('men', 1), ('earlier', 1), ('people', 1), ('They', 1), ('mutual', 1), ('who', 1), ('great', 1), ('he', 1), ('shade', 1), ('poems', 1), ('another', 1), ('lies', 1), ('force', 1), ('inspired', 1), ('If', 1), ('reflects', 1), ('then', 1), ('on', 1), ('has', 1), ('are', 1), ('impossible', 1), ('nor', 1), ('reality', 1), ('This', 1), ('belongs', 1), ('whose', 1), ('lost', 1), ('more', 1), ('memory', 1), ('letters', 1), ('about', 1), ('knowledge', 1), ('discovered', 1), ('story', 1), ('which', 1), ('song', 1), (\"didn't\", 1), ('they', 1), ('Lord', 1), ('all', 1), ('among', 1), ('way', 1), ('honor', 1), ('she', 1), ('bringing', 1), ('faith', 1), ('would', 1)]\n",
      "car:\n",
      " [('the', 84), ('a', 36), ('and', 19), ('his', 18), ('was', 13), ('with', 13), ('The', 10), ('is', 8), ('police', 7), ('that', 6), ('in', 6), ('big', 5), ('motor', 5), ('your', 5), ('parked', 5), ('my', 5), ('to', 4), ('he', 4), ('driven', 4), ('little', 4), ('of', 4), ('second', 4), ('had', 4), ('other', 3), ('new', 3), ('any', 3), ('her', 3), ('but', 3), ('by', 3), ('sports', 3), ('as', 3), ('could', 3), ('will', 3), ('for', 3), ('coming', 3), ('I', 3), ('on', 3), ('passing', 2), ('sales', 2), ('itself', 2), ('which', 2), ('speed', 2), ('box', 2), ('another', 2), ('at', 2), ('before', 2), ('industry', 2), ('until', 2), ('freight', 2), ('she', 2), ('into', 2), ('agency', 2), ('their', 2), ('door', 2), ('approaching', 2), ('here', 2), ('patrol', 2), ('own', 2), ('it', 2), ('old', 2), ('Street', 1), ('apart', 1), ('flying', 1), ('family', 1), ('throw', 1), (\"mother's\", 1), ('wanted', 1), ('heading', 1), ('number', 1), ('when', 1), ('they', 1), ('evidently', 1), ('used', 1), ('brushed', 1), ('New', 1), ('wrong', 1), ('now', 1), ('one', 1), ('herself', 1), (\"they'd\", 1), (\"isn't\", 1), ('foreign', 1), ('you', 1), ('coat', 1), ('My', 1), ('like', 1), ('Army', 1), ('smaller', 1), ('free', 1), ('back', 1), ('private', 1), ('driving', 1), ('must', 1), ('determine', 1), ('much', 1), ('buying', 1), ('covering', 1), ('more', 1), ('model', 1), ('might', 1)]\n"
     ]
    }
   ],
   "source": [
    "# 100 most frequent context words: now we are starting to see differences.\n",
    "# We also see that many of the 100 most frequent context words only have counts of one.\n",
    "print(\"100 most frequent contexts for some targets:\\n\")\n",
    "print(\"election:\\n\", brown_context_counts[\"election\"].most_common(100))\n",
    "print(\"love:\\n\", brown_context_counts[\"love\"].most_common(100))\n",
    "print(\"car:\\n\", brown_context_counts[\"car\"].most_common(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Some ambiguous words:\n",
      "bat:\n",
      " []\n",
      "bank:\n",
      " [('the', 23), ('of', 9), ('a', 4), ('and', 4), ('in', 3), ('The', 2), ('local', 2), ('south', 2), ('which', 2), ('east', 2), ('west', 2), ('over', 1), ('accounts', 1), ('customers', 1), ('have', 1), ('That', 1), ('installed', 1), ('said', 1), ('is', 1), ('policy', 1), ('cloud', 1), ('that', 1), ('would', 1), ('for', 1), ('handling', 1), ('president', 1), ('to', 1), ('with', 1), ('officials', 1), ('before', 1), ('by', 1), ('loans', 1), ('left', 1), ('wrong', 1), ('far', 1), ('soft', 1), ('toward', 1), ('through', 1), ('river', 1), ('high', 1), ('outside', 1), ('big', 1), ('roll', 1)]\n",
      "bar:\n",
      " [('the', 34), ('and', 10), ('locking', 10), ('a', 4), ('Af', 3), ('in', 3), ('while', 2), ('he', 2), ('to', 2), ('patent', 2), ('Would', 1), ('vehicles', 1), ('without', 1), ('cocktail', 1), ('which', 1), ('come', 1), ('at', 1), ('held', 1), ('A', 1), ('stock', 1), ('on', 1), ('should', 1), ('our', 1), ('is', 1), ('as', 1), ('who', 1), ('with', 1), ('so', 1), ('absolute', 1), ('not', 1), ('finding', 1), ('little', 1), ('next', 1), ('was', 1), ('breakfast', 1), ('kitchen', 1), ('off', 1), ('headed', 1), ('same', 1), ('some', 1), ('if', 1), ('windows', 1), ('running', 1), ('but', 1), ('top', 1), ('crowded', 1)]\n",
      "leave:\n",
      " [('to', 79), ('the', 41), ('and', 17), ('it', 14), ('you', 10), ('him', 9), ('I', 8), ('not', 8), ('her', 8), ('his', 6), ('a', 6), ('this', 5), ('they', 4), ('my', 4), ('for', 4), ('France', 4), ('me', 4), ('here', 3), ('will', 3), ('with', 3), ('of', 3), ('but', 3), ('on', 3), ('their', 3), ('or', 3), ('just', 3), (\"can't\", 3), ('behind', 2), ('we', 2), ('us', 2), ('them', 2), ('because', 2), ('in', 2), ('at', 2), (\"couldn't\", 2), ('cannot', 2), ('from', 2), ('better', 2), ('never', 2), ('before', 2), ('all', 2), ('that', 2), (\"wouldn't\", 2), (\"he'd\", 2), ('could', 2), (\"I'll\", 2), ('one', 2), ('Eisenhower', 1), ('Capitol', 1), ('Sunday', 1), ('have', 1), ('both', 1), ('can', 1), ('until', 1), ('Parker', 1), ('clay', 1), ('seldom', 1), ('no', 1), ('some', 1), ('received', 1), ('he', 1), ('simply', 1), ('earlier', 1), ('through', 1), ('people', 1), ('out', 1), ('ever', 1), ('room', 1), ('replied', 1), ('Mr.', 1), ('may', 1), ('London', 1), ('therefore', 1), ('our', 1), ('everything', 1), ('Rome', 1), ('General', 1), ('Under', 1), ('administrative', 1), ('then', 1), ('which', 1), ('is', 1), ('dominant', 1), ('trying', 1), ('also', 1), ('1', 1), ('should', 1), ('New', 1), ('take', 1), ('must', 1), ('off', 1), ('only', 1), ('need', 1), ('would', 1), ('But', 1), ('around', 1), ('your', 1), (\"don't\", 1), ('than', 1), (\"'em\", 1)]\n"
     ]
    }
   ],
   "source": [
    "# some ambiguous words\n",
    "print(\"Some ambiguous words:\")\n",
    "print(\"bat:\\n\", brown_context_counts[\"bat\"].most_common(100))\n",
    "print(\"bank:\\n\", brown_context_counts[\"bank\"].most_common(100))\n",
    "print(\"bar:\\n\", brown_context_counts[\"bar\"].most_common(100))\n",
    "print(\"leave:\\n\", brown_context_counts[\"leave\"].most_common(100))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Selecting words to work with\n",
    "\n",
    "We only keep words above a particular frequency threshold. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first determine the list of all words in Brown\n",
    "# repeat: frequency threshold\n",
    "frequency_threshold = 20\n",
    "\n",
    "brown_wordlist = list(w for w in brown_wordcounts if brown_wordcounts[w] >= frequency_threshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make a dictionary that maps each word to its index in the wordlist\n",
    "brown_wordlist_lookup = { }\n",
    "\n",
    "for index, word in enumerate(brown_wordlist):\n",
    "    brown_wordlist_lookup[word] = index "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# We need an array with enough space to hold \n",
    "# len(brown_wordlist) target words, and\n",
    "# len(brown_wordlist) context words.\n",
    "# We first initialize it to all zeros.\n",
    "brown_count_matrix = np.zeros((len(brown_wordlist), len(brown_wordlist)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, let's do the context word counting with this matrix.\n",
    "\n",
    "import string\n",
    "\n",
    "for sentence in nltk.corpus.brown.sents():\n",
    "    # remove punctuation.\n",
    "    # at this point you could also remove stopwords\n",
    "    # or iterate over sents_tagged() instead of sents()\n",
    "    # to get parts of speech, and only retain\n",
    "    # content words\n",
    "    wordlist = [w for w in sentence if w.strip(string.punctuation) != \"\"]\n",
    "    \n",
    "    for targetindex, target in enumerate(wordlist):\n",
    "        for contextword in each_contextword_1wordwindow(wordlist, targetindex):\n",
    "            if brown_wordcounts[target] >= frequency_threshold and brown_wordcounts[contextword] >= frequency_threshold:\n",
    "                # which cell in the matrix is this? \n",
    "                # look up both the target and the context word\n",
    "                # in the ordered list of Brown words\n",
    "                targetindex_matrix = brown_wordlist_lookup[target]\n",
    "                contextindex_matrix = brown_wordlist_lookup[contextword]\n",
    "                # and add a count of one for this cell in the matrix\n",
    "                brown_count_matrix[targetindex_matrix][contextindex_matrix] += 1   \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8750717787803817"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# We can again compute similarity in this space\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_count_matrix[said_index], brown_count_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now compute PMI again, making use of the fact that\n",
    "\n",
    "$PMI(a, b) = \\log \\frac{P(a, b)}{P(a)P(b)} = \\log\\frac{P(b|a)}{P(b)}$\n",
    "\n",
    "We re-implement PMI to run more quickly: \n",
    "numpy offers functions that apply an operation to a whole vector at once,\n",
    "rather than one at a time. \n",
    "This is much quicker."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# And we can compute PMI again.\n",
    "# numpy offers functions that\n",
    "# apply an operation to a whole vector at once,\n",
    "# rather than one at a time. \n",
    "# This is much quicker.\n",
    "\n",
    "count_all = brown_count_matrix.sum()\n",
    "\n",
    "# probability of contexts/columns:\n",
    "# this is our P(b)\n",
    "# This is a vector with a probability for each context\n",
    "# sum(axis=0) is numpy's way of saying that we want to sum each column\n",
    "col_totals = brown_count_matrix.sum(axis=0)\n",
    "# avoid zeros, they get us in trouble later when we divide by p_c\n",
    "col_totals[col_totals == 0] = 0.00001\n",
    "# this is a vector where each row total is divided by the overall count\n",
    "p_c = col_totals / count_all\n",
    "\n",
    "# probability of context given target:\n",
    "# this is our P(b|a)\n",
    "# sum(axis=1) is numpy's way of saying that we want to sum each row:\n",
    "# we divide each row by its row total, getting the probability of a context item\n",
    "# within this target. \n",
    "# do do this, we flip the matrix on its side so that columns become rows,\n",
    "# then do the division (otherwise numpy would do column-wise instead of row-wise division)\n",
    "row_totals = brown_count_matrix.sum(axis=1).astype(float)\n",
    "row_totals[row_totals == 0] = 0.00001\n",
    "p_c_given_t = (brown_count_matrix.T / row_totals).T\n",
    "\n",
    "# PMI: log( P(b|a) / P(b))\n",
    "# we again divide a matrix by a vector\n",
    "# this time we do want column-wise division\n",
    "# so we don't have to flip the matrix\n",
    "pct_divided_by_pc = p_c_given_t / p_c\n",
    "# avoid doing log(0)by replacing 0 by a small number\n",
    "pct_divided_by_pc[pct_divided_by_pc==0] = 0.00001\n",
    "\n",
    "brown_pmi_matrix = np.log(pct_divided_by_pc)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9244136837103797"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and computing similarity again\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_pmi_matrix[said_index], brown_pmi_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dimensionality reduction\n",
    "# Principal component analysis\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "pcaobj = PCA()\n",
    "brown_pca_matrix = pcaobj.fit_transform(brown_pmi_matrix)\n",
    "\n",
    "# and let's actually reduce dimensionality\n",
    "\n",
    "keep_this_many_dimensions = 100\n",
    "\n",
    "brown_pca_matrix = brown_pca_matrix[:, :keep_this_many_dimensions]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When we compute similarity again, the absolute value of the cosine similarity has changed a lot -- but that does not make a big difference. Absolute similarity values can vary widely across spaces, but they may still predict the same nearest neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.43305558791233933"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# and computing similarity again\n",
    "said_index = brown_wordlist_lookup[\"said\"]\n",
    "wrote_index = brown_wordlist_lookup[\"wrote\"]\n",
    "cosine_sim( brown_pca_matrix[said_index], brown_pca_matrix[wrote_index])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
