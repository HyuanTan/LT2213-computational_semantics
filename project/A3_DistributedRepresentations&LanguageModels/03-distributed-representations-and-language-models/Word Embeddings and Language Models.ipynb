{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A3: Word Embeddings and Language Modelling\n",
    "\n",
    "Created by Adam Ek, modified by Ricardo Muñoz Sánchez and Simon Dobnik"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "In this lab we will explore constructing *static* word embeddings (i.e. word2vec) and building language models. We'll also evaluate these systems on intermediate tasks, namely word similarity and identifying \"good\" and \"bad\" sentences.\n",
    "\n",
    "* For this we'll use pytorch.\n",
    "    * You can install it using the instructions from here: https://pytorch.org/\n",
    "    * If you would like to check out some tutorials on how to use it, you can can do so here: https://pytorch.org/tutorials/beginner/basics/intro.html\n",
    "    * Some basic operations that will be useful for you can be found here: https://jhui.github.io/2018/02/09/PyTorch-Basic-operations\n",
    "* We are not interested in getting state-of-the-art performance, focus on the implementation and not results of your model.\n",
    "    * For this reason, you can use a subset of the dataset: the first 5000-10 000 sentences or so.\n",
    "    * On linux or mac you can use: ```head -n 10000 inputfile > outputfile```. \n",
    "* Using GPUs will make things run faster.\n",
    "    * You can access the server by using SSH: ```ssh -L 8888:localhost:8888 [your_x_account]@mltgpu.flov.gu.se -p 62266```\n",
    "        * ```ssh``` tells the computer to connect remotely to the server.\n",
    "        * ```-L 8888:localhost:8888``` allows you to connect using jupyter notebooks, you can remove it if you don't want to do that.\n",
    "        * ```-p 62266``` tells the server to give you access through port 62266.\n",
    "    * You can also connect to the server using VSCode, available for Mac, Linux, and Windows.\n",
    "    * I would suggest you to set up a virtual environment on the server, such as virtual env or conda.\n",
    "    * When using pytorch on the server, remember to install the GPU-compatible version!\n",
    "    * You can also use Google Collab for free (with a monthly quota for GPU usage). We highly suggest you to use the MLT server instead, though."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "# If you're using GPUs, replace \"cpu\" with \"cuda:n\" where n is the index of the GPU\n",
    "# device = torch.device('cpu')\n",
    "device = torch.device('cuda:3') # nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec embeddings\n",
    "\n",
    "In this first part we'll construct a word2vec model which will give us *static* word embeddings (that is, they are fixed after training).\n",
    "\n",
    "After we've trained our model we will evaluate the embeddings obtained on a word similarity task."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Formatting data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First we need to load some data from data/wiki-corpus.50000.txt. The file contains 50 000 sentences randomly selected from the complete wikipedia. Each line in the file contains one sentence. The sentences are whitespace tokenized.\n",
    "\n",
    "Your first task is to create a dataset suitable for word2vec. That is, we define some ```window_size``` then iterate over all sentences in the dataset, putting the target word in one column and the context words in another (separate the columns with ```tab```). ```window_size=n``` means that we select ```n/2``` tokens to the right and left of the center word.\n",
    "\n",
    "For example, the sentece \"this is a lab and exercise\" with ```window size = 4``` will be converted to 6 (target, context) pairs:\n",
    "```\n",
    "target      context\n",
    "----------------------------\n",
    "this        is, a\n",
    "is          this, a, lab\n",
    "a           this, is, lab\n",
    "lab         is, a, and, exercise\n",
    "and         a, lab, exercise\n",
    "exercise    lab, and \n",
    "```\n",
    "\n",
    "this will be our training examples for the word2vec model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# data_path = './data/test.wiki-corpus.10000.txt'\n",
    "data_path = './data/wiki-corpus.50000.txt'\n",
    "WINDOW_SIZE = 4\n",
    "''' \n",
    "def corpus_reader(data_path, window_size=4, min_freq=4):\n",
    "    all_data = []\n",
    "    vocabulary = set(['<pad>'])\n",
    "    with open(data_path) as f:\n",
    "        # go over the lines (sentences in the files)\n",
    "        ...\n",
    "        # split sentences into tokens\n",
    "        ...\n",
    "        # save all indiviual words to the vocabulary\n",
    "        ...\n",
    "        # extract all (center word, context) with `window_size=4`, pairs from the sentence\n",
    "        ...\n",
    "        # save (center word, context) pairs into a dataset\n",
    "        ...\n",
    "    \n",
    "    # filter out words which does not occur often\n",
    "    ...\n",
    "    \n",
    "    # create a mapping from words to integers. \n",
    "    # each word should have an unique integer mapped to it. \n",
    "    # use a dictionary for this.\n",
    "    word_to_idx = ...\n",
    "    return all_data, word_to_idx\n",
    "'''\n",
    "\n",
    "def corpus_reader(data_path, window_size=4, min_freq=4):\n",
    "    all_data = []\n",
    "    vocabulary = set(['<pad>'])\n",
    "    word_freq = {}\n",
    "\n",
    "    # First pass: build vocabulary and count word frequencies\n",
    "    with open(data_path, 'r', encoding='utf-8') as f:\n",
    "        sentences = []\n",
    "        for line in f:\n",
    "            tokens = line.strip().split()\n",
    "            sentences.append(tokens)\n",
    "            for word in tokens:\n",
    "                vocabulary.add(word)\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "    # Filter out infrequent words\n",
    "    vocabulary = {word for word in vocabulary if word_freq.get(word, 0) >= min_freq or word == '<pad>'}\n",
    "\n",
    "    # Second pass: extract (center, context) pairs\n",
    "    for tokens in sentences:\n",
    "        tokens = [word for word in tokens if word in vocabulary]\n",
    "        for idx, center_word in enumerate(tokens):\n",
    "            context = []\n",
    "            for i in range(max(0, idx - window_size // 2), min(len(tokens), idx + window_size // 2 + 1)):\n",
    "                if i != idx:\n",
    "                    context.append(tokens[i])\n",
    "            if context:\n",
    "                all_data.append((center_word, context))\n",
    "\n",
    "    # Create word to index mapping\n",
    "    word_to_idx = {word: idx for idx, word in enumerate(sorted(vocabulary))}\n",
    "\n",
    "    return all_data, word_to_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We sampled 50 000 senteces completely random from the *whole* wikipedia for our training data. Give some reasons why this is good, and why it might be bad. (*note*: We'll have a few questions like these, one or two reasons for and against is sufficient)\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random sampling ensures a broad and diverse vocabulary and topics, helping the model generalize better across different types of language and contexts. Random selection also reduces the chance of introducing topic or style bias that might happen if you only select certain categories or types of articles.\n",
    "But random data can include a lot of uncommon, technical, or noisy sentences that don't contribute much to learning general word relationships. Also if our downstream task is focused on a specific domain (e.g., medical text), random Wikipedia data might not provide relevant context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the data\n",
    "\n",
    "We need to create a dataloader now. That is, some way of generating a batch of examples from the dataset. A batch is a set of ```n``` examples from the data.\n",
    "\n",
    "The recipe for a dataloader is as follows:\n",
    "\n",
    "* Select n examples from the dataset\n",
    "* (a) Translate each example into integers using `word_to_idx`\n",
    "* (b) Transform the translated examples to pytorch tensors\n",
    "* (c) Return the batch \n",
    "* Select n new examples from the dataset\n",
    "* ... repeat steps (a-c)\n",
    "\n",
    "The dataloader should stop when it have read the whole dataset.\n",
    "\n",
    "This can be done either by first computing all the batches in the dataset and returning it as a list which you can then iterate over, or as an generator that returns each batch after it has been created.\n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import namedtuple\n",
    "Batch = namedtuple('Batch', ['target_word', 'context'])\n",
    "\n",
    "'''\n",
    "def batcher(dataset, word_to_idx, batch_size=8):\n",
    "    # iterate over the dataset\n",
    "    \n",
    "    # select a batch of size `batch_size`\n",
    "    \n",
    "    # translate batch to integers using `word_to_idx`\n",
    "    \n",
    "    # add padding to the context\n",
    "    \n",
    "    # transform the batch to a pytorch tensor\n",
    "    \n",
    "    # return the dataset of batches/indiviual batches \n",
    "    batch = Batch(target_word, context)\n",
    "'''\n",
    "\n",
    "def batcher(dataset, word_to_idx, batch_size=8):\n",
    "    # Helper function to pad context lists\n",
    "    def pad_contexts(contexts, pad_value):\n",
    "        max_len = max(len(c) for c in contexts)\n",
    "        padded = []\n",
    "        for c in contexts:\n",
    "            padded.append(c + [pad_value] * (max_len - len(c)))\n",
    "        return padded\n",
    "\n",
    "    # Go through the dataset in steps of batch_size\n",
    "    for i in range(0, len(dataset), batch_size):\n",
    "        batch_samples = dataset[i:i + batch_size]\n",
    "\n",
    "        # Convert target words and contexts to indices\n",
    "        target_word_indices = [word_to_idx[target] for target, _ in batch_samples]\n",
    "        context_indices = [[word_to_idx[word] for word in context] for _, context in batch_samples]\n",
    "\n",
    "        # Pad context lists so they're all the same length\n",
    "        context_indices_padded = pad_contexts(context_indices, word_to_idx['<pad>'])\n",
    "\n",
    "        # Convert to tensors\n",
    "        target_tensor = torch.tensor(target_word_indices, dtype=torch.long)\n",
    "        context_tensor = torch.tensor(context_indices_padded, dtype=torch.long)\n",
    "\n",
    "        # Create a batch and yield it\n",
    "        batch = Batch(target_word=target_tensor, context=context_tensor)\n",
    "        yield batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We lower-cased all tokens above; give some reasons why this is a good idea, and why it may be harmful to our embeddings.\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lower-casing can reduces vocabulary size. By treating \"Apple\" and \"apple\" as the same word, we avoid data sparsity and make the model easier to train, especially for rare capitalized forms. It can also handles inconsistent capitalization. In real-world text, the same word might appear capitalized or lowercase inconsistently.\n",
    "Lower-casing may also be harmful. Like it may loss some meaning. Capitalization sometimes conveys important information (e.g., \"Apple\" the company vs. \"apple\" the fruit). Lowercasing removes this distinction. Proper nouns and acronyms may lose their identity, which could degrade the quality of embeddings in tasks needing entity recognition."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word Embeddings Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will implement the CBOW model for constructing word embedding models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the CBOW model we try to predict the center word based on the context. That is, we take as input ```n``` context words, encode them as vectors, then combine them by summation. This will give us one embedding. We then use this embedding to predict *which* word in our vocabuary is the most likely center word. \n",
    "\n",
    "Implement this model \n",
    "\n",
    "[7 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        # where the embeddings of words are stored \n",
    "        # each word in the vocabulary should have one embedding assigned to it\n",
    "        self.embeddings = ...\n",
    "        # a transformation that predicts a word from the vocabulary\n",
    "        self.prediction = ...\n",
    "    \n",
    "    def forward(self, context):\n",
    "        # translate a batch to embeddings\n",
    "        embedded_context = ...\n",
    "        # reduce dimensions of the embeddings\n",
    "        projection = ...\n",
    "        # predict the target word from the vocabulary\n",
    "        predictions = ...\n",
    "        \n",
    "        return predictions\n",
    "        \n",
    "    def projection_function(self, xs):\n",
    "        \"\"\"\n",
    "        This function will take as input a tensor of size (B, S, D)\n",
    "        where B is the batch_size, S the window size, and D the dimensionality of embeddings\n",
    "        this function should compute the sum over the embedding dimensions of the input, \n",
    "        that is, we transform (B, S, D) to (B, 1, D) or (B, D) \n",
    "        \"\"\"\n",
    "        xs_sum = ...\n",
    "        return xs_sum\n",
    "'''\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class CBOWModel(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim):\n",
    "        super(CBOWModel, self).__init__()\n",
    "        # Embedding layer: each word gets a vector of size embedding_dim\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        \n",
    "        # Linear layer to project from embedding space back to vocabulary space\n",
    "        self.prediction = nn.Linear(embedding_dim, vocab_size)\n",
    "\n",
    "    def forward(self, context):\n",
    "        \"\"\"\n",
    "        context: Tensor of shape (batch_size, context_size)\n",
    "        \"\"\"\n",
    "        # Look up embeddings for context words\n",
    "        embedded_context = self.embeddings(context)  # shape: (batch_size, context_size, embedding_dim)\n",
    "        \n",
    "        # Reduce dimension: sum over context words\n",
    "        projection = self.projection_function(context, embedded_context)  # shape: (batch_size, embedding_dim)\n",
    "        \n",
    "        # Predict the center word\n",
    "        predictions = self.prediction(projection)  # shape: (batch_size, vocab_size)\n",
    "        \n",
    "        return predictions\n",
    "\n",
    "    def projection_function(self, context, xs):\n",
    "        \"\"\"\n",
    "        xs: Tensor of shape (batch_size, context_size, embedding_dim)\n",
    "        Return: Tensor of shape (batch_size, embedding_dim)\n",
    "        \"\"\"\n",
    "        # Sum the embeddings across the context dimension (dim=1)\n",
    "        xs_sum = xs.sum(dim=1)\n",
    "        return xs_sum\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we need to train the models. First we define which hyperparameters to use. (You can change these, for example when *developing* your model you can use a batch size of 2 and a very low dimensionality (say 10), just to speed things up). When actually training your model *fo real*, you can use a batch size of [8,16,32,64], and embedding dimensionality of [128,256]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_embeddings_hyperparameters = {'epochs':10,\n",
    "                                   'batch_size':8,\n",
    "                                   'learning_rate':0.001,\n",
    "                                   'embedding_dim':128}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train your model. Iterate over the dataset, get outputs from your model, calculate loss and backpropagate.\n",
    "\n",
    "We mentioned in the lecture that we use Negative Log Likelihood (https://pytorch.org/docs/stable/generated/torch.nn.NLLLoss.html) loss to train Word2Vec model. In this lab we'll take a shortcut when *training* and use Cross Entropy Loss (https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html), basically it combines ```log_softmax``` and ```NLLLoss```. So what your model should output is a *score* for each word in our vocabulary. The ```CrossEntropyLoss``` will then assign probabilities and calculate the negative log likelihood loss.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 6.5166\n",
      "Epoch 2, Average Loss: 6.2189\n",
      "Epoch 3, Average Loss: 6.0259\n",
      "Epoch 4, Average Loss: 5.8948\n",
      "Epoch 5, Average Loss: 5.8024\n",
      "Epoch 6, Average Loss: 5.7349\n",
      "Epoch 7, Average Loss: 5.6850\n",
      "Epoch 8, Average Loss: 5.6485\n",
      "Epoch 9, Average Loss: 5.6219\n",
      "Epoch 10, Average Loss: 5.5969\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# load data\n",
    "dataset, vocab = get_data(...)\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        context = batch.context\n",
    "        target_word = batch.target_word\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = cbow_model(context)\n",
    "        \n",
    "        # compute the loss, you'll need to reshape the input\n",
    "        # you can read more about this is the documentation for\n",
    "        # CrossEntropyLoss\n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        ...\n",
    "        \n",
    "        # update parameters\n",
    "        ...\n",
    "        \n",
    "        # reset gradients\n",
    "        ...\n",
    "    print()\n",
    "'''\n",
    "\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "import random\n",
    "def get_data(data_path='wiki-corpus.txt', window_size=4, min_freq=4, batch_size=16, shuffle=True):\n",
    "    all_data, word_to_idx = corpus_reader(data_path, window_size=window_size, min_freq=min_freq)\n",
    "\n",
    "    if shuffle:\n",
    "        random.shuffle(all_data)\n",
    "\n",
    "    batches = list(batcher(all_data, word_to_idx, batch_size=batch_size))\n",
    "\n",
    "    return batches, word_to_idx\n",
    "\n",
    "# load data\n",
    "dataset, vocab = get_data(\n",
    "    data_path=data_path,\n",
    "    window_size=4,\n",
    "    min_freq=4,\n",
    "    batch_size=word_embeddings_hyperparameters['batch_size'],\n",
    "    shuffle=True\n",
    ")\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "cbow_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=word_embeddings_hyperparameters['learning_rate'])\n",
    "\n",
    "# start training loop\n",
    "for epoch in range(word_embeddings_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, batch in enumerate(dataset):\n",
    "        context = batch.context.to(device)       # move to GPU/CPU\n",
    "        target_word = batch.target_word.to(device)\n",
    "\n",
    "        # forward pass\n",
    "        output = cbow_model(context)  # shape: (batch_size, vocab_size)\n",
    "\n",
    "        # compute the loss (CrossEntropy expects (batch_size, vocab_size) and target as (batch_size))\n",
    "        loss = loss_fn(output, target_word)\n",
    "\n",
    "        # backward pass\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        loss.backward()        # compute gradients\n",
    "        optimizer.step()       # update parameters\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    # print average epoch loss\n",
    "    avg_loss = total_loss / (i + 1)\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(cbow_model.state_dict(), 'cbow_model.pth')\n",
    "# Save the vocabulary\n",
    "with open('vocab.txt', 'w') as f:\n",
    "    for word, idx in vocab.items():\n",
    "        f.write(f\"{word}\\t{idx}\\n\")\n",
    "\n",
    "# Load the model\n",
    "loaded_cbow_model = CBOWModel(len(vocab), word_embeddings_hyperparameters['embedding_dim'])\n",
    "# loaded_cbow_model.load_state_dict(torch.load('cbow_model.pth'))\n",
    "state_dict = torch.load('cbow_model.pth', weights_only=True)  # PyTorch 2.2+\n",
    "loaded_cbow_model.load_state_dict(state_dict)\n",
    "\n",
    "loaded_cbow_model.to(device)\n",
    "loaded_cbow_model.eval()  # Set the model to evaluation mode\n",
    "# Load the vocabulary\n",
    "loaded_cbow_vocab = {}\n",
    "with open('vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word, idx = line.strip().split('\\t')\n",
    "        loaded_cbow_vocab[word] = int(idx)\n",
    "# Check if the loaded model and vocabulary are the same as the original\n",
    "for key in cbow_model.state_dict():\n",
    "    if not torch.equal(cbow_model.state_dict()[key], loaded_cbow_model.state_dict()[key]):\n",
    "        print(f\"Mismatch in parameter: {key}\")\n",
    "        assert False, \"Model state dicts do not match!\"\n",
    "assert vocab == loaded_cbow_vocab, \"Vocabulary does not match!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluating the model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will evaluate the model on a dataset of word similarities, WordSim353 (http://alfonseca.org/eng/research/wordsim353.html , also avalable in the data folder). The first thing we need to do is read the dataset and translate it to integers. What we'll do is to reuse the ```Field``` that records word indexes (the second output of ```get_data()```) and use it to parse the file.\n",
    "\n",
    "The wordsim data is structured as follows:\n",
    "\n",
    "```\n",
    "word1 word2 score\n",
    "...\n",
    "```\n",
    "\n",
    "\n",
    "The ```Field``` we got from ```read_data()``` has two built-in functions, ```stoi``` which maps a string to an integer and ```itos``` which maps an integer to a string. \n",
    "\n",
    "What our datareader needs to do is: \n",
    "\n",
    "```\n",
    "for line in file:\n",
    "    word1, word2, score = file.split()\n",
    "    # encode word1 and word2 as integers\n",
    "    word1_idx = vocab.vocab.stoi[word1]\n",
    "    word2_idx = vocab.vocab.stoi[word2]\n",
    "```\n",
    "\n",
    "when we have the integers for ```word_1``` and ```word2``` we'll compute the similarity between their word embeddings with *cosine simlarity*. We can obtain the embeddings by querying the embedding layer of the model.\n",
    "\n",
    "We calculate the cosine similarity for each word pair in the dataset, then compute the pearson correlation between the similarities we obtained with the scores given in the dataset. \n",
    "\n",
    "[4 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pearson correlation between human scores and model similarities: 0.2884\n",
      "Spearman correlation between human scores and model similarities: 0.2880, p-value: 0.0005\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = f.split()\n",
    "            \n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "            \n",
    "            # get the index for the word\n",
    "            word1_idx = ...\n",
    "            word2_idx = ...\n",
    "            \n",
    "            # get the embedding of the word\n",
    "            word1_emb = ...\n",
    "            word2_emb = ...\n",
    "            \n",
    "            # compute cosine similarity, we'll use the version included in pytorch functional\n",
    "            # https://pytorch.org/docs/master/generated/torch.nn.functional.cosine_similarity.html\n",
    "            cosine_similarity = F.cosine_similarity(...)\n",
    "            \n",
    "            model_sims.append(cosine_similarity.item())\n",
    "    \n",
    "    return dataset_sims, model_sims\n",
    "\n",
    "path = 'wordsim_similarity_goldstandard.txt'\n",
    "data, model = read_wordsim(...)\n",
    "pearson_correlation = np.corrcoef(data, model)\n",
    "            \n",
    "# the non-diagonals give the pearson correlation,\n",
    "print(pearson_correlation)\n",
    "'''\n",
    "\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "def read_wordsim(path, vocab, embeddings):\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "            score = float(score)\n",
    "            dataset_sims.append(score)\n",
    "\n",
    "            # Get indices for word1 and word2, or skip if not in vocab\n",
    "            if word1 not in vocab or word2 not in vocab:\n",
    "                continue\n",
    "\n",
    "            word1_idx = vocab[word1]\n",
    "            word2_idx = vocab[word2]\n",
    "            \n",
    "\n",
    "            # Get embeddings for both words\n",
    "            # word1_emb = embeddings(word1_idx).unsqueeze(0)  # shape (1, D)\n",
    "            # word2_emb = embeddings(word2_idx).unsqueeze(0)  # shape (1, D)\n",
    "            word1_emb = embeddings(torch.tensor([word1_idx], device=device))\n",
    "            word2_emb = embeddings(torch.tensor([word2_idx], device=device))\n",
    "\n",
    "            # Compute cosine similarity\n",
    "            cosine_similarity = F.cosine_similarity(word1_emb, word2_emb).item()\n",
    "            model_sims.append(cosine_similarity)\n",
    "\n",
    "    return dataset_sims[:len(model_sims)], model_sims  # align lengths in case of missing words\n",
    "\n",
    "\n",
    "path_wordsim_similarity = './data/wordsim_similarity_goldstandard.txt'  # or the correct path to your wordsim file\n",
    "\n",
    "# data_sim, model_sim = read_wordsim(path_wordsim_similarity, vocab, cbow_model.embeddings)\n",
    "data_sim, model_sim = read_wordsim(path_wordsim_similarity, loaded_cbow_vocab, loaded_cbow_model.embeddings)\n",
    "\n",
    "# Pearson correlation\n",
    "pearson_correlation = np.corrcoef(data_sim, model_sim)[0, 1]\n",
    "print(f\"Pearson correlation between human scores and model similarities: {pearson_correlation:.4f}\")\n",
    "\n",
    "rho, pval = stats.spearmanr(data_sim, model_sim)\n",
    "print(f\"Spearman correlation between human scores and model similarities: {rho:.4f}, p-value: {pval:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do you think the model performs good or bad? Why?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model’s performance depends on the Pearson correlation we obtained. If the correlation is high (e.g., > 0.5), it means the embeddings capture semantic similarity well and the model performs decently. However, Word2Vec with a small dataset and simple CBOW architecture often achieves moderate correlation (0.2 to 0.3). Limitations like small training data, small embedding size, and lack of subword information can reduce performance. So, the model likely performs moderately, not as well as more advanced models like BERT or GloVe trained on massive corpora."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select the 10 best and 10 worst performing word pairs, can you see any patterns that explain why *these* are the best and worst word pairs?\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 10 best performing pairs:\n",
      "man-woman: human=8.30, model=0.59\n",
      "coast-shore: human=9.10, model=0.58\n",
      "type-kind: human=8.97, model=0.53\n",
      "skin-eye: human=6.22, model=0.51\n",
      "direction-combination: human=2.25, model=0.51\n",
      "student-professor: human=6.81, model=0.49\n",
      "situation-conclusion: human=4.81, model=0.48\n",
      "development-issue: human=3.97, model=0.48\n",
      "football-basketball: human=6.81, model=0.48\n",
      "planet-sun: human=8.02, model=0.47\n",
      "\n",
      "Top 10 worst performing pairs:\n",
      "precedent-cognition: human=2.81, model=-0.12\n",
      "volunteer-motto: human=2.56, model=-0.06\n",
      "bread-butter: human=6.19, model=-0.05\n",
      "precedent-group: human=1.77, model=-0.02\n",
      "architecture-century: human=3.78, model=-0.02\n",
      "calculation-computation: human=8.44, model=-0.02\n",
      "benchmark-index: human=4.25, model=-0.01\n",
      "money-cash: human=9.15, model=-0.00\n",
      "money-dollar: human=8.42, model=0.00\n",
      "media-radio: human=7.42, model=0.02\n"
     ]
    }
   ],
   "source": [
    "def evaluate_word_pairs(wordsim_path, vocab, model_embeddings):\n",
    "    word_pairs = []\n",
    "    dataset_sims = []\n",
    "    model_sims = []\n",
    "\n",
    "    with open(wordsim_path) as f:\n",
    "        for line in f:\n",
    "            word1, word2, score = line.split()\n",
    "            score = float(score)\n",
    "\n",
    "            # Only use pairs where both words are in the vocabulary\n",
    "            if word1 not in vocab or word2 not in vocab:\n",
    "                continue\n",
    "\n",
    "            word1_idx = vocab[word1]\n",
    "            word2_idx = vocab[word2]\n",
    "\n",
    "            word1_emb = model_embeddings(torch.tensor([word1_idx], device=device))\n",
    "            word2_emb = model_embeddings(torch.tensor([word2_idx], device=device))\n",
    "\n",
    "            cosine_similarity = F.cosine_similarity(word1_emb, word2_emb).item()\n",
    "\n",
    "            word_pairs.append((word1, word2))\n",
    "            dataset_sims.append(score)\n",
    "            model_sims.append(cosine_similarity)\n",
    "\n",
    "    results = list(zip(word_pairs, dataset_sims, model_sims))\n",
    "\n",
    "    # Sort by smallest difference (best performance)\n",
    "    best_10 = sorted(results, key=lambda x: x[2], reverse=True)[:10]\n",
    "\n",
    "    # Sort by largest difference (worst performance)\n",
    "    worst_10 = sorted(results, key=lambda x: x[2])[:10]\n",
    "\n",
    "    return best_10, worst_10\n",
    "\n",
    "# Example usage:\n",
    "best_10, worst_10 = evaluate_word_pairs(path_wordsim_similarity, vocab, cbow_model.embeddings)\n",
    "\n",
    "print(\"Top 10 best performing pairs:\")\n",
    "for (w1, w2), human, model in best_10:\n",
    "    print(f\"{w1}-{w2}: human={human:.2f}, model={model:.2f}\")\n",
    "\n",
    "print(\"\\nTop 10 worst performing pairs:\")\n",
    "for (w1, w2), human, model in worst_10:\n",
    "    print(f\"{w1}-{w2}: human={human:.2f}, model={model:.2f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The best-performing pairs are usually obvious synonyms or related concepts (e.g., man-woman, coast-shore, type-kind), both model and human have high similarity score. While there also some world pairs that the model have a > 0.5 score (e.g., direction-combination) and human have very low score.\n",
    "\n",
    "The worst-performing pairs often including rare words or named entities that don’t appear frequently in the corpus; polysemous words with multiple meanings or simmilar (e.g., money-cash vs. money-dollar); Antonyms which can be close in context but opposite in meaning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some ways of improving the model we apply to WordSim353.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase training data: Use more sentences from Wikipedia or other corpora to improve vocabulary coverage and context diversity.\n",
    "\n",
    "Increase embedding dimension: Use a larger embedding size (128 → 256 or 512) for better capacity to capture relationships.\n",
    "\n",
    "Train longer: More epochs can improve learning, especially with larger data.\n",
    "\n",
    "Use subword information: Like FastText, which can help with rare or unseen words.\n",
    "\n",
    "Try Skip-gram model: Sometimes performs better than CBOW for small datasets.\n",
    "\n",
    "Using bigger batch-size to avoid vibration in parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we consider a scenario where we use these embeddings in a downstream task, for example sentiment analysis (roughly: determining whether a sentence is positive or negative). \n",
    "\n",
    "Give some examples why the sentiment analysis model would benefit from our embeddnings and one examples why our embeddings could hur the performance of the sentiment model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Help:\n",
    "\n",
    "The embeddings capture semantic similarity, so sentiment words (good, great, excellent) will have similar vectors, making it easier for the sentiment model to recognize positive sentiment even with varied wording.\n",
    "\n",
    "Contextual clues learned from training (e.g., disaster and bad appearing together) improve downstream performance.\n",
    "\n",
    "Hurt:\n",
    "\n",
    "If the embeddings don’t capture sentiment polarity well (e.g., good and bad are close because they appear in similar contexts), the sentiment classifier might confuse opposites.\n",
    "\n",
    "Outdated or biased data from Wikipedia might result in embeddings that don’t reflect modern usage or subtle sentiment cues."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Language modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this second part we'll build a simple LSTM language model. Your task is to construct a model which takes a sentence as input and predicts the next word for each word in the sentence. For this you'll use the ```LSTM``` class provided by PyTorch (https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). You can read more about the LSTM here: https://colah.github.io/posts/2015-08-Understanding-LSTMs/\n",
    "\n",
    "NOTE!!!: Use the same dataset (wiki-corpus.50000.txt) as before.\n",
    "\n",
    "Our setup is similar to before, we first encode the words as distributed representations then pass these to the LSTM and for each output we predict the next word.\n",
    "\n",
    "To create a gold standard (what we want to predict), we need to manipulate the tensor containing the sentence. As wi want to predict the *next* word, we want the following setup (where `w_n` is the index of a word in the sentence, `x` is the input words, and `y` is the gold words):\n",
    "\n",
    "$x = [w_0, w_1, w_2, w_3, w_4]$\n",
    "\n",
    "$y = [w_1, w_2, w_3, w_4, w_5]$\n",
    "\n",
    "That is, to create the gold standard we need to shift the index `n` of the input by `+1`, as this gives us the next word.\n",
    "\n",
    "\n",
    "For this we'll build a new dataloader, the file we pass to the dataloader should contain one sentence per line, with words separated by whitespace.\n",
    "\n",
    "```\n",
    "word_1, ..., word_n\n",
    "word_1, ..., word_k\n",
    "...\n",
    "```\n",
    "\n",
    "in this dataloader you want to make sure that each sentence begins with a ```<start>``` token and ends with a ```<end>``` token. But other than that, just as before you read the dataset and output an iterator over the dataset, a vocabulary, and a mapping from words to indices. \n",
    "\n",
    "Implement the dataloader, language model and the training loop (the training loop will basically be the same as for word2vec).\n",
    "\n",
    "[12 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# you can change these numbers to suit your needs as before\n",
    "lm_hyperparameters = {'epochs':10,\n",
    "                      'batch_size':8,\n",
    "                      'learning_rate':0.001,\n",
    "                      'embedding_dim':128,\n",
    "                      'output_dim':128}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "data_path = 'wiki-corpus.txt'\n",
    "def get_data():\n",
    "    # your code here, roughly the same as for the word2vec dataloader\n",
    "'''\n",
    "\n",
    "def get_data(data_path='wiki-corpus.txt', batch_size=16, min_freq=4):\n",
    "    vocabulary = {'<pad>': 0, '<start>': 1, '<end>': 2}\n",
    "    word_freq = {}\n",
    "\n",
    "    sentences = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().lower().split()\n",
    "            for word in tokens:\n",
    "                word_freq[word] = word_freq.get(word, 0) + 1\n",
    "\n",
    "    # Assign indices to words above min_freq\n",
    "    idx = 3\n",
    "    for word, count in word_freq.items():\n",
    "        if count >= min_freq:\n",
    "            vocabulary[word] = idx\n",
    "            idx += 1\n",
    "\n",
    "    # Prepare dataset: for each sentence, create input and shifted output\n",
    "    data = []\n",
    "    with open(data_path, 'r') as f:\n",
    "        for line in f:\n",
    "            tokens = line.strip().lower().split()\n",
    "            tokens = ['<start>'] + [w for w in tokens if w in vocabulary] + ['<end>']\n",
    "            indices = [vocabulary[w] for w in tokens]\n",
    "\n",
    "            # Skip too short\n",
    "            if len(indices) < 2:\n",
    "                continue\n",
    "\n",
    "            data.append(indices)\n",
    "\n",
    "    # Create batches\n",
    "    batches = []\n",
    "    for i in range(0, len(data), batch_size):\n",
    "        batch_sentences = data[i:i + batch_size]\n",
    "        max_len = max(len(s) for s in batch_sentences)\n",
    "\n",
    "        x_batch = []\n",
    "        y_batch = []\n",
    "\n",
    "        for sentence in batch_sentences:\n",
    "            x = sentence[:-1] + [vocabulary['<pad>']] * (max_len - 1 - len(sentence) + 1)\n",
    "            y = sentence[1:] + [vocabulary['<pad>']] * (max_len - 1 - len(sentence) + 1)\n",
    "            x_batch.append(x)\n",
    "            y_batch.append(y)\n",
    "\n",
    "        batches.append((torch.tensor(x_batch), torch.tensor(y_batch)))\n",
    "\n",
    "    return batches, vocabulary\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(...):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "        self.embeddings = ...\n",
    "        self.LSTM = nn.LSTM(self, input_size=..., hidden_size=...)\n",
    "        self.predict_word = ...\n",
    "    \n",
    "    def forward(self, seq):\n",
    "        # extract embeddings for the sentence\n",
    "        embedded_seq = ...\n",
    "        # compute contextual representations\n",
    "        timestep_reprentation, *_ = self.LSTM(embedded_seq)\n",
    "        # predict a token from the vocabulary at each timestep\n",
    "        predicted_words = ...\n",
    "        \n",
    "        return predicted_words\n",
    " '''\n",
    "\n",
    "class LM_withLSTM(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_size):\n",
    "        super(LM_withLSTM, self).__init__()\n",
    "\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.LSTM = nn.LSTM(input_size=embedding_dim, hidden_size=hidden_size, batch_first=True)\n",
    "        self.predict_word = nn.Linear(hidden_size, vocab_size)\n",
    "\n",
    "    def forward(self, seq):\n",
    "        embedded_seq = self.embeddings(seq)  # (batch, seq_len, embedding_dim)\n",
    "        timestep_representation, _ = self.LSTM(embedded_seq)  # (batch, seq_len, hidden_size)\n",
    "        predicted_words = self.predict_word(timestep_representation)  # (batch, seq_len, vocab_size)\n",
    "        return predicted_words\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Average Loss: 5.3982\n",
      "Epoch 2, Average Loss: 4.8418\n",
      "Epoch 3, Average Loss: 4.6003\n",
      "Epoch 4, Average Loss: 4.4285\n",
      "Epoch 5, Average Loss: 4.2926\n",
      "Epoch 6, Average Loss: 4.1783\n",
      "Epoch 7, Average Loss: 4.0788\n",
      "Epoch 8, Average Loss: 3.9908\n",
      "Epoch 9, Average Loss: 3.9112\n",
      "Epoch 10, Average Loss: 3.8408\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# load data\n",
    "dataset, vocab = get_data(...)\n",
    "\n",
    "# build model and construct loss/optimizer\n",
    "lm_model = LM_withLSTM(len(vocab), \n",
    "                       lm_hyperparameters['embedding_dim'],\n",
    "                       lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = CrossEntropyLoss()\n",
    "optimizer = optim.Adam(cbow_model.parameters(), lr=lm_hyperparameters['lr'])\n",
    "\n",
    "# start training loop\n",
    "total_loss = 0\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    for i, batch in enumerate(dataset):\n",
    "        \n",
    "        # the strucure for each BATCH is:\n",
    "        # <start>, w0, ..., wn, <end>\n",
    "        sentence = batch.sentence\n",
    "        \n",
    "        # when training the model, at each input we predict the *NEXT* token\n",
    "        # consequently there is nothing to predict when we give the model \n",
    "        # <end> as input. \n",
    "        # thus, we do not want to give <end> as input to the model, select \n",
    "        # from each batch all tokens except the last. \n",
    "        # tip: use pytorch indexing/slicing (same as numpy) \n",
    "        # (https://pytorch.org/tutorials/beginner/basics/tensorqs_tutorial.html#operations-on-tensors)\n",
    "        # (https://jhui.github.io/2018/02/09/PyTorch-Basic-operations/)\n",
    "        input_sentence = ...\n",
    "        \n",
    "        # send your batch of sentences to the model\n",
    "        output = lm_model(input_sentence)\n",
    "        \n",
    "        # for each output, the model predict the NEXT token, so we have to reshape \n",
    "        # our dataset again. On timestep t, we evaluate on token t+1. That is,\n",
    "        # we never predict the <start> token ;) so this time, we select all but the first \n",
    "        # token from sentences (that is, all the tokens that we predict)\n",
    "        gold_data = ...\n",
    "        \n",
    "        # the shape of the output and sentence variable need to be changed,\n",
    "        # for the loss function. Details are in the documentation.\n",
    "        # You can use .view(...,...) to reshape the tensors  \n",
    "        loss = loss_fn(...)\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print average loss for the epoch\n",
    "        print(total_loss/(i+1), end='\\r') \n",
    "        \n",
    "        # compute gradients\n",
    "        ...\n",
    "        \n",
    "        # update parameters\n",
    "        ...\n",
    "        \n",
    "        # reset gradients\n",
    "        ...\n",
    "    print()\n",
    "'''\n",
    "\n",
    "# device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# Load data\n",
    "dataset, vocab = get_data(data_path=data_path, batch_size=lm_hyperparameters['batch_size'], min_freq=4)\n",
    "\n",
    "# Build model\n",
    "lm_model = LM_withLSTM(len(vocab), lm_hyperparameters['embedding_dim'], lm_hyperparameters['output_dim'])\n",
    "lm_model.to(device)\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss(ignore_index=vocab['<pad>'])  # Don't penalize padding predictions\n",
    "optimizer = optim.Adam(lm_model.parameters(), lr=lm_hyperparameters['learning_rate'])\n",
    "\n",
    "# Training loop\n",
    "for epoch in range(lm_hyperparameters['epochs']):\n",
    "    total_loss = 0\n",
    "\n",
    "    for i, (x_batch, y_batch) in enumerate(dataset):\n",
    "        x_batch = x_batch.to(device)\n",
    "        y_batch = y_batch.to(device)\n",
    "\n",
    "        # Model input: all but last token\n",
    "        input_sentence = x_batch\n",
    "\n",
    "        # Gold output: all but first token\n",
    "        gold_data = y_batch\n",
    "\n",
    "        # Forward pass\n",
    "        output = lm_model(input_sentence)  # (batch, seq_len, vocab_size)\n",
    "\n",
    "        # Reshape for loss: (batch * seq_len, vocab_size) vs (batch * seq_len)\n",
    "        output_flat = output.view(-1, output.size(-1))\n",
    "        gold_flat = gold_data.view(-1)\n",
    "\n",
    "        loss = loss_fn(output_flat, gold_flat)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch {epoch+1}, Average Loss: {total_loss / (i+1):.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model\n",
    "torch.save(lm_model.state_dict(), 'lm_lstm_model.pth')\n",
    "# Save the vocabulary\n",
    "with open('lm_vocab.txt', 'w') as f:\n",
    "    for word, idx in vocab.items():\n",
    "        f.write(f\"{word}\\t{idx}\\n\")\n",
    "\n",
    "# Load the model\n",
    "loaded_lm_model = LM_withLSTM(len(vocab), lm_hyperparameters['embedding_dim'], lm_hyperparameters['output_dim'])\n",
    "# loaded_lm_model.load_state_dict(torch.load('lm_lstm_model.pth'))\n",
    "lm_state_dict = torch.load('lm_lstm_model.pth', weights_only=True)  # PyTorch 2.2+\n",
    "loaded_lm_model.load_state_dict(lm_state_dict)\n",
    "\n",
    "loaded_lm_model.to(device)\n",
    "loaded_lm_model.eval()  # Set the model to evaluation mode\n",
    "# Load the vocabulary\n",
    "loaded_lm_vocab = {}\n",
    "with open('lm_vocab.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        word, idx = line.strip().split('\\t')\n",
    "        loaded_lm_vocab[word] = int(idx)\n",
    "# Check if the loaded model and vocabulary are the same as the original\n",
    "assert lm_model.state_dict() == loaded_lm_model.state_dict(), \"Model state dicts do not match!\"\n",
    "\n",
    "for key in loaded_lm_model.state_dict():\n",
    "    if not torch.equal(loaded_lm_model.state_dict()[key], loaded_lm_model.state_dict()[key]):\n",
    "        print(f\"Mismatch in parameter: {key}\")\n",
    "        assert False, \"Model state dicts do not match!\"\n",
    "\n",
    "assert vocab == loaded_lm_vocab, \"Vocabulary does not match!\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the language model\n",
    "\n",
    "We'll evaluate our model using the BLiMP dataset (https://github.com/alexwarstadt/blimp). The BLiMP dataset contains sets of linguistic minimal pairs for various syntactic and semantic phenomena, We'll evaluate our model on *existential quantifiers* (link: https://github.com/alexwarstadt/blimp/blob/master/data/existential_there_quantifiers_1.jsonl). This data, as the name suggests, investigate whether language models assign higher probability to *correct* usage of there-quantifiers. \n",
    "\n",
    "An example entry in the dataset is: \n",
    "\n",
    "```\n",
    "{\"sentence_good\": \"There was a documentary about music irritating Allison.\", \"sentence_bad\": \"There was each documentary about music irritating Allison.\", \"field\": \"semantics\", \"linguistics_term\": \"quantifiers\", \"UID\": \"existential_there_quantifiers_1\", \"simple_LM_method\": true, \"one_prefix_method\": false, \"two_prefix_method\": false, \"lexically_identical\": false, \"pairID\": \"0\"}\n",
    "```\n",
    "\n",
    "Download the dataset and build a datareader (similar to what you did for word2vec). The dataset structure you should aim for is (you don't need to worry about the other keys for this assignment):\n",
    "\n",
    "```\n",
    "good_sentence_1, bad_sentence_1\n",
    "...\n",
    "```\n",
    "\n",
    "your task now is to compare the probability assigned to the good sentence with to the probability assigned to the bad sentence. To compute a probability for a sentence we consider the product of the probabilities assigned to the *gold* tokens, remember, at timestep ```t``` we're predicting which token comes *next* e.g. ```t+1``` (basically, you do the same thing as you did when training).\n",
    "\n",
    "In rough pseudo code what your code should do is:\n",
    "\n",
    "```\n",
    "accuracy = []\n",
    "for good_sentence, bad_sentence in dataset:\n",
    "    gs_lm_output = LanguageModel(good_sentence)\n",
    "    gs_token_probabilities = softmax(gs_lm_output)\n",
    "    gs_sentence_probability = product(gs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    bs_lm_output = LanguageModel(bad_sentence)\n",
    "    bs_token_probabilities = softmax(bs_lm_output)\n",
    "    bs_sentence_probability = product(bs_token_probabilities[GOLD_TOKENS])\n",
    "\n",
    "    # int(True) = 1 and int(False) = 0\n",
    "    is_correct = int(gs_sentence_probability > bs_sentence_probability)\n",
    "    accuracy.append(is_correct)\n",
    "\n",
    "print(numpy.mean(accuracy))\n",
    "    \n",
    "```\n",
    "\n",
    "[6 marks]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final accuracy: 0.703\n"
     ]
    }
   ],
   "source": [
    "''' \n",
    "# your code goes here\n",
    "import json\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    \n",
    "    accuracy = []\n",
    "    with open(path) as f:\n",
    "        # iterate over one pair of sentences at a time\n",
    "        for line in f:\n",
    "            # load the data\n",
    "            data = json.loads(line)\n",
    "            good_s = data['sentence_good']\n",
    "            bad_s = data['sentence_bad']\n",
    "            \n",
    "            # the data is tokenized as whitespace\n",
    "            tok_good_s = ...\n",
    "            tok_bad_s = ...\n",
    "            \n",
    "            # encode your words as integers using the vocab from the dataloader, size is (S)\n",
    "            # we use unsqueeze to create the batch dimension \n",
    "            # in this case our input is only ONE batch, so the size of the tensor becomes: \n",
    "            # (S) -> (1, S) as the model expects batches\n",
    "            enc_good_s = torch.tensor([_ for x in tok_good_s], device=device).unsqueeze(0)\n",
    "            enc_bad_s = torch.tensor([_ for x in tok_bad_s], device=device).unsqueeze(0)\n",
    "            \n",
    "            # pass your encoded sentences to the model and predict the next tokens\n",
    "            good_s = LM_withLSTM(enc_good_s)\n",
    "            bad_s = LM_withLSTM(enc_bad_s)\n",
    "            \n",
    "            # get probabilities with softmax\n",
    "            gs_probs = F.softmax(...)\n",
    "            bs_probs = F.softmax(...)\n",
    "            \n",
    "            # select the probability of the gold tokens\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "            \n",
    "            accuracy.append(int(gs_sent_prob>bs_sent_prob))\n",
    "            \n",
    "    return accuracy\n",
    "            \n",
    "def find_token_probs(model_probs, encoded_sentece):\n",
    "    probs = []\n",
    "\n",
    "    # iterate over the tokens in your encoded sentence\n",
    "    for token, gold_token in enumerate(encoded_sentece):\n",
    "        # select the probability of the gold tokens and save\n",
    "        # hint: pytorch indexing is helpful here ;)\n",
    "        prob = ...\n",
    "        probs.append(prob)\n",
    "    sentence_prob = ...\n",
    "    return sentence_prob\n",
    "\n",
    "path = 'existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, ..., ...)\n",
    "\n",
    "print('Final accuracy:')\n",
    "print(np.round(np.mean(accuracy), 3))\n",
    "'''\n",
    "\n",
    "import json\n",
    "import torch.nn.functional as F\n",
    "import numpy as np\n",
    "\n",
    "def evaluate_model(path, vocab, model):\n",
    "    accuracy = []\n",
    "    model.eval()  # Turn off dropout etc.\n",
    "\n",
    "    with open(path) as f:\n",
    "        for line in f:\n",
    "            data = json.loads(line)\n",
    "            gs = data['sentence_good']\n",
    "            bs = data['sentence_bad']\n",
    "\n",
    "            # Tokenize\n",
    "            tok_good_s = ['<start>'] + gs.lower().split() + ['<end>']\n",
    "            tok_bad_s = ['<start>'] + bs.lower().split() + ['<end>']\n",
    "\n",
    "            # Encode words as indices\n",
    "            enc_good_s = [vocab[w] if w in vocab else vocab['<pad>'] for w in tok_good_s]\n",
    "            enc_bad_s = [vocab[w] if w in vocab else vocab['<pad>'] for w in tok_bad_s]\n",
    "\n",
    "            # Convert to tensors\n",
    "            enc_good_s = torch.tensor(enc_good_s, device=device).unsqueeze(0)  # (1, S)\n",
    "            enc_bad_s = torch.tensor(enc_bad_s, device=device).unsqueeze(0)    # (1, S)\n",
    "\n",
    "            # Get predictions from the model\n",
    "            good_output = model(enc_good_s)\n",
    "            bad_output = model(enc_bad_s)\n",
    "\n",
    "            # Get probabilities with softmax\n",
    "            gs_probs = F.softmax(good_output, dim=-1)\n",
    "            bs_probs = F.softmax(bad_output, dim=-1)\n",
    "\n",
    "            # Compute sentence probabilities\n",
    "            gs_sent_prob = find_token_probs(gs_probs, enc_good_s)\n",
    "            bs_sent_prob = find_token_probs(bs_probs, enc_bad_s)\n",
    "\n",
    "            # If the good sentence has higher probability, count as correct\n",
    "            accuracy.append(int(gs_sent_prob > bs_sent_prob))\n",
    "\n",
    "    return accuracy\n",
    "\n",
    "def find_token_probs(model_probs, encoded_sentence):\n",
    "    probs = []\n",
    "    S = encoded_sentence.shape[1]\n",
    "\n",
    "    for t in range(S - 1):\n",
    "        # At timestep t, model predicts token at t+1\n",
    "        gold_token = encoded_sentence[0, t + 1]\n",
    "\n",
    "        if gold_token.item() == 0:  # Padding token (<pad> has index 0)\n",
    "            continue\n",
    "\n",
    "        # Get the model probability assigned to the gold token\n",
    "        prob = model_probs[0, t, gold_token].item()\n",
    "        probs.append(prob)\n",
    "\n",
    "    # Compute the total sentence probability as the product of token probs\n",
    "    sentence_prob = np.prod(probs) if probs else 0.0\n",
    "    return sentence_prob\n",
    "\n",
    "\n",
    "path = './data/existential_there_quantifiers_1.jsonl'\n",
    "accuracy = evaluate_model(path, vocab, lm_model)\n",
    "\n",
    "print('Final accuracy:', np.round(np.mean(accuracy), 3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our model get some score, say, 55% correct predictions. Is this good? Suggest some *baseline* (i.e. a stupid \"model\" we hope ours is better than) we can compare the model against.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A 55% accuracy suggests the model performs slightly better than chance (50%), which is a common baseline for binary classification tasks like this (choosing between good vs. bad sentences). This indicates the model has learned some useful linguistic patterns, but performance is still modest.\n",
    "Baseline to compare against:\n",
    "\n",
    "Random guessing → 50% accuracy.\n",
    "\n",
    "Unigram frequency model → ranks words by frequency without any syntax or context awareness.\n",
    "Our model should outperform both baselines."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some improvements you could make to your language model.\n",
    "\n",
    "[3 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Increase the size of the training corpus to expose the model to more linguistic patterns and reduce overfitting.\n",
    "\n",
    "Use a larger embedding dimension or hidden size to capture richer word and context representations.\n",
    "\n",
    "Train for more epochs or with better hyperparameter tuning (e.g., learning rate scheduling).\n",
    "\n",
    "Switch to bidirectional LSTM (BiLSTM) to allow the model to use both past and future context.\n",
    "\n",
    "Incorporate pre-trained embeddings (e.g., GloVe, FastText) instead of training embeddings from scratch.\n",
    "\n",
    "Using bigger batch-size to avoid vibration in parameter update."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Suggest some other metrics we can use to evaluate our system\n",
    "\n",
    "[2 marks]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Literature\n",
    "\n",
    "\n",
    "Neural architectures:\n",
    "\n",
    "[1] Y. Bengio, R. Ducharme, P. Vincent, and C. Janvin. A neural probabilistic language model. (Links to an external site.) Journal of Machine Learning Research, 3(6):1137–1155, 2003. (Sections 3 and 4 are less relevant today and hence you can glance through them quickly. Instead, look at the Mikolov papers where they describe training word embeddings with the current neural network architectures.)\n",
    "\n",
    "[2] T. Mikolov, K. Chen, G. Corrado, and J. Dean. Efficient estimation of word representations in vector space. arXiv preprint arXiv:1301.3781, 2013.\n",
    "\n",
    "[3] T. Mikolov, I. Sutskever, K. Chen, G. S. Corrado, and J. Dean. Distributed representations of words and phrases and their compositionality. In Advances in neural information processing systems, pages 3111–3119, 2013.\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "The assignment is marked on a 7-level scale where 4 is sufficient to complete the assignment; 5 is good solid work; 6 is excellent work, covers most of the assignment; and 7: creative work. \n",
    "\n",
    "This assignment has a total of 63 marks. These translate to grades as follows: 1 = 17% 2 = 34%, 3 = 50%, 4 = 67%, 5 = 75%, 6 = 84%, 7 = 92% where %s are interpreted as lower bounds to achieve that grade."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
