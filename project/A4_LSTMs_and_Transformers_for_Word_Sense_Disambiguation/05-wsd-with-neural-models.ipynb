{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A4: LSTMs and Transformers for Word Sense Disambiguation\n",
    "\n",
    "by Nikolai Ilinykh, Adam Ek, and others.\n",
    "\n",
    "The lab is an exploration and learning exercise to be done in a group and also in discussion with the teachers and other students.\n",
    "\n",
    "Write all your answers and the code in the appropriate boxes below.\n",
    "\n",
    "\n",
    "A problem with static distributional vectors is the difficulty of distinguishing between different *word senses*. We will continue our exploration of word vectors by considering *trainable vectors* or *word embeddings* for Word Sense Disambiguation (WSD). We will work with both LSTMs and transformer models, e.g. BERT. The purpose of the assignment is to learn use representations neural models in a downstream task of word sense disambiguation.\n",
    "\n",
    "\n",
    "## Word Sense Disambiguation Task\n",
    "\n",
    "The goal of word sense disambiguation is to train a model to find the sense of a word (homonyms of a word-form). For example, the word \"bank\" can mean \"sloping land\" or \"financial institution\". \n",
    "\n",
    "(a) \"I deposited my money in the **bank**\" (financial institution)\n",
    "\n",
    "(b) \"I swam from the river **bank**\" (sloping land)\n",
    "\n",
    "In case a) and b), we can determine the meaning of \"bank\" based on the *context*. To utilize context in a semantic model, we use *contextualized word representations*.\n",
    "\n",
    "Previously, we worked with *static word representations*, i.e., the representation does not depend on the context. To illustrate, we can consider sentences (a) and (b), where the word **bank** would have the same static representation in both sentences, which means that it becomes difficult for us to predict its sense. What we want is to create representations that depend on the context, i.e., *contextualized embeddings*.\n",
    "\n",
    "As we have discussed in the class, contextualized representations can come in the form of pre-training the model for some \"general\" task and then fine-tuning it for some downstream task. Here we will do the following:\n",
    "\n",
    "(1) Train and test LSTM model directly for word sense disambiguation. We will learn contextualized representations within this model.\n",
    "\n",
    "(2) Take BERT that was pre-trained on masked language modeling and next sentence prediction. Fine-tune it on our data and test it for the word sense disambiguation on the task dataset. The idea for you is to explore how pre-trained contextualized representations from BERT can be updated and used for the downstream task of word sense disambiguation.\n",
    "\n",
    "Your overall task in this lab is to create a neural network model that can disambiguate the word sense of 30 different words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we import some packages that we need\n",
    "\n",
    "# here add any package that you will need later\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchtext\n",
    "\n",
    "# our hyperparameters (add more when/if you need them)\n",
    "device = torch.device('cuda:1')\n",
    "\n",
    "batch_size = 8\n",
    "learning_rate = 0.001\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Working with Data\n",
    "\n",
    "A central part of any machine learning system is the data we're working with.\n",
    "\n",
    "In this section, we will split the data (the dataset is in `wsd_data.txt`) into a training set and a test set.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "The dataset we will use contains different word senses for 30 different words. The data is organized as follows (values separated by tabs), where each line is a separate item in the dataset:\n",
    "\n",
    "- Column 1: word-sense, e.g., keep%2:42:07::\n",
    "- Column 2: word-form, e.g., keep.v\n",
    "- Column 3: index of word, e.g., 15\n",
    "- Column 4: white-space tokenized context, e.g., Action by the Committee In pursuance of its mandate , the Committee will continue to keep under review the situation relating to the question of Palestine and participate in relevant meetings of the General Assembly and the Security Council . The Committee will also continue to monitor the situation on the ground and draw the attention of the international community to urgent developments in the Occupied Palestinian Territory , including East Jerusalem , requiring international action .\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the Data\n",
    "\n",
    "Your first task is to separate the data into a *training set* and a *test set*.\n",
    "\n",
    "The training set should contain 80% of the examples, and the test set the remaining 20%.\n",
    "\n",
    "The examples for the test/training set should be selected **randomly**.\n",
    "\n",
    "Save each dataset into a .csv file for loading later.\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def data_split(dataset_path):\n",
    "    # Read the dataset assuming it's tab-separated\n",
    "    data = pd.read_csv(dataset_path, sep='\\t', header=None,\n",
    "                       names=[\"word_sense\", \"word_form\", \"word_index\", \"context\"])\n",
    "    \n",
    "    # Shuffle the data\n",
    "    data = data.sample(frac=1, random_state=42).reset_index(drop=True)\n",
    "    \n",
    "    # Calculate split index\n",
    "    split_idx = int(0.8 * len(data))\n",
    "    \n",
    "    # Split the data\n",
    "    train_split = data.iloc[:split_idx]\n",
    "    test_split = data.iloc[split_idx:]\n",
    "    \n",
    "    # Save to CSV for later use (optional)\n",
    "    train_split.to_csv(\"train_split.csv\", index=False)\n",
    "    test_split.to_csv(\"test_split.csv\", index=False)\n",
    "\n",
    "    return train_split, test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Baseline\n",
    "\n",
    "Your second task is to create a *baseline* for the task.\n",
    "\n",
    "A baseline is a \"reality check\" for a model. Given a very simple heuristic/algorithmic/model solution to the problem, can our neural network perform better than this? Baselines are important as they give us a point of comparison for the actual models. They are commonly used in NLP. Sometimes baseline models are not simple models but previous state-of-the-art.\n",
    "\n",
    "In this exercise, we will have a simple baseline model that is the \"most common sense\" (MCS) baseline. For each word form, find the most commonly assigned sense to the word and label a word with that sense. In a fictional dataset, \"bank\" has two senses: \"financial institution,\" which occurs 5 times, and \"side of the river,\" which occurs 3 times. Thus, all 8 occurrences of \"bank\" are labeled \"financial institution,\" yielding an MCS accuracy of 5/8 = 62.5%. If a model obtains a higher score than this, we can conclude that the model *at least* is better than selecting the most frequent word sense.\n",
    "\n",
    "Your task is to write the code for this baseline, train, and test it. The baseline has the knowledge about labels and their frequency only from the train data. You evaluate it on the test data by comparing the ground-truth sense with the one that the model predicts. A good \"dumb\" baseline in this case is the one that performs quite badly. Expect the model to perform around 0.30 in terms of accuracy. You should use accuracy as your main metric; you can also compute the F1-score.\n",
    "\n",
    "**[2 marks]**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def mcs_baseline(data):\n",
    "    \n",
    "#     # your code goes here\n",
    "    \n",
    "#     return\n",
    "\n",
    "\n",
    "from collections import defaultdict, Counter\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "def mcs_baseline(train_data, test_data):\n",
    "    # Step 1: Count the most common sense for each word form in training data\n",
    "    sense_counts = defaultdict(Counter)\n",
    "    for _, row in train_data.iterrows():\n",
    "        word_form = row['word_form']\n",
    "        word_sense = row['word_sense']\n",
    "        sense_counts[word_form][word_sense] += 1\n",
    "\n",
    "    # Step 2: Build the MCS dictionary\n",
    "    mcs_dict = {word: senses.most_common(1)[0][0] for word, senses in sense_counts.items()}\n",
    "\n",
    "    # Step 3: Predict the most common sense for each word form in test data\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    for _, row in test_data.iterrows():\n",
    "        word_form = row['word_form']\n",
    "        true_sense = row['word_sense']\n",
    "        predicted_sense = mcs_dict.get(word_form, None)  # Fallback could be random or majority class\n",
    "        if predicted_sense is not None:\n",
    "            y_true.append(true_sense)\n",
    "            y_pred.append(predicted_sense)\n",
    "\n",
    "    # Step 4: Compute accuracy\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    return accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MCS Baseline Accuracy: 31.93%\n"
     ]
    }
   ],
   "source": [
    "# using MCS baseline\n",
    "train_data, test_data = data_split(\"wsd_data.txt\")\n",
    "mcs_accuracy = mcs_baseline(train_data, test_data)\n",
    "print(f\"MCS Baseline Accuracy: {mcs_accuracy:.2%}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Data Iterators\n",
    "\n",
    "To train a neural network, we first need to prepare the data. This involves converting words (and labels) to a number and organizing the data into batches. We also want the ability to shuffle the examples such that they appear in a random order.\n",
    "\n",
    "Your task is to create a dataloader for the training and test set you created previously.\n",
    "\n",
    "You are encouraged to adjust your own dataloader you built for previous assignments. Some things to take into account:\n",
    "\n",
    "1. Tokenize inputs, keep a dictionary of word-to-IDs and IDs-to-words (vocabulary), fix paddings. You might need to consider doing these for each of the four fields in the dataset.\n",
    "2. Your dataloader probably has a function to process data. Process each column in the dataset.\n",
    "3. You might want to clean the data a bit. For example, the first column has some symbols, which might be unnecessary. It is up to you whether you want to remove them and clean this column or keep labels the way they are. In any case, you must provide an explanation of your decision and how you think it will affect the performance of your model. Data and its preprocessing matters, so motivate your decisions.\n",
    "4. Organize your dataset into batches and shuffle them. You should have something akin to data iterators so that your model can take them.\n",
    "\n",
    "Implement the dataloader and perform necessary preprocessings.\n",
    "\n",
    "[**2 marks**]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataloader(path):\n",
    "\n",
    "#     # your code goes here\n",
    "#     # below are only some examples!\n",
    "    \n",
    "#     def __getitem__(self, idx):\n",
    "        \n",
    "#         return Y\n",
    "\n",
    "#     def __len__(self):\n",
    "        \n",
    "#         return X\n",
    "    \n",
    "# def data_load(something):\n",
    "    \n",
    "#     return dataloader_train, dataloader_test\n",
    "\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "class dataloader(Dataset):\n",
    "    def __init__(self, dataframe, word2idx, label_encoder):\n",
    "        self.data = dataframe\n",
    "        self.word2idx = word2idx\n",
    "        self.label_encoder = label_encoder\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.data.iloc[idx]\n",
    "        context_words = row['context'].split()\n",
    "        word_ids = [self.word2idx.get(w, self.word2idx['<unk>']) for w in context_words]\n",
    "        word_tensor = torch.tensor(word_ids, dtype=torch.long)\n",
    "\n",
    "        target_index = int(row['word_index'])\n",
    "        label = self.label_encoder.transform([row['word_sense']])[0]\n",
    "\n",
    "        return word_tensor, target_index, label\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "def collate_batch(batch):\n",
    "    contexts, indices, labels = zip(*batch)\n",
    "    padded_contexts = pad_sequence(contexts, batch_first=True, padding_value=0)\n",
    "    indices = torch.tensor(indices, dtype=torch.long)\n",
    "    labels = torch.tensor(labels, dtype=torch.long)\n",
    "    return padded_contexts, indices, labels\n",
    "\n",
    "def build_vocab(df):\n",
    "    word2idx = {'<pad>': 0, '<unk>': 1}\n",
    "    idx = 2\n",
    "    for text in df['context']:\n",
    "        for word in text.split():\n",
    "            if word not in word2idx:\n",
    "                word2idx[word] = idx\n",
    "                idx += 1\n",
    "    return word2idx\n",
    "\n",
    "def data_load(train_df, test_df, batch_size=8):\n",
    "    # Build vocabulary\n",
    "    word2idx = build_vocab(train_df)\n",
    "\n",
    "    # Encode labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(train_df['word_sense'])\n",
    "\n",
    "    # Create datasets\n",
    "    train_dataset = dataloader(train_df, word2idx, label_encoder)\n",
    "    test_dataset = dataloader(test_df, word2idx, label_encoder)\n",
    "\n",
    "    # Create DataLoaders\n",
    "    dataloader_train = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, collate_fn=collate_batch)\n",
    "    dataloader_test = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, collate_fn=collate_batch)\n",
    "\n",
    "    return dataloader_train, dataloader_test, word2idx, label_encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.1 LSTM for Word Sense Disambiguation\n",
    "\n",
    "In this section, we will train an LSTM model to predict word senses based on *contextualized representations*.\n",
    "\n",
    "You can read more about LSTMs [here](https://colah.github.io/posts/2015-08-Understanding-LSTMs/).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model\n",
    "\n",
    "We will use a **bidirectional** Long Short-Term Memory (LSTM) network to create a representation for the sentences and a **linear** classifier to predict the sense of each word.\n",
    "\n",
    "As we discussed in the lecture, bidirectional LSTM is using **two** hidden states: one that goes in the left-to-right direction, and another one that goes in the right-to-left direction. PyTorch documentation on LSTMs can be found [here](https://pytorch.org/docs/stable/generated/torch.nn.LSTM.html). It says that if the bidirectional parameter is set to True, then \"h_n will contain a concatenation of the final forward and reverse hidden states, respectively.\" Keep it in mind because you will have to ensure that your linear layer for prediction takes input of that size.\n",
    "\n",
    "When we initialize the model, we need a few things:\n",
    "\n",
    "1) An embedding layer: a dictionary from which we can obtain word embeddings\n",
    "2) A LSTM-module to obtain contextual representations\n",
    "3) A classifier that computes scores for each word-sense given *some* input\n",
    "\n",
    "The general procedure is the following:\n",
    "\n",
    "1) For each word in the sentence, obtain word embeddings\n",
    "2) Run the embedded sentences through the LSTM\n",
    "3) Select the appropriate hidden state\n",
    "4) Predict the word-sense \n",
    "\n",
    "**Suggestion for efficiency:** *Use a low dimensionality (32) for word embeddings and the LSTM when developing and testing the code, then scale up when running the full training/tests*\n",
    "\n",
    "Your tasks will be to create **two different models** (both follow the two outlines described above)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your first model should make a prediction from the LSTM's representation of the target word.\n",
    "\n",
    "In particular, you run your LSTM on the context in which the target word is used. LSTM will produce a sequence of hidden states. Each hidden state corresponds to a single word from the input context. For example, you should be able to get 37 hidden states for a context that has 37 words/elements in it. Next, take the LSTM's representation of the target word. For example, it can be hidden state number 5, because the fifth word in your context is the target word that you want to predict the meaning for. This target's word representation is the input to your linear layer that makes the final prediction.\n",
    "\n",
    "**[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WSDModel_approach1(nn.Module):\n",
    "#     def __init__(self, ...):\n",
    "        \n",
    "#         # your code goes here\n",
    "#         self.embeddings = ...\n",
    "#         self.rnn = ...\n",
    "#         self.classifier = ...\n",
    "    \n",
    "#     def forward(self, batch):\n",
    "#         # your code goes here\n",
    "        \n",
    "#         return predictions\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class WSDModel_approach1(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(WSDModel_approach1, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        text, word_indices = batch  # text: [batch_size, seq_len], word_indices: [batch_size]\n",
    "        embedded = self.embeddings(text)  # [batch_size, seq_len, embedding_dim]\n",
    "        outputs, _ = self.lstm(embedded)  # [batch_size, seq_len, hidden_dim * 2]\n",
    "\n",
    "        # Gather hidden states corresponding to target word indices\n",
    "        batch_size = outputs.size(0)\n",
    "        target_outputs = outputs[torch.arange(batch_size), word_indices]  # [batch_size, hidden_dim*2]\n",
    "\n",
    "        predictions = self.classifier(target_outputs)  # [batch_size, output_dim]\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Your second model should make a prediction from the final hidden state of your LSTM.\n",
    "\n",
    "In particular, do the same first steps as in the first approach. But then to make a prediction with your linear layer, you will need to take the last hidden state that your LSTM produces for the whole sequence.\n",
    "\n",
    "**[5 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class WSDModel_approach2(nn.Module):\n",
    "#     def __init__(self, ...):\n",
    "#         # your code goes here\n",
    "    \n",
    "#     def forward(self, ...):\n",
    "#         # your code goes here\n",
    "        \n",
    "#         return predictions\n",
    "\n",
    "\n",
    "class WSDModel_approach2(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, output_dim, padding_idx):\n",
    "        super(WSDModel_approach2, self).__init__()\n",
    "        self.embeddings = nn.Embedding(vocab_size, embedding_dim, padding_idx=padding_idx)\n",
    "        self.lstm = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.classifier = nn.Linear(hidden_dim * 2, output_dim)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        text, _ = batch  # word index not used here\n",
    "        embedded = self.embeddings(text)\n",
    "        _, (hidden, _) = self.lstm(embedded)\n",
    "\n",
    "        # hidden shape: [2, batch_size, hidden_dim] for bidirectional\n",
    "        # Concatenate forward and backward hidden states\n",
    "        final_hidden = torch.cat((hidden[0], hidden[1]), dim=1)  # [batch_size, hidden_dim * 2]\n",
    "\n",
    "        predictions = self.classifier(final_hidden)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training and Testing the Model\n",
    "\n",
    "Now we are ready to train and test our model. What we need now is a loss function, an optimizer, and our data. \n",
    "\n",
    "- First, create the loss function and the optimizer.\n",
    "- Next, iterate over the number of epochs (i.e., how many times we let the model see our data). \n",
    "- For each epoch, iterate over the dataset to obtain batches. Use the batch as input to the model, and let the model output scores for the different word senses.\n",
    "- For each model output, calculate the loss (and print the loss) on the output and update the model parameters.\n",
    "- Reset the gradients and repeat.\n",
    "- After all epochs are done, test your trained model on the test set and calculate the total and per-word-form accuracy of your model.\n",
    "\n",
    "Implement the training and testing of the model.\n",
    "\n",
    "**[4 marks]**\n",
    "\n",
    "**Suggestion for efficiency:** *When developing your model, try training and testing the model on one or two batches (for each epoch) of data to make sure everything works! It's very annoying if you train for N epochs to find out that something went wrong when testing the model, or to find that something goes wrong when moving from epoch 0 to epoch 1.*\n",
    "\n",
    "Do not forget to save your best models as .pickle files. The results should be reproducible for us to evaluate your models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_iter, test_iter, vocab, labels = dataloader(path_to_folder)\n",
    "\n",
    "# loss_function = ...\n",
    "# optimizer = ...\n",
    "# model = ...\n",
    "\n",
    "# for _ in range(epochs):\n",
    "#     # train model\n",
    "#     ...\n",
    "    \n",
    "# # test model after all epochs are completed\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.metrics import accuracy_score\n",
    "import pickle\n",
    "import numpy\n",
    "\n",
    "def train_and_evaluate(path_to_folder, model_class, epochs=3, lr=0.001, batch_size=4, save_path=\"best_model.pkl\"):\n",
    "    # Load data\n",
    "    # train_iter, test_iter, vocab, label_encoder = dataloader(path_to_folder)\n",
    "    train_data, test_data = data_split(path_to_folder)\n",
    "\n",
    "    # make sure the data is loaded correctly\n",
    "    # print(train_data.columns)\n",
    "    # print(train_data.head())\n",
    "\n",
    "    train_iter, test_iter, vocab, label_encoder = data_load(train_data, test_data, batch_size=batch_size)\n",
    "\n",
    "\n",
    "    # Model, loss, optimizer\n",
    "    model = model_class(\n",
    "        vocab_size=len(vocab),\n",
    "        embedding_dim=32,\n",
    "        hidden_dim=32,\n",
    "        output_dim=len(label_encoder.classes_),\n",
    "        padding_idx=vocab['<pad>']\n",
    "    )\n",
    "\n",
    "    loss_function = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    loss_function.to(device)\n",
    "\n",
    "    # Training loop\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for batch in train_iter:\n",
    "            inputs, word_indices, labels = [x.to(device) for x in batch]\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model((inputs, word_indices))  # shape: [batch_size, num_classes]\n",
    "            loss = loss_function(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs} | Loss: {total_loss:.4f}\")\n",
    "\n",
    "    # Evaluation on test data\n",
    "    model.eval()\n",
    "    all_preds, all_labels = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_iter:\n",
    "            inputs, word_indices, labels = [x.to(device) for x in batch]\n",
    "            outputs = model((inputs, word_indices))\n",
    "            \n",
    "            # Test for debugging, numpy error\n",
    "            # print(type(outputs))\n",
    "            # print(type(torch.argmax(outputs, dim=1)))\n",
    "            '''\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            '''\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().tolist()\n",
    "            all_preds.extend(preds)\n",
    "            all_labels.extend(labels.cpu().tolist())\n",
    "            \n",
    "\n",
    "\n",
    "    accuracy = accuracy_score(all_labels, all_preds)\n",
    "    print(f\"Test Accuracy: {accuracy:.2%}\")\n",
    "\n",
    "    # Save model\n",
    "    with open(save_path, \"wb\") as f:\n",
    "        pickle.dump(model.state_dict(), f)\n",
    "\n",
    "    return model\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 22824.9954\n",
      "Epoch 2/5 | Loss: 14674.2469\n",
      "Epoch 3/5 | Loss: 12468.6999\n",
      "Epoch 4/5 | Loss: 10776.5447\n",
      "Epoch 5/5 | Loss: 9302.1645\n",
      "Test Accuracy: 67.97%\n"
     ]
    }
   ],
   "source": [
    "# using model approach 1\n",
    "trained_model = train_and_evaluate(\"wsd_data.txt\", WSDModel_approach1, epochs=5, lr=0.001, batch_size=4, save_path=\"WSDModel_approach1_best_model.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 | Loss: 73954.5119\n",
      "Epoch 2/5 | Loss: 67911.8845\n",
      "Epoch 3/5 | Loss: 60084.2378\n",
      "Epoch 4/5 | Loss: 51437.4014\n",
      "Epoch 5/5 | Loss: 42160.7167\n",
      "Test Accuracy: 34.40%\n"
     ]
    }
   ],
   "source": [
    "# using model approach 2\n",
    "trained_model2 = train_and_evaluate(\"wsd_data.txt\", WSDModel_approach2, epochs=5, lr=0.001, batch_size=4, save_path=\"WSDModel_approach2_best_model.pkl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2.2 Fine-tuning and Testing BERT for Word Sense Disambiguation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section of the lab, you'll try out the transformer, specifically the BERT model. For this, we'll use the Hugging Face library ([https://huggingface.co/](https://huggingface.co/)).\n",
    "\n",
    "You can find the documentation for the BERT model [here](https://huggingface.co/transformers/model_doc/bert.html) and a general usage guide [here](https://huggingface.co/transformers/v2.9.1/quickstart.html).\n",
    "\n",
    "What we're going to do is *fine-tune* the BERT model, i.e., update the weights of a pre-trained model. That is, we have a model that is pre-trained on masked language modeling and next sentence prediction (kind of basic, general tasks which are useful for a lot of more specific tasks), but now we apply it to word sense disambiguation with the word representations it has learned.\n",
    "\n",
    "We'll use the same data splits for training and testing as before, but this time you will use a different dataloader.\n",
    "\n",
    "Now you create an iterator that collects N sentences (where N is the batch size) then use the BertTokenizer to transform the sentence into integers. For your dataloader, remember to:\n",
    "* Shuffle the data in each batch\n",
    "* Make sure you get a new iterator for each *epoch*\n",
    "* Create a vocabulary of *sense-labels* so you can calculate accuracy \n",
    "\n",
    "We then pass this batch into the BERT model (you must have pre-loaded its weights) and update the weights (fine-tune). The BERT model will encode the sentence, then we send this encoded sentence into a prediction layer and collect what it outputs.\n",
    "\n",
    "As input to the prediction layer, you are free to play with different types of information. For example, the expected way would be to use CLS representation. You can also use other representations and compare them.\n",
    "\n",
    "About the hyperparameters and training:\n",
    "* For BERT, usually a lower learning rate works best, between 0.0001-0.000001.\n",
    "* BERT takes a lot of resources, running it on CPU will take ages, utilize the GPUs :)\n",
    "* Since BERT takes a lot of resources, use a small batch size (4-8)\n",
    "* Computing the BERT representation, make sure you pass the mask\n",
    "\n",
    "**[12 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dataloader_for_bert(path_to_file, batch_size):\n",
    "#     ...\n",
    "\n",
    "import torch\n",
    "from transformers import BertTokenizer\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import pandas as pd\n",
    "\n",
    "class WSD_BERT_Dataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, label_encoder):\n",
    "        self.tokenizer = tokenizer\n",
    "        self.label_encoder = label_encoder\n",
    "        self.contexts = dataframe['context'].tolist()\n",
    "        self.labels = label_encoder.transform(dataframe['word_sense'])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.contexts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        encoding = self.tokenizer(self.contexts[idx],\n",
    "                                  padding='max_length',\n",
    "                                  truncation=True,\n",
    "                                  max_length=128,\n",
    "                                  return_tensors='pt')\n",
    "        input_ids = encoding['input_ids'].squeeze(0)\n",
    "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
    "        # label = self.labels[idx]\n",
    "        # label = int(self.labels[idx][0])\n",
    "        label = int(self.labels[idx])\n",
    "        return input_ids, attention_mask, label\n",
    "\n",
    "\n",
    "def dataloader_for_bert(path_to_file, batch_size):\n",
    "    df = pd.read_csv(path_to_file, sep='\\t', header=None,\n",
    "                     names=['word_sense', 'word_form', 'word_index', 'context'])\n",
    "\n",
    "    tokenizer = BertTokenizer.from_pretrained(\"bert-base-uncased\")\n",
    "    label_encoder = LabelEncoder()\n",
    "    label_encoder.fit(df['word_sense'])\n",
    "\n",
    "    dataset = WSD_BERT_Dataset(df, tokenizer, label_encoder)\n",
    "    dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n",
    "    \n",
    "    return dataloader, label_encoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class BERT_WSD(nn.Module):\n",
    "#     def __init__(self, ...):\n",
    "#         # your code goes here\n",
    "#         self.bert = ...\n",
    "#         self.classifier = ...\n",
    "    \n",
    "#     def forward(self, batch):\n",
    "#         # your code goes here\n",
    "        \n",
    "#         return predictions\n",
    "\n",
    "\n",
    "from transformers import BertModel, BertForSequenceClassification\n",
    "\n",
    "class BERT_WSD(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(BERT_WSD, self).__init__()\n",
    "        # self.bert = BertModel.from_pretrained(\"bert-base-uncased\")\n",
    "        self.bert = BertForSequenceClassification.from_pretrained(\n",
    "        \"bert-base-uncased\",\n",
    "        num_labels=num_classes)\n",
    "        \n",
    "        self.classifier = nn.Linear(self.bert.config.hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, batch):\n",
    "        input_ids, attention_mask = batch\n",
    "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        cls_output = outputs.last_hidden_state[:, 0, :]  # Use CLS token\n",
    "        predictions = self.classifier(cls_output)\n",
    "        return predictions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loss_function = ...\n",
    "# optimizer = ...\n",
    "# model = ...\n",
    "\n",
    "# for _ in range(epochs):\n",
    "#     # train model\n",
    "#     ...\n",
    "    \n",
    "# # test model after all epochs are completed\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import os\n",
    "\n",
    "def evaluate_bert_model(model, dataloader, device):\n",
    "    model.eval()\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids, attention_mask = input_ids.to(device), attention_mask.to(device)\n",
    "            outputs = model((input_ids, attention_mask))\n",
    "            preds = torch.argmax(outputs, dim=1).cpu().numpy()\n",
    "            y_pred.extend(preds)\n",
    "            y_true.extend(labels.numpy())\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    f1 = f1_score(y_true, y_pred, average='macro')\n",
    "    return acc, f1\n",
    "\n",
    "\n",
    "def train_bert_model(path_to_file, batch_size=8, epochs=3, lr=1e-5, save_path=\"bert_wsd_best.pt\"):\n",
    "    from transformers import BertTokenizer\n",
    "    dataloader, label_encoder = dataloader_for_bert(path_to_file, batch_size)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BERT_WSD(num_classes=len(label_encoder.classes_)).to(device)\n",
    "\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=lr)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    best_accuracy = 0\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "\n",
    "        for input_ids, attention_mask, labels in dataloader:\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model((input_ids, attention_mask))\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{epochs}, Loss: {total_loss:.4f}\")\n",
    "\n",
    "        # Evaluate after each epoch\n",
    "        acc, f1 = evaluate_bert_model(model, dataloader, device)\n",
    "        print(f\" → Eval Acc: {acc:.2%}, F1: {f1:.4f}\")\n",
    "\n",
    "        # Save best model\n",
    "        if acc > best_accuracy:\n",
    "            best_accuracy = acc\n",
    "            torch.save(model.state_dict(), save_path)\n",
    "            print(f\" ✓ Best model saved to {save_path}\")\n",
    "\n",
    "    print(f\"\\nBest Accuracy: {best_accuracy:.2%}\")\n",
    "    return model, label_encoder\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trained_model_bert = train_bert_model(\"wsd_data.txt\", batch_size=4, epochs=1, lr=1e-5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Explain the difference between the two LSTMs that you have implemented for word sense disambiguation.\n",
    "\n",
    "Important note: your LSTMs should be nearly the same, but your linear layer must take different inputs. Describe why and how you think this difference will affect the performance of different LSTMs. How does the contextual representation of the whole sequence perform? How does the representation of the target word perform? What is better and for what situations? Why do we observe these differences?\n",
    "\n",
    "What kind of representations are the different approaches using to predict word senses?\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Evaluate your model with per-word form *accuracy* and comment on the results you get. How does the model perform in comparison to the baseline, and how do the models compare to each other? \n",
    "\n",
    "Expand on the evaluation by sorting the word-forms by the number of senses they have. Are word forms with fewer senses easier to predict? Give a short explanation of the results you get based on the number of senses per word.\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do the LSTMs perform in comparison to BERT? What's the difference between representations obtained by the LSTMs and BERT?\n",
    "\n",
    "**[4 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What could we do to improve all WSD models that we have worked with in this assignment?\n",
    "\n",
    "**[2 marks]**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Readings\n",
    "\n",
    "[1] Kågebäck, M., & Salomonsson, H. (2016). Word Sense Disambiguation using a Bidirectional LSTM. arXiv preprint arXiv:1606.03568.\n",
    "\n",
    "[2] ON WSD: https://web.stanford.edu/~jurafsky/slp3/slides/Chapter18.wsd.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Statement of contribution\n",
    "\n",
    "Briefly state how many times you have met for discussions, who was present, to what degree each member contributed to the discussion and the final answers you are submitting."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Marks\n",
    "\n",
    "This assignment has a total of 46 marks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3.9_torchtext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
